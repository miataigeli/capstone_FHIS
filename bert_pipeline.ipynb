{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGuJYYY0M9NNAs4Ke9i2wE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miataigeli/capstone_FHIS/blob/darya/bert_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xKLzIOjIkBE"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q-um0cGSk2X"
      },
      "source": [
        "Based on tutorial here: https://www.youtube.com/watch?v=mw7ay38--ak"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj0W0YOdIPi1",
        "outputId": "be95241f-3485-43da-da5d-40d66f72b043"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 46.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o-wxqMdS5Od"
      },
      "source": [
        "import transformers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak550KxaIpl1"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast, BertModel\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IInqOgwdI5lb"
      },
      "source": [
        "#specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS80BGDmKUIS",
        "outputId": "7d4b7b58-dbe6-46aa-b6ab-54cf04b4f107"
      },
      "source": [
        "#connect to my drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m7XORvdJGiw"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VnN6jmTLkdJ",
        "outputId": "4eb43ba3-194b-40fd-cde1-34d7e6e1a635"
      },
      "source": [
        "# Read in all json files into one dataframe\n",
        "import os\n",
        "\n",
        "corpus_dir = \"/content/drive/MyDrive/capstone/corpus\"\n",
        "corpus_df = pd.DataFrame([], columns = ['content', 'level'])\n",
        "\n",
        "for filename in os.listdir(corpus_dir):\n",
        "    if filename.endswith(\".json\"): \n",
        "         file_path = os.path.join(corpus_dir, filename)\n",
        "         df = pd.read_json(file_path)\n",
        "         df = df.drop(columns=['source', 'author', 'title'])\n",
        "         corpus_df = pd.concat([corpus_df, df])\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "print(corpus_df.describe())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  content level\n",
            "count                                                 308   308\n",
            "unique                                                308     5\n",
            "top     Señor, porque sé que habréis placer de la gran...    A1\n",
            "freq                                                    1    94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYx2n0zMRHcc",
        "outputId": "8026585d-fad5-427e-8243-8c11a6e66b29"
      },
      "source": [
        "corpus_df['level'].value_counts(normalize = True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "A1    0.305195\n",
              "B     0.288961\n",
              "A2    0.201299\n",
              "B1    0.136364\n",
              "B2    0.068182\n",
              "Name: level, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikuOZB4nn_R7",
        "outputId": "eac6f93b-62b2-4caf-c0b2-787292224dde"
      },
      "source": [
        "labels = corpus_df['level'].unique()\n",
        "print(labels)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['A1' 'A2' 'B1' 'B2' 'B']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vab74dGRXHR"
      },
      "source": [
        "### Split into train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekaasyTsJHsy"
      },
      "source": [
        "train_text, test_text, train_levels, test_levels = train_test_split(list(df['content']), list(df['level']),\n",
        "                                                                    random_state = 2021,\n",
        "                                                                    test_size = 0.3) #did not include stratify\n",
        "\n",
        "# split test into validation and test\n",
        "val_text, test_text, val_levels, test_levels = train_test_split(test_text, test_levels,\n",
        "                                                                random_state = 2021,\n",
        "                                                                test_size=0.5)"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5c0NQMV3V9p"
      },
      "source": [
        "End-to-end BERT Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NeauiSJ33Rm"
      },
      "source": [
        "model_path = 'dccuchile/bert-base-spanish-wwm-uncased'\n",
        "# tokenizer from pre-trained BERT model\n",
        "tokenizer = BertTokenizerFast.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', return_tensors='pt')\n",
        "# Define label to number dictionary\n",
        "lab2ind = {'A1': 1,\n",
        "           'A2': 2,\n",
        "           'B1': 3,\n",
        "           'B2': 4,\n",
        "           'B': 5}"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WLkZ3i040Ah"
      },
      "source": [
        "# Prepare data\n",
        "def prepare_data(text, levels, max_len = 32):\n",
        "\n",
        "  # Tokenize text\n",
        "  tokenized_texts = tokenizer.batch_encode_plus(list(text), padding=True, return_token_type_ids=False, return_tensors='pt')\n",
        "  print (\"Tokenize the first sentence:\\n\",tokenized_texts[0])\n",
        "  print(tokenized_texts)\n",
        "  print(tokenized_texts['input_ids'][0].shape)\n",
        "  print(tokenized_texts['attention_mask'][0].shape)\n",
        "\n",
        "  # Create label tensor\n",
        "  labels = [lab2ind[i] for i in levels]\n",
        "  labels = torch.tensor(labels)\n",
        "  print(\"Labels:\\n\", labels)\n",
        "\n",
        "  # Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
        "  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "  print (\"Index numbers of the first sentence:\\n\",input_ids[0])\n",
        "\n",
        "  # Pad our input seqeunce to the fixed length (i.e., max_len) with index of [PAD] token\n",
        "  #pad_ind = tokenizer.convert_tokens_to_ids(['[PAD]'])[0]\n",
        "  #input_ids = pad_sequences(list(input_ids), maxlen=max_len+2, dtype=\"long\", truncating=\"post\", padding=\"post\", value=pad_ind)\n",
        "  input_ids = tokenized_texts['input_ids']\n",
        "  print (\"Index numbers of the first sentence after padding:\\n\",input_ids[0])\n",
        "\n",
        "  # Create attention masks\n",
        "  #attention_masks = []\n",
        "\n",
        "  # Create a mask of 1s for each token followed by 0s for pad tokens\n",
        "  #for seq in input_ids:\n",
        "  #    seq_mask = [float(i>1) for i in seq]\n",
        "  #    attention_masks.append(seq_mask)\n",
        "  attention_masks = tokenized_texts['attention_mask']\n",
        "\n",
        "  # Convert all of our data into torch tensors, the required datatype for our model\n",
        "  inputs = torch.tensor(input_ids)\n",
        "  masks = torch.tensor(attention_masks)\n",
        "\n",
        "  return inputs, labels, masks"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cESkbgF8BQKo",
        "outputId": "da2fefd2-9c93-4d12-f17f-a6be4f894156"
      },
      "source": [
        "# Training data\n",
        "train_inputs, train_labels, train_masks = prepare_data(train_text, train_levels)\n",
        "print(train_inputs.shape)\n",
        "print(train_labels.shape)\n",
        "print(train_masks.shape)"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            " Encoding(num_tokens=2648, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "{'input_ids': tensor([[   4, 5390, 1306,  ...,    1,    1,    1],\n",
            "        [   4, 5390, 1129,  ...,    1,    1,    1],\n",
            "        [   4, 5390, 1098,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [   4, 5390, 1098,  ...,    1,    1,    1],\n",
            "        [   4, 5390,  997,  ...,    1,    1,    1],\n",
            "        [   4, 5390, 2116,  ...,    1,    1,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
            "torch.Size([2648])\n",
            "torch.Size([2648])\n",
            "Labels:\n",
            " tensor([1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 1, 2])\n",
            "Index numbers of the first sentence:\n",
            " 3\n",
            "Index numbers of the first sentence after padding:\n",
            " tensor([   4, 5390, 1306,  ...,    1,    1,    1])\n",
            "torch.Size([37, 2648])\n",
            "torch.Size([37])\n",
            "torch.Size([37, 2648])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLKAj5UHE7VQ",
        "outputId": "63603a0a-c455-4090-fdec-3a600dde911e"
      },
      "source": [
        "# Validation data\n",
        "valid_inputs, valid_labels, valid_masks = prepare_data(val_text, val_levels)\n",
        "print(valid_inputs[0])\n",
        "print(valid_labels)\n",
        "print(valid_masks)"
      ],
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            " Encoding(num_tokens=2292, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "{'input_ids': tensor([[   4, 5390,  999,  ...,    1,    1,    1],\n",
            "        [   4, 5390, 1001,  ...,    1,    1,    1],\n",
            "        [   4, 5390, 1002,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [   4, 5390, 1098,  ...,    1,    1,    1],\n",
            "        [   4, 5390, 2116,  ...,    1,    1,    1],\n",
            "        [   4, 5390, 1413,  ...,    1,    1,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
            "torch.Size([2292])\n",
            "torch.Size([2292])\n",
            "Labels:\n",
            " tensor([1, 1, 1, 2, 1, 2, 1, 1])\n",
            "Index numbers of the first sentence:\n",
            " 3\n",
            "Index numbers of the first sentence after padding:\n",
            " tensor([   4, 5390,  999,  ...,    1,    1,    1])\n",
            "tensor([   4, 5390,  999,  ...,    1,    1,    1])\n",
            "tensor([1, 1, 1, 2, 1, 2, 1, 1])\n",
            "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGSjlIj9FPfo",
        "outputId": "32d04353-d94e-4bf9-c1b3-4bd945547187"
      },
      "source": [
        "# Test data\n",
        "test_inputs, test_labels, test_masks = prepare_data(test_text, test_levels)\n",
        "print(test_inputs[0])\n",
        "print(test_labels)\n",
        "print(test_masks)"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenize the first sentence:\n",
            " Encoding(num_tokens=2097, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
            "{'input_ids': tensor([[   4, 5390, 2296,  ...,    1,    1,    1],\n",
            "        [   4, 5390, 1673,  ...,    1,    1,    1],\n",
            "        [   4, 5390, 1129,  ...,    1,    1,    1],\n",
            "        ...,\n",
            "        [   4, 5390, 1001,  ...,    1,    1,    1],\n",
            "        [   4, 5390, 1129,  ...,    1,    1,    1],\n",
            "        [   4, 5390, 2104,  ...,    1,    1,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
            "torch.Size([2097])\n",
            "torch.Size([2097])\n",
            "Labels:\n",
            " tensor([1, 1, 1, 1, 1, 1, 1, 2])\n",
            "Index numbers of the first sentence:\n",
            " 3\n",
            "Index numbers of the first sentence after padding:\n",
            " tensor([   4, 5390, 2296,  ...,    1,    1,    1])\n",
            "tensor([   4, 5390, 2296,  ...,    1,    1,    1])\n",
            "tensor([1, 1, 1, 1, 1, 1, 1, 2])\n",
            "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbHcyc0F25rz"
      },
      "source": [
        "# Create an iterator for our data\n",
        "batch_size = 32\n",
        "# We'll take training samples in random order in each epoch. \n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_dataloader = DataLoader(train_data, \n",
        "                              sampler = RandomSampler(train_data), # Select batches randomly\n",
        "                              batch_size=batch_size)\n",
        "\n",
        "# We'll just read validation set sequentially.\n",
        "validation_data = TensorDataset(valid_inputs, valid_masks, valid_labels)\n",
        "validation_dataloader = DataLoader(validation_data, \n",
        "                                   sampler = SequentialSampler(validation_data), # Pull out batches sequentially.\n",
        "                                   batch_size=batch_size)"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqJ4VUD9NcKN",
        "outputId": "72ceb2a5-2cca-47fb-9d1f-154283318414"
      },
      "source": [
        "model_path = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
        "\n",
        "bert_model = BertModel.from_pretrained(model_path, output_hidden_states=True, output_attentions=True).to(device)"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4I_iHm7SRwQ"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT8Ns18DSTFO",
        "outputId": "9d991731-0660-4682-808e-1d8cb9e091a0"
      },
      "source": [
        "# import BERT-based pretrained model\n",
        "#bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "bert_model = AutoModel.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased').to(device)#, output_hidden_states=True, output_attentions=True).to(device)\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNCyCyBRSvCL",
        "outputId": "a519a7ae-75a2-4f72-c4eb-2c0fa9c5374c"
      },
      "source": [
        "#sample data\n",
        "text = ['Me llamo Darya', 'vamos a probar un modelo de red neuronal.']\n",
        "\n",
        "#encode text\n",
        "tokenized_texts = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False, return_tensors='pt').to(device)\n",
        "print(tokenized_texts)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[    4,  1094,  5592,  1785,  1742,     5,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    4,  1441,  1012,  6909,  1044,  4209,  1009,  2946, 12212,  7592,\n",
            "          1008,     5]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4PDQaamkadX",
        "outputId": "cb77a598-dd7a-4706-af08-1ed990b83929"
      },
      "source": [
        "#input_ids = [tokenizer.convert_tokens_to_ids(x, return_tensors='pt') for x in tokenized_texts]\n",
        "#print(input_ids)\n",
        "outputs = bert_model(tokenized_texts['input_ids'])\n",
        "print(outputs.keys())"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['last_hidden_state', 'pooler_output'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOVxxnqUmPbL"
      },
      "source": [
        "last_hidden_state = outputs['last_hidden_state']\n",
        "pooler_output = outputs['pooler_output']"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPy8xRhZnMo9",
        "outputId": "df37cc9c-07c1-42fb-b92e-4da11e0d845d"
      },
      "source": [
        "last_hidden_state.shape"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 12, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-Hci3OhnKEX",
        "outputId": "4723420c-fb98-425c-e999-26fb76735f8a"
      },
      "source": [
        "pooler_output.shape"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW5lHzTnmvH3"
      },
      "source": [
        "We use `pooler_output` as context representation and pass it to a fully connected layer which outputs the prediction probabilities across all labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8YSNixYm62t"
      },
      "source": [
        "Two new feed-forward layers for classification are added on top of BERT. Each input is a context representation (`pooler_output`) that is a 768-dimensional vector, and the output is the probability distribution across all labels that is a 5-dimensional vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx8RtBqmm2P2"
      },
      "source": [
        "dense = nn.Linear(768, 768).to(device)\n",
        "dropout = nn.Dropout(0.1).to(device)\n",
        "fc = nn.Linear(768, 5).to(device)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eefWXz4dne_x",
        "outputId": "ee22f9e2-6d20-40e2-ee45-42e9040ffb4f"
      },
      "source": [
        "dense_output = dense(pooler_output).to(device)\n",
        "drop_output = dropout(dense_output).to(device)\n",
        "print(drop_output)\n",
        "fc_output = fc(drop_output).to(device)\n",
        "print(fc_output)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000, -0.0498,  0.1908,  ...,  0.1056, -0.1257, -0.0713],\n",
            "        [-0.1448,  0.0772,  0.0095,  ...,  0.3866,  0.1082, -0.3076]],\n",
            "       device='cuda:0', grad_fn=<FusedDropoutBackward>)\n",
            "tensor([[ 0.0202,  0.1299,  0.0990,  0.1591,  0.0502],\n",
            "        [ 0.0592, -0.0887,  0.1519, -0.0142, -0.1652]], device='cuda:0',\n",
            "       grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr8-Vc5cn0nY"
      },
      "source": [
        "# We use nn.CrossEntropyLoss() as our loss function. \n",
        "#fc_output = torch.cuda.FloatTensor(fc_output)\n",
        "#print(fc_output)\n",
        "#fc_output = fc_output.long()\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# labels = [1, 2, 3, 4, 5]\n",
        "# criterion(fc_output, torch.Tensor(labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WreWl_0KTY1X",
        "outputId": "48e96ce1-9c34-45b0-cf3d-eda12c3e352d"
      },
      "source": [
        "# FROM HUGGINGFACE BETO TUTORIAL\n",
        "\n",
        "text = \"[CLS] Para solucionar los [MASK] de Chile, el presidente debe [MASK] de inmediato. [SEP]\"\n",
        "masked_indxs = (4,11)\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "predictions = bert(tokens_tensor)[0]\n",
        "\n",
        "for i,midx in enumerate(masked_indxs):\n",
        "    idxs = torch.argsort(predictions[0,midx], descending=True)\n",
        "    predicted_token = tokenizer.convert_ids_to_tokens(idxs[:5])\n",
        "    print('MASK',i,':',predicted_token)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MASK 0 : ['[unused400]', '[unused244]', '[unused678]', '[unused282]', '[unused599]']\n",
            "MASK 1 : ['[unused282]', '[unused504]', '[unused145]', '[unused546]', '[unused749]']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}