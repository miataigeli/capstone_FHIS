{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOVZVUsTjnuMpeBR7Afq7bi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miataigeli/capstone_FHIS/blob/darya/bert_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xKLzIOjIkBE"
      },
      "source": [
        "## BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q-um0cGSk2X"
      },
      "source": [
        "Based on tutorial here: https://www.youtube.com/watch?v=mw7ay38--ak"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj0W0YOdIPi1",
        "outputId": "be95241f-3485-43da-da5d-40d66f72b043"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 46.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o-wxqMdS5Od"
      },
      "source": [
        "import transformers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ak550KxaIpl1"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IInqOgwdI5lb"
      },
      "source": [
        "#specify GPU\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS80BGDmKUIS",
        "outputId": "7d4b7b58-dbe6-46aa-b6ab-54cf04b4f107"
      },
      "source": [
        "#connect to my drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m7XORvdJGiw"
      },
      "source": [
        "### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VnN6jmTLkdJ",
        "outputId": "4eb43ba3-194b-40fd-cde1-34d7e6e1a635"
      },
      "source": [
        "# Read in all json files into one dataframe\n",
        "import os\n",
        "\n",
        "corpus_dir = \"/content/drive/MyDrive/capstone/corpus\"\n",
        "corpus_df = pd.DataFrame([], columns = ['content', 'level'])\n",
        "\n",
        "for filename in os.listdir(corpus_dir):\n",
        "    if filename.endswith(\".json\"): \n",
        "         file_path = os.path.join(corpus_dir, filename)\n",
        "         df = pd.read_json(file_path)\n",
        "         df = df.drop(columns=['source', 'author', 'title'])\n",
        "         corpus_df = pd.concat([corpus_df, df])\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "print(corpus_df.describe())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                                  content level\n",
            "count                                                 308   308\n",
            "unique                                                308     5\n",
            "top     Señor, porque sé que habréis placer de la gran...    A1\n",
            "freq                                                    1    94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYx2n0zMRHcc",
        "outputId": "8026585d-fad5-427e-8243-8c11a6e66b29"
      },
      "source": [
        "corpus_df['level'].value_counts(normalize = True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "A1    0.305195\n",
              "B     0.288961\n",
              "A2    0.201299\n",
              "B1    0.136364\n",
              "B2    0.068182\n",
              "Name: level, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vab74dGRXHR"
      },
      "source": [
        "### Split into train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekaasyTsJHsy"
      },
      "source": [
        "train_text, test_text, train_labels, test_labels = train_test_split(df['content'], df['level'],\n",
        "                                                                    random_state = 2021,\n",
        "                                                                    test_size = 0.3) #did not include stratify\n",
        "\n",
        "# split test into validation and test\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(test_text, test_labels,\n",
        "                                                                random_state = 2021,\n",
        "                                                                test_size=0.5)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4I_iHm7SRwQ"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT8Ns18DSTFO",
        "outputId": "eb28aa0f-b99b-484e-d8eb-579f9b7cc4a1"
      },
      "source": [
        "# import BERT-based pretrained model\n",
        "#bert = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "bert_model = AutoModel.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')#, output_hidden_states=True, output_attentions=True).to(device)\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "#tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased', return_tensors='pt')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNCyCyBRSvCL",
        "outputId": "3f59e75f-d9d4-4d91-e2fc-c04a7a24e6f8"
      },
      "source": [
        "#sample data\n",
        "text = ['Me llamo Darya', 'vamos a probar un modelo de red neuronal.']\n",
        "\n",
        "#encode text\n",
        "tokenized_texts = tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False, return_tensors='pt')\n",
        "print(tokenized_texts)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[    4,  1094,  5592,  1785,  1742,     5,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    4,  1441,  1012,  6909,  1044,  4209,  1009,  2946, 12212,  7592,\n",
            "          1008,     5]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4PDQaamkadX",
        "outputId": "3c51362c-61ee-4e4f-fc5f-47eed2f62c6f"
      },
      "source": [
        "#input_ids = [tokenizer.convert_tokens_to_ids(x, return_tensors='pt') for x in tokenized_texts]\n",
        "#print(input_ids)\n",
        "outputs = bert_model(tokenized_texts['input_ids'])\n",
        "print(outputs.keys())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "odict_keys(['last_hidden_state', 'pooler_output'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOVxxnqUmPbL"
      },
      "source": [
        "last_hidden_state = outputs['last_hidden_state']\n",
        "pooler_output = outputs['pooler_output']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WreWl_0KTY1X",
        "outputId": "48e96ce1-9c34-45b0-cf3d-eda12c3e352d"
      },
      "source": [
        "# Now test it\n",
        "\n",
        "text = \"[CLS] Para solucionar los [MASK] de Chile, el presidente debe [MASK] de inmediato. [SEP]\"\n",
        "masked_indxs = (4,11)\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "predictions = bert(tokens_tensor)[0]\n",
        "\n",
        "for i,midx in enumerate(masked_indxs):\n",
        "    idxs = torch.argsort(predictions[0,midx], descending=True)\n",
        "    predicted_token = tokenizer.convert_ids_to_tokens(idxs[:5])\n",
        "    print('MASK',i,':',predicted_token)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MASK 0 : ['[unused400]', '[unused244]', '[unused678]', '[unused282]', '[unused599]']\n",
            "MASK 1 : ['[unused282]', '[unused504]', '[unused145]', '[unused546]', '[unused749]']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}