{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra en Espanol Transcript Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textract in c:\\users\\dasha\\miniconda3\\lib\\site-packages (1.6.3)\n",
      "Requirement already satisfied: xlrd==1.2.0 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from textract) (1.2.0)\n",
      "Requirement already satisfied: extract-msg==0.23.1 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from textract) (0.23.1)\n",
      "Requirement already satisfied: python-pptx==0.6.18 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from textract) (0.6.18)\n",
      "Requirement already satisfied: six==1.12.0 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from textract) (1.12.0)\n",
      "Requirement already satisfied: chardet==3.0.4 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from textract) (3.0.4)\n",
      "Requirement already satisfied: argcomplete==1.10.0 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from textract) (1.10.0)\n",
      "Requirement already satisfied: pdfminer.six==20181108 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from textract) (20181108)\n",
      "Requirement already satisfied: beautifulsoup4==4.8.0 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from textract) (4.8.0)\n",
      "Requirement already satisfied: EbookLib==0.17.1 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from textract) (0.17.1)\n",
      "Requirement already satisfied: docx2txt==0.8 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from textract) (0.8)\n",
      "Requirement already satisfied: SpeechRecognition==3.8.1 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from textract) (3.8.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from beautifulsoup4==4.8.0->textract) (2.0.1)\n",
      "Requirement already satisfied: lxml in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from EbookLib==0.17.1->textract) (4.6.2)\n",
      "Requirement already satisfied: tzlocal==1.5.1 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from extract-msg==0.23.1->textract) (1.5.1)\n",
      "Requirement already satisfied: imapclient==2.1.0 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from extract-msg==0.23.1->textract) (2.1.0)\n",
      "Requirement already satisfied: olefile==0.46 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from extract-msg==0.23.1->textract) (0.46)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from pdfminer.six==20181108->textract) (2.3.0)\n",
      "Requirement already satisfied: pycryptodome in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from pdfminer.six==20181108->textract) (3.10.1)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from python-pptx==0.6.18->textract) (1.4.3)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from python-pptx==0.6.18->textract) (8.1.0)\n",
      "Requirement already satisfied: pytz in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from tzlocal==1.5.1->extract-msg==0.23.1->textract) (2020.5)\n"
     ]
    }
   ],
   "source": [
    "# Installing package to extract text from a doc file\n",
    "!pip install textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir = \"C:/Users/dasha/Documents/Education/UBC/Capstone/Extra en Espanol/\"\n",
    "filenames = ['01 La llegada de Sam.docx',\n",
    "              '02 Sam va de compras.doc',\n",
    "              '03 Sam aprende a ligar.doc',\n",
    "              '04 Sam busca un trabajo.doc',\n",
    "              '05 Ha nacido una estrella.docx',\n",
    "              '06 El día de la primitiva.doc',\n",
    "              '07 La gemela.doc',\n",
    "              '08 La prima de la dueña.doc',\n",
    "              '09 Trabajos para los chicos.doc',\n",
    "              '10 ANA PROTESTA.doc',\n",
    "              '11 Tiempo de vacaciones.doc',\n",
    "              '12 Fanáticos del fútbol.doc',\n",
    "              '13 Boda en el aire.doc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some functions to aid in preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A set of functions that help clean up the text\n",
    "\n",
    "def remove_upto(unwanted_string, text):\n",
    "    '''This function removes all lines in the text up to and including the line \n",
    "       that contains unwanted_string. \n",
    "    '''\n",
    "    unwanted_idx = text.index(unwanted_string)\n",
    "    end_of_unwanted_line = text[unwanted_idx+len(unwanted_string):].index('\\n') + unwanted_idx+len(unwanted_string)\n",
    "    \n",
    "    return text[end_of_unwanted_line+1:]\n",
    "\n",
    "def remove_all_lines(unwanted_string, text):\n",
    "    ''' This function removes all lines that contain unwanted_string. '''\n",
    "    # Find the unwanted lines\n",
    "    while(unwanted_string in text):\n",
    "        unwanted_idx = text.index(unwanted_string)\n",
    "        end_of_unwanted_line = text[unwanted_idx+len(unwanted_string):].index('\\n') + unwanted_idx+len(unwanted_string)\n",
    "        text = text[:unwanted_idx] + text[end_of_unwanted_line+1:]\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in and Clean Up Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_dir = 'C:/Users/dasha/Documents/Education/UBC/Capstone/capstone_FHIS/text/'\n",
    "text_list = []\n",
    "for idx, filename in enumerate(filenames):\n",
    "    text = textract.process(files_dir+filename).decode('utf-8')\n",
    "    # Extract title from filename\n",
    "    title_start = filenames[idx].index(' ')+1\n",
    "    title_end = filenames[idx].index('.')\n",
    "    title = filenames[idx][title_start:title_end]\n",
    "    \n",
    "    # Remove extra space in Episode 5\n",
    "    if idx == 4: #Episode 5\n",
    "        text_lines = text.split('\\n')\n",
    "        stripped_lines = []\n",
    "        for line in text_lines:\n",
    "            line = line.strip()\n",
    "            stripped_lines.append(line)\n",
    "        text = \"\\n\".join(stripped_lines)\n",
    "    \n",
    "    # Clean up issues common to all texts\n",
    "    # First, remove the word 'Transcripts' from the text\n",
    "    if 'Transcripts' in text:\n",
    "        text = remove_upto('Transcripts', text)\n",
    "        \n",
    "    # Remove vertical lines\n",
    "    if '|' in text:\n",
    "        text = text.replace('|', '')\n",
    "        \n",
    "    # Remove horizontal lines\n",
    "    if '_________________________' in text:\n",
    "        text = text.replace('_________________________\\n', '')\n",
    "    if '—–' in text:\n",
    "        text = text.replace('—–\\n', '')\n",
    "    \n",
    "    # Remove episode name from top - sometimes this comes after the word 'Episode' or 'Episodio',\n",
    "    # sometimes it is on a line by itself\n",
    "    # We need to search for both because in some files, the title after 'Episodio' is not the same as the one in the filename\n",
    "    \n",
    "    if 'Episodio' in text:\n",
    "        text = remove_upto('Episodio', text)\n",
    "    if 'Episode' in text:\n",
    "        text = remove_upto('Episode', text)\n",
    "    if title in text:\n",
    "        text = remove_upto(title, text)\n",
    "        \n",
    "    if 'SCENE' in text:\n",
    "        text = remove_all_lines('SCENE', text)\n",
    "    if 'INTERTITIAL' in text:\n",
    "        text = remove_all_lines('INTERTITIAL', text)\n",
    "    if 'INTERSTITIAL' in text:\n",
    "        text = remove_all_lines('INTERSTITIAL', text)\n",
    "    if 'INTERSITIAL' in text:\n",
    "        text = remove_all_lines('INTERSITIAL', text)    \n",
    "        \n",
    "    for word in re.findall('\\n[0-9]{1,2}', text):\n",
    "        text = text.replace(word, '')\n",
    "        \n",
    "    text = text.strip()\n",
    "    with open(text_dir + filename[:filename.index('.')] + \".txt\", 'w') as text_file:\n",
    "        text_file.write(text)\n",
    "    text_list.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2878\n",
      "41221\n"
     ]
    }
   ],
   "source": [
    "# Check number of sentences and number of tokens\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "total_sents = 0\n",
    "total_tokens = 0\n",
    "for text in text_list:\n",
    "    doc = nlp(text)\n",
    "    sents_list = []\n",
    "    for sent in doc.sents:\n",
    "        sents_list.append(sent.text)\n",
    "    total_sents += len(sents_list)\n",
    "    total_tokens += len(doc)\n",
    "    \n",
    "print(total_sents)\n",
    "print(total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON files\n",
    "from collections import defaultdict\n",
    "dict_list = []\n",
    "author = \"Channel 4 Learning\" #the author is the producer in this case\n",
    "level = \"A1\"\n",
    "source_list = ['https://www.dropbox.com/s/8ia5r3wfsmobg07/01%20La%20llegada%20de%20Sam.docx?dl=0',\n",
    "              'https://www.dropbox.com/s/x92adnk4cp3u93p/02%20Sam%20va%20de%20compras.doc',\n",
    "              'https://www.dropbox.com/s/fzf6avuqttlj5kr/03%20Sam%20aprende%20a%20ligar.doc',\n",
    "              'https://www.dropbox.com/s/uah40240xw7fxnr/04%20Sam%20busca%20un%20trabajo.doc',\n",
    "              'https://www.dropbox.com/s/c290fxn2kyaiyc0/05%20Ha%20nacido%20una%20estrella.docx?dl=0',\n",
    "              'https://www.dropbox.com/s/07ro7dqj3qlj69r/06%20El%20d%C3%ADa%20de%20la%20primitiva.doc',\n",
    "              'https://www.dropbox.com/s/u0o32nmnxxdje3l/07%20La%20gemela.doc',\n",
    "              'https://www.dropbox.com/s/4socaaai36edz9n/08%20La%20prima%20de%20la%20due%C3%B1a.doc',\n",
    "              'https://www.dropbox.com/s/nyyv08m6amnxh39/09%20Trabajos%20para%20los%20chicos.doc',\n",
    "              'https://www.dropbox.com/s/lgr4jod4sap55ej/10%20ANA%20PROTESTA.doc',\n",
    "              'https://www.dropbox.com/s/xsk5dzi48qseez4/11%20Tiempo%20de%20vacaciones.doc',\n",
    "              'https://www.dropbox.com/s/lkpbqgzj8ba3zan/12%20Fan%C3%A1ticos%20del%20f%C3%BAtbol.doc',\n",
    "              'https://www.dropbox.com/s/59g7w7r30xnuczn/13%20Boda%20en%20el%20aire.doc']\n",
    "\n",
    "for idx, text in enumerate(text_list):\n",
    "    # metadata\n",
    "    # source\n",
    "    source = source_list[idx]\n",
    "    \n",
    "    # Title\n",
    "    title_start = filenames[idx].index(' ')+1\n",
    "    title_end = filenames[idx].index('.')\n",
    "    title = filenames[idx][title_start:title_end]\n",
    "    \n",
    "    # content\n",
    "    content = text\n",
    "    # make dictionary\n",
    "    text_dict = defaultdict(str)\n",
    "    text_dict['author'] = author\n",
    "    text_dict['source'] = source\n",
    "    text_dict['level'] = level\n",
    "    text_dict['title'] = title\n",
    "    text_dict['content'] = content\n",
    "    # append dictionary to list of dictionaries\n",
    "    dict_list.append(text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json_dir = \"C:/Users/dasha/Documents/Education/UBC/Capstone/capstone_FHIS/corpus/\"\n",
    "with open(json_dir + 'extra_en_espanol.json', 'w') as outfile:\n",
    "    json.dump(dict_list, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
