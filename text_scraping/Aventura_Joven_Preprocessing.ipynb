{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aventura Joven Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OotNOM4KF5QT"
      },
      "source": [
        "## Pipeline for Extracting Text from Aventura Joven Books"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBSWME1K9R17",
        "outputId": "7a58a5f3-ab93-4f5f-b9ea-bd157a2fc6e0"
      },
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAjmp5XmRYTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a3818df-56d0-405d-df76-8575ec8076be"
      },
      "source": [
        "# Install the pdf reader\n",
        "!pip install tika"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tika\n",
            "  Downloading https://files.pythonhosted.org/packages/96/07/244fbb9c74c0de8a3745cc9f3f496077a29f6418c7cbd90d68fd799574cb/tika-1.24.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tika) (56.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (3.0.4)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-cp37-none-any.whl size=32885 sha256=d89f08dde39068c79f2a1cef27f8dac5de83be88f619e13a794e10b1e72c65c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/9c/f5/0b1b738442fc2a2862bef95b908b374f8e80215550fb2a8975\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-1.24\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdhA9xqkGd6j"
      },
      "source": [
        "files_dir = \"/content/drive/MyDrive/capstone/\"\n",
        "filenames = ['Aventura Joven 01 - Persecucion - Elvira Sancho, Jordi Suris.pdf',\n",
        "              'Aventura Joven 02 - Misterio en - Elvira Sancho, Jordi Suris.pdf',\n",
        "              'Aventura Joven 03 - Perdidos en - Elvira Sancho, Jordi Suris.pdf',\n",
        "              'Aventura Joven 04 - La chica de - Elvira Sancho, Jordi Suris.pdf',\n",
        "              'Aventura Joven 05 - El fantasma - Elvira Sancho, Jordi Suris.pdf',\n",
        "              'Aventura Joven 06 - El monstruo - Elvira Sancho, Jordi Suris.pdf']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L51tR8Hl5ANj"
      },
      "source": [
        "#### Parse the PDF text and Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEZYkcy19Jft"
      },
      "source": [
        "from tika import parser  \n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "data_list = []\n",
        "text_orig_list = []\n",
        "text_proc_list = []\n",
        "for filename in filenames:\n",
        "  parsed_pdf = parser.from_file(files_dir+filename) \n",
        "  data = parsed_pdf['content'] \n",
        "  data_list.append(data)\n",
        "\n",
        "  # Find beginning and end of text\n",
        "  beg_idx = data.lower().index('capítulo')\n",
        "  end_idx = data.lower().index('después de la lectura\\n')\n",
        "  text = data[beg_idx:end_idx]\n",
        "  text_orig_list.append(text)\n",
        "\n",
        "  # write the initial text to a file (no preprocessing done here)\n",
        "  with open(files_dir+filename[:17]+' Text No Preproc.txt', 'w') as f:\n",
        "    f.write(text)\n",
        "\n",
        "  # preprocess text\n",
        "  # remove words with numbers in them, ex altura1 or hostal2 (footnote indicator)\n",
        "  text_wo_ft_words = text\n",
        "  for word in re.findall('[-a-zA-ZÀ-ÖØ-öø-ÿ]+»?,?!?\\.{0,3}[1234567890]{1,2}', text_wo_ft_words):\n",
        "    word_idx = text_wo_ft_words.index(word)\n",
        "    m = re.search('[-a-zA-ZÀ-ÖØ-öø-ÿ]+»?,?!?\\.{0,3}', word)\n",
        "    stripped_word = m.group(0)\n",
        "    text_wo_ft_words = text_wo_ft_words[:word_idx] + stripped_word + text_wo_ft_words[word_idx+len(word):]\n",
        "\n",
        "  # remove words that contain '-\\n' because they didn't fit on one line\n",
        "  text_wo_broken_words = text_wo_ft_words\n",
        "  for word in re.findall('[-a-zA-ZÀ-ÖØ-öø-ÿ]+\\-\\n{1,2}[-a-zØ-öø-ÿ]+', text_wo_broken_words):\n",
        "    word_idx = text_wo_broken_words.index(word)\n",
        "    hyphen_idx = word.index('-')\n",
        "    if '-\\n\\n' in word:\n",
        "      len_sep = 3\n",
        "    else:\n",
        "      len_sep = 2\n",
        "    modified_word = word[:hyphen_idx] + word[hyphen_idx+len_sep:]\n",
        "    text_wo_broken_words = text_wo_broken_words[:word_idx] + modified_word + text_wo_broken_words[word_idx+len(word):]\n",
        "\n",
        "  # remove page numbers\n",
        "  text_wo_page_nums = text_wo_broken_words\n",
        "  for word in re.findall('\\n[0-9]{1,2}\\n', text_wo_page_nums):\n",
        "    word_idx = text_wo_page_nums.index(word)\n",
        "    m = re.search('[0-9]{1,2}', word)\n",
        "    text_wo_page_nums = text_wo_page_nums[:word_idx] + '\\n' + text_wo_page_nums[word_idx+len(word):]\n",
        "  \n",
        "  # remove unnecessary newline breaks\n",
        "  text_wo_sent_breaks = text_wo_page_nums\n",
        "  for word in re.findall('[-a-zA-ZÀ-ÖØ-öø-ÿ]+ ?\\n\\n[-a-zA-ZÀ-ÖØ-öø-ÿ]+', text_wo_sent_breaks):\n",
        "    word_idx = text_wo_sent_breaks.index(word)\n",
        "    newline_idx = word.index('\\n\\n')\n",
        "    modified_word = word[:newline_idx] + word[newline_idx+2:]\n",
        "    text_wo_sent_breaks = text_wo_sent_breaks[:word_idx] + modified_word + text_wo_sent_breaks[word_idx+len(word):]\n",
        "  \n",
        "  text_proc_list.append(text_wo_sent_breaks)\n",
        "  \n",
        "  with open(files_dir+ filename[:17] + ' Text.txt', 'w') as f:\n",
        "    f.write(text_wo_sent_breaks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7ISTx-RP3VH"
      },
      "source": [
        "#### Separate chapters and create JSON objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azRKxpK6P2pM"
      },
      "source": [
        "from collections import defaultdict\n",
        "dict_list = []\n",
        "author = \"Elvira Sancho, Jordi Suris\"\n",
        "for book_idx, (data, text) in enumerate(zip(data_list,text_proc_list)):\n",
        "  # metadata\n",
        "  # ISBN\n",
        "  isbn_index = data_list[book_idx].index('ISBN')\n",
        "  newline_after_isbn = data_list[book_idx][isbn_index+5:].index('\\n\\n') + isbn_index+5\n",
        "  source = data_list[book_idx][isbn_index+6:newline_after_isbn]\n",
        "\n",
        "  # Level\n",
        "  if book_idx <= 4:\n",
        "    level = \"A1\"\n",
        "  else:\n",
        "    level= \"A2\"\n",
        "\n",
        "  # Title\n",
        "  title_index = data_list[book_idx].index('Título')\n",
        "  newline_after_title = data_list[book_idx][title_index+7:].index('\\n\\n') + title_index+7\n",
        "  title = data_list[book_idx][title_index+8:newline_after_title]\n",
        "  content = text\n",
        "\n",
        "  # separate by chapters\n",
        "  chapter_indices = defaultdict(list)\n",
        "  text_chap = text\n",
        "  j = 0\n",
        "  cur_data_chap = 0\n",
        "  while 'capítulo' in text_chap.lower():\n",
        "    chapter_index = text_chap.lower().index('capítulo')\n",
        "    newline_after_chapter_index = text_chap[chapter_index+8:].index('\\n') + chapter_index+8\n",
        "    text_chap = text_chap[newline_after_chapter_index:]\n",
        "    if 'capítulo' in text_chap.lower():\n",
        "      end_chap_idx = text_chap.lower().index('capítulo')\n",
        "    else:\n",
        "      end_chap_idx = len(text)\n",
        "    chapter_indices[str(j+1)] = [chapter_index + cur_data_chap, chapter_index+end_chap_idx+cur_data_chap+10]\n",
        "    cur_data_chap += newline_after_chapter_index\n",
        "    j += 1\n",
        "    \n",
        "  # make a dictionary per chapter\n",
        "  for chap, chap_indices in chapter_indices.items():\n",
        "    chapter_text = text[chap_indices[0]: chap_indices[1]]\n",
        "    chap_dict = defaultdict(str)\n",
        "    chap_dict['author'] = author\n",
        "    chap_dict['source'] = source\n",
        "    chap_dict['level'] = level\n",
        "    chap_dict['title'] = title + \", \" + chap\n",
        "    #chap_dict['chapter'] = chap\n",
        "    chap_dict['content'] = chapter_text\n",
        "    dict_list.append(chap_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-Dt6DSDiRtP"
      },
      "source": [
        "import json\n",
        "with open(files_dir + 'aventura.json', 'w') as outfile:\n",
        "    json.dump(dict_list, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFbzF87zNso0"
      },
      "source": [
        "Attempt at Removing Footnote Definitions (Incomplete)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvCM5gUsNf6v"
      },
      "source": [
        "# def modify_words(regex_find, regex_modify, text, modify):\n",
        "#   ''' Modifies all words in text that match the regex_find regex to match the regex_modify regex. '''\n",
        "#   text_removed = text\n",
        "#   for word in re.findall(regex_find, text_removed):\n",
        "#     word_idx = text_removed.index(word)\n",
        "    \n",
        "#     if modify == 'regex':\n",
        "#       m = re.search(regex_modify, word)\n",
        "#       modified_word = m.group(0)\n",
        "#     elif modify == 'remove':\n",
        "#       rm_char_idx = word.index(regex_modify)\n",
        "#       modified_word = word[:rm_char_idx] + word[rm_char_idx+len(regex_modify):]\n",
        "    \n",
        "#     text_removed = text_removed[:word_idx] + modified_word + text_removed[word_idx+len(word):]\n",
        "\n",
        "#   return text_removed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STSN5IHiQOD8"
      },
      "source": [
        "#text_wo_broken_words = modify_words('[-a-zA-ZÀ-ÖØ-öø-ÿ]+\\-\\n{1,2}[-a-zØ-öø-ÿ]+', '-\\n', text_wo_ft_words, 'remove')\n",
        "#print(text_wo_broken_words[800:2000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASOHuL-vEhak"
      },
      "source": [
        "# Attempt at removing the footnotes\n",
        "import re\n",
        "\n",
        "# normalize the data to ignore special characters in Spanish\n",
        "#norm_data = unicodedata.normalize('NFD', data).encode('ascii', 'ignore').decode('utf-8')\n",
        "#norm_text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode('utf-8')\n",
        "\n",
        "# m = re.search('[A-z]+[1234567890]{1,2}', norm_data[1880:])\n",
        "\n",
        "# m.group(0)\n",
        "# footnoted_words = [word.strip() for word in re.findall('[A-z]+[1234567890]', norm_data[1880:])]\n",
        "# footnote_defs = [word.strip() for word in re.findall('[0-9]{1,2}  [A-z]+[\\s\\w]*:[\\w \\n]*\\.', norm_data[1880:])]\n",
        "\n",
        "#print(footnoted_words)\n",
        "#print(footnote_defs)\n",
        "all_matches = re.findall('([0-9]{1,2}  [A-z]+: ((([A-z]|[0-9]|,)+ ?)+\\n))', '''1  altura: Cusco esta a unos 3500 metros de altura, lo que en algunas personas \n",
        "provoca el llamado mal de altura o soroche. Los sintomas del mal de altura \n",
        "son: dolor de cabeza, mareos, trastornos estomacales y cansancio. Puede com-\n",
        "batirse con pastillas, ejercicios de respiracion o mate.\n",
        "\n",
        "2  hostal: alojamiento normalmente mas barato y personal que un hotel, aunque \n",
        "sin sus comodidades. La palabra se utiliza principalmente en medios rurales. \n",
        "''')\n",
        "print(len(all_matches))\n",
        "print(all_matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDZxXbjyF33T"
      },
      "source": [
        ""
      ]
    }
  ]
}