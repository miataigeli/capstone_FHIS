{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OotNOM4KF5QT"
   },
   "source": [
    "## Pipeline for Extracting Text from Aventura Joven Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBSWME1K9R17",
    "outputId": "7a58a5f3-ab93-4f5f-b9ea-bd157a2fc6e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gAjmp5XmRYTB",
    "outputId": "2a3818df-56d0-405d-df76-8575ec8076be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tika\n",
      "  Downloading https://files.pythonhosted.org/packages/96/07/244fbb9c74c0de8a3745cc9f3f496077a29f6418c7cbd90d68fd799574cb/tika-1.24.tar.gz\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tika) (56.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tika) (2.23.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tika) (3.0.4)\n",
      "Building wheels for collected packages: tika\n",
      "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tika: filename=tika-1.24-cp37-none-any.whl size=32885 sha256=d89f08dde39068c79f2a1cef27f8dac5de83be88f619e13a794e10b1e72c65c9\n",
      "  Stored in directory: /root/.cache/pip/wheels/73/9c/f5/0b1b738442fc2a2862bef95b908b374f8e80215550fb2a8975\n",
      "Successfully built tika\n",
      "Installing collected packages: tika\n",
      "Successfully installed tika-1.24\n"
     ]
    }
   ],
   "source": [
    "# Install the pdf reader\n",
    "!pip install tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qdhA9xqkGd6j"
   },
   "outputs": [],
   "source": [
    "files_dir = \"/content/drive/MyDrive/capstone/\"\n",
    "filenames = ['Aventura Joven 01 - Persecucion - Elvira Sancho, Jordi Suris.pdf',\n",
    "              'Aventura Joven 02 - Misterio en - Elvira Sancho, Jordi Suris.pdf',\n",
    "              'Aventura Joven 03 - Perdidos en - Elvira Sancho, Jordi Suris.pdf',\n",
    "              'Aventura Joven 04 - La chica de - Elvira Sancho, Jordi Suris.pdf',\n",
    "              'Aventura Joven 05 - El fantasma - Elvira Sancho, Jordi Suris.pdf',\n",
    "              'Aventura Joven 06 - El monstruo - Elvira Sancho, Jordi Suris.pdf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L51tR8Hl5ANj"
   },
   "source": [
    "#### Parse the PDF text and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEZYkcy19Jft"
   },
   "outputs": [],
   "source": [
    "from tika import parser  \n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "data_list = []\n",
    "text_orig_list = []\n",
    "text_proc_list = []\n",
    "for filename in filenames:\n",
    "  parsed_pdf = parser.from_file(files_dir+filename) \n",
    "  data = parsed_pdf['content'] \n",
    "  data_list.append(data)\n",
    "\n",
    "  # Find beginning and end of text\n",
    "  beg_idx = data.lower().index('capítulo')\n",
    "  end_idx = data.lower().index('después de la lectura\\n')\n",
    "  text = data[beg_idx:end_idx]\n",
    "  text_orig_list.append(text)\n",
    "\n",
    "  # write the initial text to a file (no preprocessing done here)\n",
    "  with open(files_dir+filename[:17]+' Text No Preproc.txt', 'w') as f:\n",
    "    f.write(text)\n",
    "\n",
    "  # preprocess text\n",
    "  # remove words with numbers in them, ex altura1 or hostal2 (footnote indicator)\n",
    "  text_wo_ft_words = text\n",
    "  for word in re.findall('[-a-zA-ZÀ-ÖØ-öø-ÿ]+»?,?!?\\.{0,3}[1234567890]{1,2}', text_wo_ft_words):\n",
    "    word_idx = text_wo_ft_words.index(word)\n",
    "    m = re.search('[-a-zA-ZÀ-ÖØ-öø-ÿ]+»?,?!?\\.{0,3}', word)\n",
    "    stripped_word = m.group(0)\n",
    "    text_wo_ft_words = text_wo_ft_words[:word_idx] + stripped_word + text_wo_ft_words[word_idx+len(word):]\n",
    "\n",
    "  # remove words that contain '-\\n' because they didn't fit on one line\n",
    "  text_wo_broken_words = text_wo_ft_words\n",
    "  for word in re.findall('[-a-zA-ZÀ-ÖØ-öø-ÿ]+\\-\\n{1,2}[-a-zØ-öø-ÿ]+', text_wo_broken_words):\n",
    "    word_idx = text_wo_broken_words.index(word)\n",
    "    hyphen_idx = word.index('-')\n",
    "    if '-\\n\\n' in word:\n",
    "      len_sep = 3\n",
    "    else:\n",
    "      len_sep = 2\n",
    "    modified_word = word[:hyphen_idx] + word[hyphen_idx+len_sep:]\n",
    "    text_wo_broken_words = text_wo_broken_words[:word_idx] + modified_word + text_wo_broken_words[word_idx+len(word):]\n",
    "\n",
    "  # remove page numbers\n",
    "  text_wo_page_nums = text_wo_broken_words\n",
    "  for word in re.findall('\\n[0-9]{1,2}\\n', text_wo_page_nums):\n",
    "    word_idx = text_wo_page_nums.index(word)\n",
    "    m = re.search('[0-9]{1,2}', word)\n",
    "    text_wo_page_nums = text_wo_page_nums[:word_idx] + '\\n' + text_wo_page_nums[word_idx+len(word):]\n",
    "  \n",
    "  # remove unnecessary newline breaks\n",
    "  text_wo_sent_breaks = text_wo_page_nums\n",
    "  for word in re.findall('[-a-zA-ZÀ-ÖØ-öø-ÿ]+ ?\\n\\n[-a-zA-ZÀ-ÖØ-öø-ÿ]+', text_wo_sent_breaks):\n",
    "    word_idx = text_wo_sent_breaks.index(word)\n",
    "    newline_idx = word.index('\\n\\n')\n",
    "    modified_word = word[:newline_idx] + word[newline_idx+2:]\n",
    "    text_wo_sent_breaks = text_wo_sent_breaks[:word_idx] + modified_word + text_wo_sent_breaks[word_idx+len(word):]\n",
    "  \n",
    "  text_proc_list.append(text_wo_sent_breaks)\n",
    "  \n",
    "  with open(files_dir+ filename[:17] + ' Text.txt', 'w') as f:\n",
    "    f.write(text_wo_sent_breaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7ISTx-RP3VH"
   },
   "source": [
    "#### Separate chapters and create JSON objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azRKxpK6P2pM"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dict_list = []\n",
    "author = \"Elvira Sancho, Jordi Suris\"\n",
    "for book_idx, (data, text) in enumerate(zip(data_list,text_proc_list)):\n",
    "  # metadata\n",
    "  # ISBN\n",
    "  isbn_index = data_list[book_idx].index('ISBN')\n",
    "  newline_after_isbn = data_list[book_idx][isbn_index+5:].index('\\n\\n') + isbn_index+5\n",
    "  source = data_list[book_idx][isbn_index+6:newline_after_isbn]\n",
    "\n",
    "  # Level\n",
    "  if book_idx <= 4:\n",
    "    level = \"A1\"\n",
    "  else:\n",
    "    level= \"A2\"\n",
    "\n",
    "  # Title\n",
    "  title_index = data_list[book_idx].index('Título')\n",
    "  newline_after_title = data_list[book_idx][title_index+7:].index('\\n\\n') + title_index+7\n",
    "  title = data_list[book_idx][title_index+8:newline_after_title]\n",
    "  content = text\n",
    "\n",
    "  # separate by chapters\n",
    "  chapter_indices = defaultdict(list)\n",
    "  text_chap = text\n",
    "  j = 0\n",
    "  cur_data_chap = 0\n",
    "  while 'capítulo' in text_chap.lower():\n",
    "    chapter_index = text_chap.lower().index('capítulo')\n",
    "    newline_after_chapter_index = text_chap[chapter_index+8:].index('\\n') + chapter_index+8\n",
    "    text_chap = text_chap[newline_after_chapter_index:]\n",
    "    if 'capítulo' in text_chap.lower():\n",
    "      end_chap_idx = text_chap.lower().index('capítulo')\n",
    "    else:\n",
    "      end_chap_idx = len(text)\n",
    "    chapter_indices[str(j+1)] = [chapter_index + cur_data_chap, chapter_index+end_chap_idx+cur_data_chap+10]\n",
    "    cur_data_chap += newline_after_chapter_index\n",
    "    j += 1\n",
    "    \n",
    "  # make a dictionary per chapter\n",
    "  for chap, chap_indices in chapter_indices.items():\n",
    "    chapter_text = text[chap_indices[0]: chap_indices[1]]\n",
    "    chap_dict = defaultdict(str)\n",
    "    chap_dict['author'] = author\n",
    "    chap_dict['source'] = source\n",
    "    chap_dict['level'] = level\n",
    "    chap_dict['title'] = title + \", \" + chap\n",
    "    #chap_dict['chapter'] = chap\n",
    "    chap_dict['content'] = chapter_text\n",
    "    dict_list.append(chap_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R-Dt6DSDiRtP"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open(files_dir + 'aventura.json', 'w') as outfile:\n",
    "    json.dump(dict_list, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFbzF87zNso0"
   },
   "source": [
    "Attempt at Removing Footnote Definitions (Incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvCM5gUsNf6v"
   },
   "outputs": [],
   "source": [
    "# def modify_words(regex_find, regex_modify, text, modify):\n",
    "#   ''' Modifies all words in text that match the regex_find regex to match the regex_modify regex. '''\n",
    "#   text_removed = text\n",
    "#   for word in re.findall(regex_find, text_removed):\n",
    "#     word_idx = text_removed.index(word)\n",
    "    \n",
    "#     if modify == 'regex':\n",
    "#       m = re.search(regex_modify, word)\n",
    "#       modified_word = m.group(0)\n",
    "#     elif modify == 'remove':\n",
    "#       rm_char_idx = word.index(regex_modify)\n",
    "#       modified_word = word[:rm_char_idx] + word[rm_char_idx+len(regex_modify):]\n",
    "    \n",
    "#     text_removed = text_removed[:word_idx] + modified_word + text_removed[word_idx+len(word):]\n",
    "\n",
    "#   return text_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STSN5IHiQOD8"
   },
   "outputs": [],
   "source": [
    "#text_wo_broken_words = modify_words('[-a-zA-ZÀ-ÖØ-öø-ÿ]+\\-\\n{1,2}[-a-zØ-öø-ÿ]+', '-\\n', text_wo_ft_words, 'remove')\n",
    "#print(text_wo_broken_words[800:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASOHuL-vEhak"
   },
   "outputs": [],
   "source": [
    "# Attempt at removing the footnotes\n",
    "import re\n",
    "\n",
    "# normalize the data to ignore special characters in Spanish\n",
    "#norm_data = unicodedata.normalize('NFD', data).encode('ascii', 'ignore').decode('utf-8')\n",
    "#norm_text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "# m = re.search('[A-z]+[1234567890]{1,2}', norm_data[1880:])\n",
    "\n",
    "# m.group(0)\n",
    "# footnoted_words = [word.strip() for word in re.findall('[A-z]+[1234567890]', norm_data[1880:])]\n",
    "# footnote_defs = [word.strip() for word in re.findall('[0-9]{1,2}  [A-z]+[\\s\\w]*:[\\w \\n]*\\.', norm_data[1880:])]\n",
    "\n",
    "#print(footnoted_words)\n",
    "#print(footnote_defs)\n",
    "all_matches = re.findall('([0-9]{1,2}  [A-z]+: ((([A-z]|[0-9]|,)+ ?)+\\n))', '''1  altura: Cusco esta a unos 3500 metros de altura, lo que en algunas personas \n",
    "provoca el llamado mal de altura o soroche. Los sintomas del mal de altura \n",
    "son: dolor de cabeza, mareos, trastornos estomacales y cansancio. Puede com-\n",
    "batirse con pastillas, ejercicios de respiracion o mate.\n",
    "\n",
    "2  hostal: alojamiento normalmente mas barato y personal que un hotel, aunque \n",
    "sin sus comodidades. La palabra se utiliza principalmente en medios rurales. \n",
    "''')\n",
    "print(len(all_matches))\n",
    "print(all_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDZxXbjyF33T"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Aventura Joven Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
