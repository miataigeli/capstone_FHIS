{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "relative-front",
   "metadata": {},
   "source": [
    "## Statistics about the scraped corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-survivor",
   "metadata": {},
   "source": [
    "For each level and for the corpus overall:\n",
    "* Number of sentences\n",
    "* Number of tokens\n",
    "* Number of types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "advanced-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_corpus\n",
    "import nltk\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "sublime-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "significant-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_dict = {}\n",
    "tokens_types_dict = {}\n",
    "for level, list_of_texts in corpus.items():\n",
    "    stats_per_text = []\n",
    "    tt_per_text = []\n",
    "    for text in list_of_texts:\n",
    "        num_sents = len(nltk.tokenize.sent_tokenize(text[\"content\"]))\n",
    "        tokens = nltk.wordpunct_tokenize(text[\"content\"])\n",
    "        num_tokens = len(tokens)\n",
    "        types = set(tokens)\n",
    "        num_types = len(types)\n",
    "        stats_per_text.append(\n",
    "            {\n",
    "                \"num_sents\": num_sents,\n",
    "                \"num_tokens\": num_tokens,\n",
    "                \"num_types\": num_types,\n",
    "            }\n",
    "        )\n",
    "        tt_per_text.append(\n",
    "            {\n",
    "                \"tokens\": tokens,\n",
    "                \"types\": types,\n",
    "            }\n",
    "        )\n",
    "    stat_dict[level] = stats_per_text\n",
    "    tokens_types_dict[level] = tt_per_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "imported-romania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1\n",
      "{'num_sents': 6, 'num_tokens': 134, 'num_types': 83}\n",
      "{'num_sents': 7, 'num_tokens': 257, 'num_types': 133}\n",
      "{'num_sents': 6, 'num_tokens': 206, 'num_types': 118}\n",
      "{'num_sents': 4, 'num_tokens': 103, 'num_types': 67}\n",
      "{'num_sents': 155, 'num_tokens': 1631, 'num_types': 526}\n",
      "\n",
      "A2\n",
      "{'num_sents': 35, 'num_tokens': 2153, 'num_types': 757}\n",
      "{'num_sents': 42, 'num_tokens': 2588, 'num_types': 887}\n",
      "{'num_sents': 44, 'num_tokens': 2733, 'num_types': 875}\n",
      "{'num_sents': 75, 'num_tokens': 2974, 'num_types': 949}\n",
      "{'num_sents': 40, 'num_tokens': 1940, 'num_types': 665}\n",
      "\n",
      "B\n",
      "{'num_sents': 67, 'num_tokens': 1951, 'num_types': 564}\n",
      "{'num_sents': 60, 'num_tokens': 3139, 'num_types': 843}\n",
      "{'num_sents': 33, 'num_tokens': 3163, 'num_types': 766}\n",
      "{'num_sents': 15, 'num_tokens': 242, 'num_types': 136}\n",
      "{'num_sents': 164, 'num_tokens': 5147, 'num_types': 1437}\n",
      "\n",
      "B1\n",
      "{'num_sents': 108, 'num_tokens': 1915, 'num_types': 629}\n",
      "{'num_sents': 62, 'num_tokens': 1797, 'num_types': 590}\n",
      "{'num_sents': 21, 'num_tokens': 1269, 'num_types': 485}\n",
      "{'num_sents': 24, 'num_tokens': 733, 'num_types': 328}\n",
      "{'num_sents': 62, 'num_tokens': 2035, 'num_types': 744}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for level, l in stat_dict.items():\n",
    "    print(level)\n",
    "    for i, d in enumerate(l):\n",
    "        if i == 5:\n",
    "            break\n",
    "        print(d)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-interference",
   "metadata": {},
   "source": [
    "## Aggregate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "american-broadcasting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most and least sentences in any document in the corpus:\n",
      "\tMost: 554,\tLeast: 1\n",
      "Most and least tokens in any document in the corpus:\n",
      "\tMost: 5470,\tLeast: 12\n",
      "Most and least types in any document in the corpus:\n",
      "\tMost: 1696,\tLeast: 10\n",
      "\n",
      "Breakdown of total sentences, tokens and types by reading level:\n",
      "{'A1': {'total_sents': 7258, 'total_tokens': 74484, 'total_types': 6559},\n",
      " 'A2': {'total_sents': 2805, 'total_tokens': 53883, 'total_types': 8241},\n",
      " 'B': {'total_sents': 3183, 'total_tokens': 79013, 'total_types': 13787},\n",
      " 'B1': {'total_sents': 1837, 'total_tokens': 40892, 'total_types': 7548}}\n",
      "\n",
      "Total number of sentences in corpus: 15083\n",
      "Total number of tokens in corpus: 248272\n",
      "Total number of types in corpus: 23751\n"
     ]
    }
   ],
   "source": [
    "least_sents = 10000\n",
    "least_tokens = 10000\n",
    "least_types = 10000\n",
    "most_sents = 0\n",
    "most_tokens = 0\n",
    "most_types = 0\n",
    "total_sents = 0\n",
    "total_tokens = 0\n",
    "breakdown_by_level = {}\n",
    "for level, stat_list in stat_dict.items():\n",
    "    level_sents = 0\n",
    "    level_tokens = 0\n",
    "    for stats in stat_list:\n",
    "        n_sents = stats[\"num_sents\"]\n",
    "        n_tokens = stats[\"num_tokens\"]\n",
    "        n_types = stats[\"num_types\"]\n",
    "        if n_sents < least_sents:\n",
    "            least_sents = n_sents\n",
    "        if n_tokens < least_tokens:\n",
    "            least_tokens = n_tokens\n",
    "        if n_types < least_types:\n",
    "            least_types = n_types\n",
    "        \n",
    "        if n_sents > most_sents:\n",
    "            most_sents = n_sents\n",
    "        if n_tokens > most_tokens:\n",
    "            most_tokens = n_tokens\n",
    "        if n_types > most_types:\n",
    "            most_types = n_types\n",
    "        \n",
    "        level_sents += n_sents\n",
    "        level_tokens += n_tokens\n",
    "        total_sents += n_sents\n",
    "        total_tokens += n_tokens\n",
    "        \n",
    "    breakdown_by_level[level] = {\n",
    "        \"total_sents\": level_sents,\n",
    "        \"total_tokens\": level_tokens,\n",
    "    }\n",
    "\n",
    "total_types = set()\n",
    "for level, tt_list in tokens_types_dict.items():\n",
    "    level_types = set()\n",
    "    for tt in tt_list:\n",
    "        types = tt[\"types\"]\n",
    "        level_types |= types\n",
    "        total_types |= types\n",
    "    breakdown_by_level[level][\"total_types\"] = len(level_types)\n",
    "total_types = len(total_types)        \n",
    "\n",
    "print(f\"Most and least sentences in any document in the corpus:\\n\\tMost: {most_sents},\\tLeast: {least_sents}\")\n",
    "print(f\"Most and least tokens in any document in the corpus:\\n\\tMost: {most_tokens},\\tLeast: {least_tokens}\")\n",
    "print(f\"Most and least types in any document in the corpus:\\n\\tMost: {most_types},\\tLeast: {least_types}\")\n",
    "\n",
    "print(\"\\nBreakdown of total sentences, tokens and types by reading level:\")\n",
    "pprint.pprint(breakdown_by_level)\n",
    "\n",
    "print(f\"\\nTotal number of sentences in corpus: {total_sents}\")\n",
    "print(f\"Total number of tokens in corpus: {total_tokens}\")\n",
    "print(f\"Total number of types in corpus: {total_types}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
