{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "identified-praise",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw to /Users/eun-\n",
      "[nltk_data]     youngchristinapark/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils import text_processor, read_corpus\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import altair as alt\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "nltk.download('omw')\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-vinyl",
   "metadata": {},
   "source": [
    "### Set up the Spanish Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "choice-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_root = '/Users/eun-youngchristinapark/MDS-CAPSTONE/wn-mcr-transform/wordnet_spa'\n",
    "wncr = nltk.corpus.reader.wordnet.WordNetCorpusReader(result_root, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-analysis",
   "metadata": {},
   "source": [
    "### Set up the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bearing-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dir = '/Users/eun-youngchristinapark/MDS-CAPSTONE/capstone_FHIS/corpus/'\n",
    "file_list = os.listdir(text_dir)\n",
    "corpus = read_corpus(text_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "assured-worth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Me llamo María Pérez y tengo diecinueve años. Nací en Málaga, pero vivo en Granada. Soy estudiante de primer curso de Periodismo. De lunes a viernes me levanto a las siete y media, desayuno y camino hasta la universidad. Entro en clase a las nueve y salgo a la una. Al medio día, como en mi casa y veo la televisión. Por la tarde, estudio hasta las siete y después quedo con mis amigas. A nosotras nos gusta mucho el cine, el teatro y la música. Los viernes por la noche cenamos pizza y bailamos en la discoteca. Todos los sábados visito a mi familia en Málaga. El domingo por la tarde regreso a Granada y, si hace sol, salgo con mi perro a dar un paseo. ¡Me encantan los animales!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_item = corpus['A1'][0]\n",
    "corpus_item['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-apple",
   "metadata": {},
   "source": [
    "### Pronoun Density\n",
    "\n",
    "Darya will work on this so ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "documentary-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Voy a la escuela. Voy a la escuela el lunes, el martes, el miércoles, el jueves y el viernes.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "described-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pronoun_density(text):\n",
    "    '''\n",
    "    This function returns the density of pronoun in the text. \n",
    "    -----------------------------------------\n",
    "    Argument: text (str) - a string of text\n",
    "    Returns: density of pronoun in the text as (number of pronouns in the text / number of tokens in the text)\n",
    "    '''\n",
    "    \n",
    "    num_tokens, num_pron = 0, 0\n",
    "    tp = text_processor(text)\n",
    "    for sent in tp.tags:\n",
    "        num_pron += sent.count('PRON')\n",
    "        num_tokens += len(sent)\n",
    "    return num_pron / num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "administrative-passage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pronoun_density(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-gibraltar",
   "metadata": {},
   "source": [
    "### Spanish Connectives\n",
    "\n",
    "Darya will workon this so ignore this part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "absent-argument",
   "metadata": {},
   "outputs": [],
   "source": [
    "addition = set(['y', 'además', 'también', 'incluso'])\n",
    "opposition = set(['pero', 'aunque', 'sin embargo', 'no obstante', 'a pesar de que', 'a pesar de'])\n",
    "cause = set(['porque', 'ya que', 'puesto que', 'debido a que', 'a causa de que', 'como'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-target",
   "metadata": {},
   "source": [
    "### Noun Phrase Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "circular-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = corpus_item['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "outer-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = text_processor(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-intranet",
   "metadata": {},
   "source": [
    "### Degree of Abstraction (Hypernym levels)\n",
    "\n",
    "This feature is based one of the measures suggested in the paper: \"Coh-Metrix: Analysis of text on cohesion and language\". \n",
    "The authors of the paper suggest that the degree of abstraction can be measured by hypernym values in WordNet. Words having many hypernym levels tend to be more concrete, whereas those with few hupernym levels tend to be more abstract. The authors calculated mean values of hypernym are computed for the words to measure the abstractness of the text. \n",
    "\n",
    "We calculate two measures for Spanish tokens in Spanish text. One is the average degree of abstraction calculated as the average number of levels for the tokens in the text. \n",
    "The other is the min number of hypernym levels of the tokens in the text. This is determined by the most abstract word in the text. \n",
    "\n",
    "Note that abstractness of a text is inversely proportional to the degree of abstraction (number of hypernyms in the WordNet). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "floating-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "def degree_of_abstraction(text):\n",
    "    ''' This function measures degree of abstraction of a text by measuring the distance of its nouns to the top level in the wordnet.\n",
    "    ------------------------------------------------\n",
    "    Argument: text (str)\n",
    "            \n",
    "    Returns: the average degree of abstraction (the higher, less abstract) and the min degree of abstraction in the text \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    tp = text_processor(text)\n",
    "    top_synset = wncr.synset('entidad.n.01')           # Top synset\n",
    "    sent_nouns, sent_levels = [], []\n",
    "    num_levels, num_nouns = 0.0, 0\n",
    "    \n",
    "    for i_sent, sent in enumerate(tp.tokens):         # For each sentence        \n",
    "        for i_token, token in enumerate(sent):              # For each token \n",
    "            token_levels, num_senses = 0.0, 0                    # calculate levels for eacn sense of the token \n",
    "            tag = tp.tags[i_sent][i_token]\n",
    "            token = token.lower()\n",
    "            synsets = wncr.synsets(token)\n",
    "            if len(synsets) > 0 and tag == 'NOUN': \n",
    "                for synset in synsets:\n",
    "                    if synset.name().split('.')[1] == 'n': # only process noun\n",
    "                        try:\n",
    "                            levels = 1/synset.path_similarity(top_synset)\n",
    "                            token_levels += levels\n",
    "                            num_senses += 1\n",
    "                        except:\n",
    "                            pass\n",
    "                if num_senses > 0:    \n",
    "                    num_nouns += 1 \n",
    "                    sent_nouns.append(token)\n",
    "                    sent_levels.append(token_levels/num_senses)\n",
    "                    num_levels += token_levels/num_senses            # average level over the senses \n",
    "                \n",
    "    \n",
    "    if num_nouns == 0:\n",
    "        return 1000                                                  # no abstraction \n",
    "    else:\n",
    "        return num_levels/ num_nouns, min(sent_levels)               # first returns the average number of levels in the text, second returns the minimum num of levels in the text  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designing-inquiry",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "#### 1. Boundary Cases\n",
    "\n",
    "These cases ensure that the implementationl handles the boundary cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "personal-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''   # empty strinbg\n",
    "assert degree_of_abstraction(text) == 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "handled-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '1.' # numerical value \n",
    "assert degree_of_abstraction(text) == 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "duplicate-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Maria'  # proper noun \n",
    "assert degree_of_abstraction(text) == 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-glory",
   "metadata": {},
   "source": [
    "#### 2. Brute Force vs. Implementation\n",
    "\n",
    "This section shows the details of each text. It calculates each noun in the text and shows the number of levels to the top synset and the average number of levels.\n",
    "Then, it shows the degree of abstraction calculated using brute force approach and the results of implementation.\n",
    "If they are equal, we can be assured of the correct implementation. \n",
    "\n",
    "Please note that we print out all the details to make debugging easy (in case there are bugs). Therefore, the codes in this section are not necessarily the most efficient because the purpose of this section is to make potential errors conspicuous. \n",
    "\n",
    "##### 2.1)  A very simple case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "arranged-going",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "escuela\n",
      "8.0\n",
      "6.0\n",
      "8.0\n",
      "9.0\n",
      "average: 7.75 \n",
      "\n",
      "Bruce force: 7.75\n",
      "Implementation: 7.75\n",
      "\n",
      "Bruce force: 7.75\n",
      "Implementation: 7.75\n"
     ]
    }
   ],
   "source": [
    "top_synset = wncr.synset('entidad.n.01')\n",
    "text = 'Voy a la escuela.'\n",
    "\n",
    "tp = text_processor(text)\n",
    "text_nouns = []\n",
    "text_levels = []\n",
    "\n",
    "\n",
    "for i_sent, sent in enumerate(tp.tags):\n",
    "    for i_tag, tag in enumerate(sent):\n",
    "        if tag == 'NOUN':\n",
    "            token_level = 0\n",
    "            num_senses = 0\n",
    "            print(tp.tokens[i_sent][i_tag])\n",
    "            \n",
    "            synsets = wncr.synsets(tp.tokens[i_sent][i_tag])\n",
    "            for synset in synsets:\n",
    "                if synset.name().split('.')[1]=='n': # noun\n",
    "                    try:\n",
    "                        token_level += 1.0/synset.path_similarity(top_synset)\n",
    "                        num_senses += 1\n",
    "                        print(1.0/synset.path_similarity(top_synset))\n",
    "                    except:\n",
    "                        pass\n",
    "            if num_senses > 0:\n",
    "                text_nouns.append(tp.tokens[i_sent][i_tag])\n",
    "                text_levels.append(token_level/num_senses)        \n",
    "                print(f'average: {token_level/num_senses} \\n')\n",
    "\n",
    "avg, min_level = degree_of_abstraction(text)\n",
    "print(f'Bruce force: {np.mean(text_levels)}')\n",
    "print(f'Implementation: {avg}\\n')\n",
    "\n",
    "print(f'Bruce force: {min(text_levels)}')\n",
    "print(f'Implementation: {min_level}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "narrow-maine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "6.0\n",
      "8.0\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "wncr.synsets('escuela')\n",
    "print(1/wncr.synsets('escuela')[0].path_similarity(top_synset))\n",
    "print(1/wncr.synsets('escuela')[1].path_similarity(top_synset))\n",
    "print(1/wncr.synsets('escuela')[2].path_similarity(top_synset))\n",
    "print(1/wncr.synsets('escuela')[3].path_similarity(top_synset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-florist",
   "metadata": {},
   "source": [
    "##### 2.2) Moderate cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "spectacular-entity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me llamo María Pérez y tengo diecinueve años. Nací en Málaga, pero vivo en Granada. Soy estudiante de primer curso de Periodismo. De lunes a viernes me levanto a las siete y media, desayuno y camino hasta la universidad. Entro en clase a las nueve y salgo a la una. Al medio día, como en mi casa y veo la televisión. Por la tarde, estudio hasta las siete y después quedo con mis amigas. A nosotras nos gusta mucho el cine, el teatro y la música. Los viernes por la noche cenamos pizza y bailamos en la discoteca. Todos los sábados visito a mi familia en Málaga. El domingo por la tarde regreso a Granada y, si hace sol, salgo con mi perro a dar un paseo. ¡Me encantan los animales! \n",
      "\n",
      "años\n",
      "6.0\n",
      "6.0\n",
      "6.0\n",
      "6.0\n",
      "average: 6.0 \n",
      "\n",
      "estudiante\n",
      "6.0\n",
      "average: 6.0 \n",
      "\n",
      "curso\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "6.0\n",
      "6.0\n",
      "5.0\n",
      "average: 6.777777777777778 \n",
      "\n",
      "lunes\n",
      "9.0\n",
      "average: 9.0 \n",
      "\n",
      "viernes\n",
      "9.0\n",
      "average: 9.0 \n",
      "\n",
      "desayuno\n",
      "8.0\n",
      "6.0\n",
      "average: 7.0 \n",
      "\n",
      "camino\n",
      "7.0\n",
      "11.0\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "6.0\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "5.0\n",
      "average: 7.25 \n",
      "\n",
      "universidad\n",
      "8.0\n",
      "6.0\n",
      "8.0\n",
      "average: 7.333333333333333 \n",
      "\n",
      "clase\n",
      "8.0\n",
      "11.0\n",
      "11.0\n",
      "9.0\n",
      "6.0\n",
      "9.0\n",
      "10.0\n",
      "5.0\n",
      "5.0\n",
      "6.0\n",
      "6.0\n",
      "average: 7.818181818181818 \n",
      "\n",
      "nueve\n",
      "8.0\n",
      "average: 8.0 \n",
      "\n",
      "día\n",
      "7.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "average: 6.166666666666667 \n",
      "\n",
      "casa\n",
      "9.0\n",
      "8.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "5.0\n",
      "9.0\n",
      "7.0\n",
      "average: 7.818181818181818 \n",
      "\n",
      "televisión\n",
      "10.0\n",
      "11.0\n",
      "10.0\n",
      "average: 10.333333333333334 \n",
      "\n",
      "tarde\n",
      "7.0\n",
      "average: 7.0 \n",
      "\n",
      "estudio\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "8.0\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "9.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "average: 8.416666666666666 \n",
      "\n",
      "amigas\n",
      "6.0\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "average: 6.25 \n",
      "\n",
      "cine\n",
      "9.0\n",
      "8.0\n",
      "average: 8.5 \n",
      "\n",
      "teatro\n",
      "9.0\n",
      "11.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "average: 8.166666666666666 \n",
      "\n",
      "música\n",
      "7.0\n",
      "10.0\n",
      "5.0\n",
      "average: 7.333333333333333 \n",
      "\n",
      "viernes\n",
      "9.0\n",
      "average: 9.0 \n",
      "\n",
      "noche\n",
      "10.0\n",
      "7.0\n",
      "5.0\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "6.0\n",
      "11.0\n",
      "average: 7.25 \n",
      "\n",
      "pizza\n",
      "discoteca\n",
      "10.0\n",
      "average: 10.0 \n",
      "\n",
      "sábados\n",
      "9.0\n",
      "average: 9.0 \n",
      "\n",
      "familia\n",
      "5.0\n",
      "6.0\n",
      "8.0\n",
      "5.0\n",
      "7.0\n",
      "6.0\n",
      "average: 6.166666666666667 \n",
      "\n",
      "domingo\n",
      "9.0\n",
      "average: 9.0 \n",
      "\n",
      "tarde\n",
      "7.0\n",
      "average: 7.0 \n",
      "\n",
      "regreso\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "average: 8.333333333333334 \n",
      "\n",
      "sol\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "average: 7.5 \n",
      "\n",
      "perro\n",
      "9.0\n",
      "7.0\n",
      "average: 8.0 \n",
      "\n",
      "paseo\n",
      "11.0\n",
      "10.0\n",
      "11.0\n",
      "11.0\n",
      "12.0\n",
      "8.0\n",
      "8.0\n",
      "average: 10.142857142857142 \n",
      "\n",
      "animales\n",
      "7.0\n",
      "average: 7.0 \n",
      "\n",
      "Bruce force: 7.824419308290277\n",
      "Implementation: 7.824419308290276\n",
      "\n",
      "Bruce force: 6.0\n",
      "Implementation: 6.0\n"
     ]
    }
   ],
   "source": [
    "text = corpus['A1'][0]['content']\n",
    "print(text, '\\n')\n",
    "\n",
    "tp = text_processor(text)\n",
    "text_nouns = []\n",
    "text_levels = []\n",
    "\n",
    "\n",
    "for i_sent, sent in enumerate(tp.tags):\n",
    "    for i_tag, tag in enumerate(sent):\n",
    "        if tag == 'NOUN':\n",
    "            token_level = 0\n",
    "            num_senses = 0\n",
    "            print(tp.tokens[i_sent][i_tag])\n",
    "            \n",
    "            synsets = wncr.synsets(tp.tokens[i_sent][i_tag])\n",
    "            for synset in synsets:\n",
    "                if synset.name().split('.')[1]=='n': # noun\n",
    "                    try:\n",
    "                        token_level += 1.0/synset.path_similarity(top_synset)\n",
    "                        num_senses += 1\n",
    "                        print(1.0/synset.path_similarity(top_synset))\n",
    "                    except:\n",
    "                        pass\n",
    "            if num_senses > 0:\n",
    "                text_nouns.append(tp.tokens[i_sent][i_tag])\n",
    "                text_levels.append(token_level/num_senses)        \n",
    "                print(f'average: {token_level/num_senses} \\n')\n",
    "\n",
    "avg, min_level = degree_of_abstraction(text)\n",
    "print(f'Bruce force: {np.mean(text_levels)}')\n",
    "print(f'Implementation: {avg}\\n')\n",
    "\n",
    "print(f'Bruce force: {min(text_levels)}')\n",
    "print(f'Implementation: {min_level}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "adolescent-technique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capítulo I \n",
      "Que trata de la condición y ejercicio del famoso\n",
      "hi dalgo don Quijote de la Mancha  \n",
      " de cuyo nombre no \n",
      "quiero acordarme, no ha mucho tiempo que vivía un hidalgo \n",
      "de los de lanza en astillero, adarga antigua, rocín flaco y galgo \n",
      "corredor. Una olla de algo más vaca que carnero, salpicón las \n",
      "más noches, duelos y quebrantos los sábados, lentejas los \n",
      "viernes, algún palomino de añadidura los domingos, consumían \n",
      "las tres partes de su hacienda. El resto della concluían sayo \n",
      "de velarte, calzas de velludo para las fiestas, con sus pantu-\n",
      "flos de lo mesmo, y los días de entresemana se honraba consu \n",
      "vellorí de lo más fino. Tenía en su casa una ama que pasaba  \n",
      "de los cuarenta, y una sobrina que no llegaba a los veinte, y \n",
      "un mozo de campo y plaza, que así ensillaba el rocín como \n",
      "tomaba la podadera. Frisaba la edad de nuestro hidalgo con los  \n",
      "cincuenta años; era de complexión recia, seco de carnes, enjuto de \n",
      "rostro, gran madrugador y amigo de la caza. Quieren decir que tenía el \n",
      "sobrenombre \n",
      "de Quijada o \n",
      "Quesada, que en \n",
      "esto hay alguna  \n",
      "diferencia en los\n",
      "autores que deste \n",
      "caso escriben; \n",
      "aunque, por\n",
      "conjeturas vero-\n",
      "símiles, se deja \n",
      "entender que se \n",
      "lla maba Quejana. \n",
      "Pero esto importa\n",
      "poco a nuestro  \n",
      "cuento; basta \n",
      "que en la narra-\n",
      "ción dél no se \n",
      "salga un punto \n",
      "de la verdad.  Es, pues, de saber que este sobredicho hidalgo, los ratos que \n",
      "estaba ocioso, que eran los más del año, se daba a leer libros de \n",
      "caballerías, con tanta afición y gusto, que olvidó casi de todo \n",
      "punto el ejercicio de la caza y aun la administración de su       \n",
      "hacienda; y llegó a tanto su curiosidad y desatino en esto, que \n",
      "vendió muchas hanegas de tierra de sembradura para  \n",
      "comprar libros de caballerías en que leer, y así, llevó a su casa  \n",
      "todos cuantos pudo haber dellos; y de todos, ningunos le parecían \n",
      "tan bien como los que compuso el famoso Feliciano de Silva, porque \n",
      "la claridad de su prosa y aquellas entricadas razones suyas le parecían \n",
      "de perlas, y más cuando llegaba a leer aquellos requiebros y cartas \n",
      "de desafíos, donde en muchas partes hallaba escrito: La razón de la \n",
      "sinrazón que a mi razón se hace, de tal manera mi razón enflaquece, que \n",
      "con razón me quejo de la vuestra fermosura*. Y también cuando leía; \n",
      "…los altos cielos que de vuestra divinidad divinamente con las estrellas os \n",
      "fortifican y os hacen merecedora del merecimiento que merece la vuestra  \n",
      "grandeza. \n",
      " Con estas razones perdía el pobre caballero el juicio, \n",
      "y desvelábase por entenderlas y desentrañarles el sentido,  \n",
      "que no se lo sacara ni las entendiera el mesmo Aristóteles, si     \n",
      "resucitara para sólo ello. No estaba muy bien con las heridas \n",
      "que don Belianís daba y recebía, porque se imaginaba que, por  \n",
      "grandes maestros que le hubiesen curado, no dejaría de tener  \n",
      "el rostro y todo el cuerpo lleno de cicatrices y señales. Pero, \n",
      "con todo, alababa en su autor aquel acabar su libro con la   \n",
      "promesa de aquella inacabable aventura, y muchas veces le vino \n",
      "deseo de tomar la pluma y dalle fin al pie de la letra, como allí \n",
      "se promete; y sin duda alguna lo hiciera, y aun saliera con ello, \n",
      "si otros mayores y continuos pensamientos no se lo estorbaran. \n",
      "Tuvo muchas veces competencia con el cura de su lugar —que \n",
      "era hombre docto, graduado en Sigüenza—, sobre cuál había \n",
      "sido mejor caballero; Palmerín de Inglaterra o Amadís de \n",
      "Gaula; mas maese Nicolás, barbero del mesmo pueblo, decía \n",
      "que     ninguno llegaba al Caballero del Febo, y que si alguno se le  \n",
      "podía comparar, era don Galaor, hermano de Amadís de Gaula, \n",
      "porque tenía muy acomodada condición para todo; que no eracaballero melindroso, ni tan llorón como su hermano, y que en lo \n",
      "de la valentía no le iba en zaga. \n",
      " En resolución, él se enfrascó tanto en su lectura, que se \n",
      "le pasaban las noches leyendo de claro en claro, y los días de     \n",
      "turbio en turbio; y así, del poco dormir y del mucho leer, se  \n",
      "le secó el cerebro, de manera que vino a perder el juicio.      \n",
      "Llenósele la fantasía de todo aquello que leía en los libros, así de \n",
      "encantamientos como de pendencias, batallas, desafíos, heridas, \n",
      "requiebros, amores, tormentas y disparates imposibles; y \n",
      "asentósele de tal modo en la imaginación que era verdad toda \n",
      "aquella máquina de aquellas sonadas soñadas invenciones que \n",
      "leía, que para él no había otra historia más cierta en el mundo. \n",
      "Decía él que el Cid Ruy Díaz había sido muy buen caballero,   \n",
      "pero que no tenía que ver con el Caballero de la Ardiente \n",
      "Espada, que de sólo un revés había partido por medio dos  \n",
      "fieros y descomunales gigantes. Mejor estaba con Bernardo \n",
      "del Carpio, porque en Roncesvalles había muerto a Roldán, el \n",
      "encantado, valiéndose de la industria de Hércules, cuando ahogó \n",
      "a Anteo, el hijo de la Tierra, entre los brazos. Decía mucho bien \n",
      "del gigante Morgante, porque, con ser de aquella generación  \n",
      "gigantea, que todos son soberbios y descomedidos, él solo  \n",
      "era afable y bien criado. Pero, sobre todos, estaba bien con   \n",
      "Reinaldos de Montalbán, y más cuando le veía salir de su castillo \n",
      "y robar cuantos topaba, y cuando en allende robó aquel ídolo  \n",
      "de Mahoma que era todo de oro, según dice su historia. Diera él, \n",
      "por dar una mano de coces al traidor de Galalón, al ama que tenía \n",
      "y aun a su sobrina de añadidura. \n",
      " En efecto, rematado ya su juicio, vino a dar en el más       \n",
      "extraño pensamiento que jamás dio loco en el mundo; y fue \n",
      "que le pareció convenible y necesario, así para el aumento de \n",
      "su honra como para el servicio de su república, hacerse caballero \n",
      "andante, e irse por todo el mundo con sus armas y caballo \n",
      "a buscar las aventuras y a ejercitarse en todo aquello que él  \n",
      "había leído que los caballeros andantes se ejercitaban, deshaciendo todo género de agravio, y poniéndose en ocasiones y peligros \n",
      "donde, acabándolos, cobrase eterno nombre y fama Imaginábase el \n",
      "pobre ya coronado por el valor de su brazo, por lo menos, del imperio \n",
      "de Trapisonda; y así, con estos tan agradables pensamientos, llevado \n",
      "del extraño gusto que en ellos sentía, se dio priesa a poner en efeto \n",
      "lo que deseaba. \n",
      " Y lo primero que hizo fue limpiar unas armas que habían \n",
      "sido de sus bisabuelos, que, tomadas de orín y llenas de moho, \n",
      "luengos siglos había que estaban puestas y olvidadas en un \n",
      "rincón. Limpiólas y aderezólas lo mejor que pudo, pero \n",
      "vio que tenían una gran falta, y era que no tenían celada de        \n",
      "encaje, sino morrión simple (era solamente media celada);  \n",
      "mas a esto suplió su industria, porque de cartones hizo un  \n",
      "modo de media celada (una visera), que, encajada con el         \n",
      "morrión, hacían una apariencia de celada entera. Es verdad que  \n",
      "para probar si era fuerte y podía estar al riesgo de una  \n",
      "cuchillada, sacó su espada y le dio dos golpes, y con el primero  \n",
      "y en un punto deshizo lo que había hecho en una semana; y no \n",
      "dejó de parecerle mal la facilidad con que la había hecho       \n",
      "pedazos, y, por asegurarse deste peligro, la tornó a hacer de nuevo, \n",
      "poniéndole unas barras de hierro por de dentro, de tal manera \n",
      "que él quedó satisfecho de su fortaleza; y, sin querer hacer nueva \n",
      "experiencia della, la diputó y tuvo por celada finísima de encaje. \n",
      " Fue luego a ver su rocín, y, aunque tenía más cuartos que un \n",
      "real y más tachas que el caballo de Gonela, que tantum pellis et \n",
      "ossa fuit, le pareció que ni el Bucéfalo de Alejandro ni Babieca el \n",
      "del Cid con él se igualaban. Cuatro días se le pasaron en imaginar \n",
      "qué nombre le pondría; porque, según se decía él a sí mesmo, no \n",
      "era razón que caballo de caballero tan famoso, y tan bueno él por sí, \n",
      "estuviese sin nombre conocido; y ansí, procuraba acomodársele de \n",
      "manera que declarase quién había sido, antes que fuese de caballero \n",
      "andante, y lo que era  entonces; pues estaba muy puesto en razón que, \n",
      "mudando su señor estado, mudase él también el nombre, y cobrase   \n",
      "famoso y de estruendo, como convenía a la nueva orden y al nuevo \n",
      "ejercicio que ya profesaba. Y así, después de muchos nombres  \n",
      "que formó, borró y quitó, añadió, deshizo y tornó a hacer en   \n",
      "su memoria e imaginación, al fin le vino a llamar Rocinante, \n",
      "nombre, a su parecer, alto, sonoro y significativo de lo que  \n",
      "había sido cuando fue rocín, antes de lo que ahora era, que era \n",
      "antes y primero de todos los rocines del mundo. Puesto nombre, y tan a su gusto, a su caballo, quiso  \n",
      "ponérsele a sí mismo, y en este pensamiento duró otros ocho  \n",
      "días, y al cabo se vino a llamar don Quijote; de donde, como  \n",
      "queda dicho, tomaron ocasión los autores desta tan verdadera \n",
      "historia que, sin duda, se debía de llamar Quijada, y no Quesada, \n",
      "como otros quisieron decir. Pero, acordándose que el valeroso  \n",
      "Amadís no sólo se había contentado con llamarse Amadís  \n",
      "a secas, sino que añadió el nombre de su reino y patria, por \n",
      "Hepila famosa, y se llamó Amadís de Gaula, así quiso, como \n",
      "buen caballero, añadir al suyo el nombre de la suya y llamarse \n",
      "don Quijote de la Mancha, con que, a su parecer, declaraba muy al  \n",
      "vivo su linaje y patria, y la honraba con tomar el sobrenombre  \n",
      "della. \n",
      " Limpias, pues, sus armas, hecho del morrión celada, \n",
      "puesto nombre a su rocín y confirmándose a sí mismo, se dio a \n",
      "entender que no le faltaba otra cosa sino buscar una dama de quien \n",
      "enamorarse; porque el caballero andante sin amores era árbol sin \n",
      "hojas y sin fruto y cuerpo sin alma. Decíase él: \n",
      " —Si yo, por malos de mis pecados, o por mi buena suerte, \n",
      "me encuentro por ahí con algún gigante, como de ordinario les  \n",
      "acontece a los caballeros andantes, y le derribo de un encuentro, o le \n",
      "parto por mitad del cuerpo, o, finalmente, le venzo y le rindo, \n",
      "rendido.\n",
      " —Yo, señora, soy el gigante Caraculiambro, señor de la \n",
      "ínsula Malindrania, a quien venció en singular batalla el jamás  \n",
      "como se debe alabado caballero don Quijote de la Mancha, el  \n",
      "cual me mandó que me presentase ante vuestra merced, para  \n",
      "que la vuestra grandeza disponga de mí a su talante. Capítulo 2  \n",
      " ¡Oh, cómo se holgó nuestro buen caballero cuando hubo \n",
      "hecho este discurso, y más cuando halló a quien dar nombre de \n",
      "su dama! Y fue, a lo que se cree, que en un lugar cerca del suyo \n",
      "había una moza labradora de muy buen parecer, de quien él un \n",
      "tiempo anduvo enamorado, aunque, según se entiende, ella jamás \n",
      "lo supo, ni le dio cata dello. Llamábase Aldonza Lorenzo, y a \n",
      "ésta le pareció ser bien darle título de señora de sus pensamientos; \n",
      "y, buscándole nombre que no desdijese mucho del suyo, y que \n",
      "tirase y se encaminase al de princesa y gran señora, vino a lla marla \n",
      "Dulcinea del Toboso, porque era natural del Toboso; nombre, a su \n",
      "parecer, músico y peregrino y significativo, como todos los demás \n",
      "que a él y a sus cosas había puesto. \n",
      "condición\n",
      "6.0\n",
      "9.0\n",
      "10.0\n",
      "6.0\n",
      "5.0\n",
      "5.0\n",
      "7.0\n",
      "average: 6.857142857142857 \n",
      "\n",
      "ejercicio\n",
      "10.0\n",
      "12.0\n",
      "8.0\n",
      "7.0\n",
      "average: 9.25 \n",
      "\n",
      "don\n",
      "7.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "average: 8.5 \n",
      "\n",
      "nombre\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "9.0\n",
      "average: 8.0 \n",
      "\n",
      "tiempo\n",
      "4.0\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "average: 5.75 \n",
      "\n",
      "hidalgo\n",
      "lanza\n",
      "astillero\n",
      "8.0\n",
      "average: 8.0 \n",
      "\n",
      "adarga\n",
      "corredor\n",
      "13.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "average: 8.428571428571429 \n",
      "\n",
      "olla\n",
      "vaca\n",
      "12.0\n",
      "18.0\n",
      "8.0\n",
      "average: 12.666666666666666 \n",
      "\n",
      "salpicón\n",
      "noches\n",
      "10.0\n",
      "7.0\n",
      "5.0\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "6.0\n",
      "11.0\n",
      "average: 7.25 \n",
      "\n",
      "duelos\n",
      "9.0\n",
      "8.0\n",
      "average: 8.5 \n",
      "\n",
      "quebrantos\n",
      "sábados\n",
      "9.0\n",
      "average: 9.0 \n",
      "\n",
      "lentejas\n",
      "viernes\n",
      "9.0\n",
      "average: 9.0 \n",
      "\n",
      "palomino\n",
      "14.0\n",
      "average: 14.0 \n",
      "\n",
      "añadidura\n",
      "domingos\n",
      "9.0\n",
      "average: 9.0 \n",
      "\n",
      "partes\n",
      "8.0\n",
      "4.0\n",
      "5.0\n",
      "11.0\n",
      "12.0\n",
      "8.0\n",
      "8.0\n",
      "9.0\n",
      "5.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "4.0\n",
      "9.0\n",
      "average: 6.944444444444445 \n",
      "\n",
      "hacienda\n",
      "8.0\n",
      "9.0\n",
      "13.0\n",
      "7.0\n",
      "6.0\n",
      "8.0\n",
      "8.0\n",
      "average: 8.428571428571429 \n",
      "\n",
      "resto\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "5.0\n",
      "average: 6.0 \n",
      "\n",
      "sayo\n",
      "calzas\n",
      "velludo\n",
      "fiestas\n",
      "9.0\n",
      "7.0\n",
      "7.0\n",
      "6.0\n",
      "average: 7.25 \n",
      "\n",
      "flos\n",
      "días\n",
      "7.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "average: 6.166666666666667 \n",
      "\n",
      "entresemana\n",
      "casa\n",
      "9.0\n",
      "8.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "5.0\n",
      "9.0\n",
      "7.0\n",
      "average: 7.818181818181818 \n",
      "\n",
      "ama\n",
      "8.0\n",
      "average: 8.0 \n",
      "\n",
      "sobrina\n",
      "7.0\n",
      "average: 7.0 \n",
      "\n",
      "mozo\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "6.0\n",
      "average: 6.8 \n",
      "\n",
      "campo\n",
      "9.0\n",
      "7.0\n",
      "8.0\n",
      "7.0\n",
      "11.0\n",
      "7.0\n",
      "6.0\n",
      "8.0\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "7.0\n",
      "7.0\n",
      "average: 7.6923076923076925 \n",
      "\n",
      "plaza\n",
      "9.0\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "average: 8.0 \n",
      "\n",
      "rocín\n",
      "podadera\n",
      "edad\n",
      "5.0\n",
      "7.0\n",
      "7.0\n",
      "average: 6.333333333333333 \n",
      "\n",
      "hidalgo\n",
      "años\n",
      "6.0\n",
      "6.0\n",
      "6.0\n",
      "6.0\n",
      "average: 6.0 \n",
      "\n",
      "complexión\n",
      "carnes\n",
      "7.0\n",
      "6.0\n",
      "8.0\n",
      "average: 7.0 \n",
      "\n",
      "enjuto\n",
      "rostro\n",
      "6.0\n",
      "8.0\n",
      "average: 7.0 \n",
      "\n",
      "madrugador\n",
      "amigo\n",
      "6.0\n",
      "6.0\n",
      "5.0\n",
      "6.0\n",
      "average: 5.75 \n",
      "\n",
      "caza\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "11.0\n",
      "8.0\n",
      "average: 9.166666666666666 \n",
      "\n",
      "sobrenombre\n",
      "7.0\n",
      "8.0\n",
      "average: 7.5 \n",
      "\n",
      "diferencia\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "6.0\n",
      "5.0\n",
      "average: 6.166666666666667 \n",
      "\n",
      "autores\n",
      "caso\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "average: 7.2 \n",
      "\n",
      "conjeturas\n",
      "6.0\n",
      "average: 6.0 \n",
      "\n",
      "cuento\n",
      "9.0\n",
      "8.0\n",
      "9.0\n",
      "average: 8.666666666666666 \n",
      "\n",
      "ción\n",
      "punto\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "6.0\n",
      "6.0\n",
      "6.0\n",
      "7.0\n",
      "10.0\n",
      "5.0\n",
      "6.0\n",
      "9.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "5.0\n",
      "4.0\n",
      "average: 7.117647058823529 \n",
      "\n",
      "verdad\n",
      "9.0\n",
      "7.0\n",
      "6.0\n",
      "7.0\n",
      "average: 7.25 \n",
      "\n",
      "hidalgo\n",
      "ratos\n",
      "año\n",
      "6.0\n",
      "6.0\n",
      "6.0\n",
      "average: 6.0 \n",
      "\n",
      "libros\n",
      "9.0\n",
      "8.0\n",
      "7.0\n",
      "10.0\n",
      "7.0\n",
      "6.0\n",
      "average: 7.833333333333333 \n",
      "\n",
      "caballerías\n",
      "8.0\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "average: 8.5 \n",
      "\n",
      "afición\n",
      "8.0\n",
      "9.0\n",
      "6.0\n",
      "7.0\n",
      "average: 7.5 \n",
      "\n",
      "gusto\n",
      "7.0\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "7.0\n",
      "6.0\n",
      "7.0\n",
      "average: 7.875 \n",
      "\n",
      "punto\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "6.0\n",
      "6.0\n",
      "6.0\n",
      "7.0\n",
      "10.0\n",
      "5.0\n",
      "6.0\n",
      "9.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "5.0\n",
      "4.0\n",
      "average: 7.117647058823529 \n",
      "\n",
      "ejercicio\n",
      "10.0\n",
      "12.0\n",
      "8.0\n",
      "7.0\n",
      "average: 9.25 \n",
      "\n",
      "caza\n",
      "9.0\n",
      "10.0\n",
      "9.0\n",
      "8.0\n",
      "11.0\n",
      "8.0\n",
      "average: 9.166666666666666 \n",
      "\n",
      "administración\n",
      "9.0\n",
      "8.0\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "10.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "8.0\n",
      "average: 7.8 \n",
      "\n",
      "hacienda\n",
      "8.0\n",
      "9.0\n",
      "13.0\n",
      "7.0\n",
      "6.0\n",
      "8.0\n",
      "8.0\n",
      "average: 8.428571428571429 \n",
      "\n",
      "curiosidad\n",
      "4.0\n",
      "8.0\n",
      "average: 6.0 \n",
      "\n",
      "desatino\n",
      "6.0\n",
      "average: 6.0 \n",
      "\n",
      "hanegas\n",
      "tierra\n",
      "8.0\n",
      "5.0\n",
      "9.0\n",
      "9.0\n",
      "4.0\n",
      "4.0\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "7.0\n",
      "average: 6.5 \n",
      "\n",
      "sembradura\n",
      "libros\n",
      "9.0\n",
      "8.0\n",
      "7.0\n",
      "10.0\n",
      "7.0\n",
      "6.0\n",
      "average: 7.833333333333333 \n",
      "\n",
      "caballerías\n",
      "8.0\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "average: 8.5 \n",
      "\n",
      "casa\n",
      "9.0\n",
      "8.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "5.0\n",
      "9.0\n",
      "7.0\n",
      "average: 7.818181818181818 \n",
      "\n",
      "claridad\n",
      "6.0\n",
      "5.0\n",
      "6.0\n",
      "6.0\n",
      "8.0\n",
      "9.0\n",
      "average: 6.666666666666667 \n",
      "\n",
      "prosa\n",
      "6.0\n",
      "5.0\n",
      "average: 5.5 \n",
      "\n",
      "razones\n",
      "perlas\n",
      "9.0\n",
      "10.0\n",
      "7.0\n",
      "average: 8.666666666666666 \n",
      "\n",
      "requiebros\n",
      "cartas\n",
      "7.0\n",
      "9.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "7.0\n",
      "average: 8.0 \n",
      "\n",
      "desafíos\n",
      "7.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "6.0\n",
      "average: 6.8 \n",
      "\n",
      "partes\n",
      "8.0\n",
      "4.0\n",
      "5.0\n",
      "11.0\n",
      "12.0\n",
      "8.0\n",
      "8.0\n",
      "9.0\n",
      "5.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "4.0\n",
      "9.0\n",
      "average: 6.944444444444445 \n",
      "\n",
      "razón\n",
      "10.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "5.0\n",
      "average: 7.125 \n",
      "\n",
      "sinrazón\n",
      "razón\n",
      "10.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "5.0\n",
      "average: 7.125 \n",
      "\n",
      "razón\n",
      "10.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "5.0\n",
      "average: 7.125 \n",
      "\n",
      "razón\n",
      "10.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "5.0\n",
      "average: 7.125 \n",
      "\n",
      "fermosura\n",
      "cielos\n",
      "9.0\n",
      "9.0\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "average: 7.4 \n",
      "\n",
      "divinidad\n",
      "8.0\n",
      "average: 8.0 \n",
      "\n",
      "estrellas\n",
      "7.0\n",
      "6.0\n",
      "8.0\n",
      "7.0\n",
      "average: 7.0 \n",
      "\n",
      "merecimiento\n",
      "grandeza\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "7.0\n",
      "average: 7.142857142857143 \n",
      "\n",
      "razones\n",
      "caballero\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "average: 7.666666666666667 \n",
      "\n",
      "juicio\n",
      "6.0\n",
      "8.0\n",
      "5.0\n",
      "5.0\n",
      "8.0\n",
      "8.0\n",
      "6.0\n",
      "average: 6.571428571428571 \n",
      "\n",
      "sentido\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "7.0\n",
      "9.0\n",
      "6.0\n",
      "6.0\n",
      "8.0\n",
      "6.0\n",
      "average: 7.666666666666667 \n",
      "\n",
      "heridas\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "average: 9.75 \n",
      "\n",
      "don\n",
      "7.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "average: 8.5 \n",
      "\n",
      "maestros\n",
      "9.0\n",
      "6.0\n",
      "5.0\n",
      "7.0\n",
      "7.0\n",
      "9.0\n",
      "8.0\n",
      "average: 7.285714285714286 \n",
      "\n",
      "rostro\n",
      "6.0\n",
      "8.0\n",
      "average: 7.0 \n",
      "\n",
      "cuerpo\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "7.0\n",
      "10.0\n",
      "5.0\n",
      "5.0\n",
      "8.0\n",
      "9.0\n",
      "5.0\n",
      "6.0\n",
      "average: 6.7272727272727275 \n",
      "\n",
      "cicatrices\n",
      "señales\n",
      "autor\n",
      "7.0\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "average: 6.5 \n",
      "\n",
      "libro\n",
      "9.0\n",
      "8.0\n",
      "7.0\n",
      "10.0\n",
      "7.0\n",
      "6.0\n",
      "average: 7.833333333333333 \n",
      "\n",
      "promesa\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "average: 6.8 \n",
      "\n",
      "aventura\n",
      "9.0\n",
      "9.0\n",
      "6.0\n",
      "7.0\n",
      "average: 7.75 \n",
      "\n",
      "veces\n",
      "pluma\n",
      "7.0\n",
      "10.0\n",
      "10.0\n",
      "6.0\n",
      "7.0\n",
      "average: 8.0 \n",
      "\n",
      "fin\n",
      "10.0\n",
      "6.0\n",
      "9.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "5.0\n",
      "6.0\n",
      "average: 7.125 \n",
      "\n",
      "letra\n",
      "6.0\n",
      "8.0\n",
      "8.0\n",
      "9.0\n",
      "9.0\n",
      "average: 8.0 \n",
      "\n",
      "pensamientos\n",
      "7.0\n",
      "9.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "11.0\n",
      "average: 7.833333333333333 \n",
      "\n",
      "veces\n",
      "competencia\n",
      "6.0\n",
      "6.0\n",
      "5.0\n",
      "8.0\n",
      "average: 6.25 \n",
      "\n",
      "cura\n",
      "7.0\n",
      "7.0\n",
      "6.0\n",
      "average: 6.666666666666667 \n",
      "\n",
      "lugar\n",
      "4.0\n",
      "10.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "6.0\n",
      "8.0\n",
      "7.0\n",
      "5.0\n",
      "average: 6.166666666666667 \n",
      "\n",
      "hombre\n",
      "14.0\n",
      "4.0\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "5.0\n",
      "average: 7.0 \n",
      "\n",
      "caballero\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "average: 7.666666666666667 \n",
      "\n",
      "barbero\n",
      "don\n",
      "7.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "average: 8.5 \n",
      "\n",
      "hermano\n",
      "7.0\n",
      "8.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "average: 7.0 \n",
      "\n",
      "condición\n",
      "6.0\n",
      "9.0\n",
      "10.0\n",
      "6.0\n",
      "5.0\n",
      "5.0\n",
      "7.0\n",
      "average: 6.857142857142857 \n",
      "\n",
      "hermano\n",
      "7.0\n",
      "8.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "average: 7.0 \n",
      "\n",
      "valentía\n",
      "8.0\n",
      "8.0\n",
      "average: 8.0 \n",
      "\n",
      "zaga\n",
      "resolución\n",
      "8.0\n",
      "8.0\n",
      "5.0\n",
      "8.0\n",
      "9.0\n",
      "7.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "average: 7.454545454545454 \n",
      "\n",
      "lectura\n",
      "8.0\n",
      "7.0\n",
      "6.0\n",
      "10.0\n",
      "8.0\n",
      "8.0\n",
      "average: 7.833333333333333 \n",
      "\n",
      "noches\n",
      "10.0\n",
      "7.0\n",
      "5.0\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "6.0\n",
      "11.0\n",
      "average: 7.25 \n",
      "\n",
      "días\n",
      "7.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "average: 6.166666666666667 \n",
      "\n",
      "cerebro\n",
      "8.0\n",
      "8.0\n",
      "5.0\n",
      "7.0\n",
      "6.0\n",
      "average: 6.8 \n",
      "\n",
      "juicio\n",
      "6.0\n",
      "8.0\n",
      "5.0\n",
      "5.0\n",
      "8.0\n",
      "8.0\n",
      "6.0\n",
      "average: 6.571428571428571 \n",
      "\n",
      "fantasía\n",
      "11.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "7.0\n",
      "average: 8.4 \n",
      "\n",
      "libros\n",
      "9.0\n",
      "8.0\n",
      "7.0\n",
      "10.0\n",
      "7.0\n",
      "6.0\n",
      "average: 7.833333333333333 \n",
      "\n",
      "encantamientos\n",
      "10.0\n",
      "7.0\n",
      "average: 8.5 \n",
      "\n",
      "pendencias\n",
      "9.0\n",
      "average: 9.0 \n",
      "\n",
      "batallas\n",
      "8.0\n",
      "7.0\n",
      "6.0\n",
      "8.0\n",
      "average: 7.25 \n",
      "\n",
      "desafíos\n",
      "7.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "6.0\n",
      "average: 6.8 \n",
      "\n",
      "heridas\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "average: 9.75 \n",
      "\n",
      "requiebros\n",
      "amores\n",
      "tormentas\n",
      "10.0\n",
      "7.0\n",
      "average: 8.5 \n",
      "\n",
      "disparates\n",
      "8.0\n",
      "8.0\n",
      "5.0\n",
      "6.0\n",
      "9.0\n",
      "average: 7.2 \n",
      "\n",
      "modo\n",
      "7.0\n",
      "6.0\n",
      "9.0\n",
      "10.0\n",
      "6.0\n",
      "average: 7.6 \n",
      "\n",
      "imaginación\n",
      "11.0\n",
      "7.0\n",
      "average: 9.0 \n",
      "\n",
      "verdad\n",
      "9.0\n",
      "7.0\n",
      "6.0\n",
      "7.0\n",
      "average: 7.25 \n",
      "\n",
      "máquina\n",
      "11.0\n",
      "8.0\n",
      "10.0\n",
      "5.0\n",
      "average: 8.5 \n",
      "\n",
      "invenciones\n",
      "historia\n",
      "9.0\n",
      "5.0\n",
      "8.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "6.0\n",
      "average: 7.333333333333333 \n",
      "\n",
      "mundo\n",
      "4.0\n",
      "7.0\n",
      "5.0\n",
      "9.0\n",
      "6.0\n",
      "average: 6.2 \n",
      "\n",
      "caballero\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "average: 7.666666666666667 \n",
      "\n",
      "ver\n",
      "revés\n",
      "11.0\n",
      "9.0\n",
      "13.0\n",
      "8.0\n",
      "8.0\n",
      "6.0\n",
      "average: 9.166666666666666 \n",
      "\n",
      "fieros\n",
      "encantado\n",
      "industria\n",
      "9.0\n",
      "8.0\n",
      "average: 8.5 \n",
      "\n",
      "hijo\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "average: 7.0 \n",
      "\n",
      "brazos\n",
      "10.0\n",
      "11.0\n",
      "8.0\n",
      "9.0\n",
      "average: 9.5 \n",
      "\n",
      "gigante\n",
      "8.0\n",
      "7.0\n",
      "9.0\n",
      "6.0\n",
      "6.0\n",
      "average: 7.2 \n",
      "\n",
      "generación\n",
      "8.0\n",
      "7.0\n",
      "5.0\n",
      "5.0\n",
      "6.0\n",
      "average: 6.2 \n",
      "\n",
      "castillo\n",
      "10.0\n",
      "average: 10.0 \n",
      "\n",
      "ídolo\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "6.0\n",
      "average: 8.25 \n",
      "\n",
      "oro\n",
      "11.0\n",
      "8.0\n",
      "average: 9.5 \n",
      "\n",
      "historia\n",
      "9.0\n",
      "5.0\n",
      "8.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "6.0\n",
      "average: 7.333333333333333 \n",
      "\n",
      "mano\n",
      "10.0\n",
      "10.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "5.0\n",
      "average: 8.166666666666666 \n",
      "\n",
      "coces\n",
      "traidor\n",
      "ama\n",
      "8.0\n",
      "average: 8.0 \n",
      "\n",
      "sobrina\n",
      "7.0\n",
      "average: 7.0 \n",
      "\n",
      "juicio\n",
      "6.0\n",
      "8.0\n",
      "5.0\n",
      "5.0\n",
      "8.0\n",
      "8.0\n",
      "6.0\n",
      "average: 6.571428571428571 \n",
      "\n",
      "pensamiento\n",
      "7.0\n",
      "9.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "11.0\n",
      "average: 7.833333333333333 \n",
      "\n",
      "loco\n",
      "7.0\n",
      "7.0\n",
      "average: 7.0 \n",
      "\n",
      "mundo\n",
      "4.0\n",
      "7.0\n",
      "5.0\n",
      "9.0\n",
      "6.0\n",
      "average: 6.2 \n",
      "\n",
      "aumento\n",
      "9.0\n",
      "11.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "4.0\n",
      "5.0\n",
      "average: 7.555555555555555 \n",
      "\n",
      "honra\n",
      "servicio\n",
      "9.0\n",
      "12.0\n",
      "8.0\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "6.0\n",
      "9.0\n",
      "8.0\n",
      "8.0\n",
      "9.0\n",
      "average: 8.636363636363637 \n",
      "\n",
      "república\n",
      "8.0\n",
      "6.0\n",
      "6.0\n",
      "average: 6.666666666666667 \n",
      "\n",
      "caballero\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "average: 7.666666666666667 \n",
      "\n",
      "mundo\n",
      "4.0\n",
      "7.0\n",
      "5.0\n",
      "9.0\n",
      "6.0\n",
      "average: 6.2 \n",
      "\n",
      "armas\n",
      "9.0\n",
      "11.0\n",
      "9.0\n",
      "8.0\n",
      "average: 9.25 \n",
      "\n",
      "aventuras\n",
      "9.0\n",
      "9.0\n",
      "6.0\n",
      "7.0\n",
      "average: 7.75 \n",
      "\n",
      "caballeros\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "average: 7.666666666666667 \n",
      "\n",
      "género\n",
      "6.0\n",
      "8.0\n",
      "7.0\n",
      "10.0\n",
      "7.0\n",
      "5.0\n",
      "6.0\n",
      "average: 7.0 \n",
      "\n",
      "agravio\n",
      "ocasiones\n",
      "peligros\n",
      "10.0\n",
      "6.0\n",
      "4.0\n",
      "5.0\n",
      "average: 6.25 \n",
      "\n",
      "nombre\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "9.0\n",
      "average: 8.0 \n",
      "\n",
      "fama\n",
      "8.0\n",
      "9.0\n",
      "average: 8.5 \n",
      "\n",
      "valor\n",
      "9.0\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "8.0\n",
      "9.0\n",
      "7.0\n",
      "9.0\n",
      "6.0\n",
      "6.0\n",
      "10.0\n",
      "8.0\n",
      "6.0\n",
      "4.0\n",
      "7.0\n",
      "average: 7.466666666666667 \n",
      "\n",
      "brazo\n",
      "10.0\n",
      "11.0\n",
      "8.0\n",
      "9.0\n",
      "average: 9.5 \n",
      "\n",
      "imperio\n",
      "8.0\n",
      "8.0\n",
      "10.0\n",
      "8.0\n",
      "7.0\n",
      "average: 8.2 \n",
      "\n",
      "pensamientos\n",
      "7.0\n",
      "9.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "11.0\n",
      "average: 7.833333333333333 \n",
      "\n",
      "gusto\n",
      "7.0\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "7.0\n",
      "6.0\n",
      "7.0\n",
      "average: 7.875 \n",
      "\n",
      "priesa\n",
      "efeto\n",
      "armas\n",
      "9.0\n",
      "11.0\n",
      "9.0\n",
      "8.0\n",
      "average: 9.25 \n",
      "\n",
      "bisabuelos\n",
      "9.0\n",
      "average: 9.0 \n",
      "\n",
      "orín\n",
      "8.0\n",
      "average: 8.0 \n",
      "\n",
      "luengos\n",
      "rincón\n",
      "8.0\n",
      "7.0\n",
      "average: 7.5 \n",
      "\n",
      "Limpiólas\n",
      "falta\n",
      "7.0\n",
      "8.0\n",
      "10.0\n",
      "8.0\n",
      "average: 8.25 \n",
      "\n",
      "encaje\n",
      "8.0\n",
      "7.0\n",
      "9.0\n",
      "8.0\n",
      "average: 8.0 \n",
      "\n",
      "morrión\n",
      "industria\n",
      "9.0\n",
      "8.0\n",
      "average: 8.5 \n",
      "\n",
      "cartones\n",
      "9.0\n",
      "average: 9.0 \n",
      "\n",
      "modo\n",
      "7.0\n",
      "6.0\n",
      "9.0\n",
      "10.0\n",
      "6.0\n",
      "average: 7.6 \n",
      "\n",
      "visera\n",
      "morrión\n",
      "apariencia\n",
      "11.0\n",
      "5.0\n",
      "6.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "6.0\n",
      "10.0\n",
      "7.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "average: 7.416666666666667 \n",
      "\n",
      "celada\n",
      "verdad\n",
      "9.0\n",
      "7.0\n",
      "6.0\n",
      "7.0\n",
      "average: 7.25 \n",
      "\n",
      "riesgo\n",
      "10.0\n",
      "5.0\n",
      "average: 7.5 \n",
      "\n",
      "cuchillada\n",
      "11.0\n",
      "average: 11.0 \n",
      "\n",
      "espada\n",
      "9.0\n",
      "10.0\n",
      "average: 9.5 \n",
      "\n",
      "golpes\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "10.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "13.0\n",
      "5.0\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "average: 9.058823529411764 \n",
      "\n",
      "punto\n",
      "10.0\n",
      "10.0\n",
      "10.0\n",
      "9.0\n",
      "6.0\n",
      "6.0\n",
      "6.0\n",
      "7.0\n",
      "10.0\n",
      "5.0\n",
      "6.0\n",
      "9.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "5.0\n",
      "4.0\n",
      "average: 7.117647058823529 \n",
      "\n",
      "semana\n",
      "6.0\n",
      "6.0\n",
      "average: 6.0 \n",
      "\n",
      "facilidad\n",
      "5.0\n",
      "average: 5.0 \n",
      "\n",
      "pedazos\n",
      "5.0\n",
      "5.0\n",
      "8.0\n",
      "6.0\n",
      "5.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "6.0\n",
      "average: 5.555555555555555 \n",
      "\n",
      "deste\n",
      "peligro\n",
      "10.0\n",
      "6.0\n",
      "4.0\n",
      "5.0\n",
      "average: 6.25 \n",
      "\n",
      "barras\n",
      "8.0\n",
      "11.0\n",
      "7.0\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "7.0\n",
      "average: 8.5 \n",
      "\n",
      "hierro\n",
      "7.0\n",
      "average: 7.0 \n",
      "\n",
      "fortaleza\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "5.0\n",
      "average: 7.5 \n",
      "\n",
      "experiencia\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "average: 6.333333333333333 \n",
      "\n",
      "diputó\n",
      "encaje\n",
      "8.0\n",
      "7.0\n",
      "9.0\n",
      "8.0\n",
      "average: 8.0 \n",
      "\n",
      "rocín\n",
      "cuartos\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "11.0\n",
      "7.0\n",
      "7.0\n",
      "average: 8.166666666666666 \n",
      "\n",
      "real\n",
      "7.0\n",
      "average: 7.0 \n",
      "\n",
      "tachas\n",
      "6.0\n",
      "average: 6.0 \n",
      "\n",
      "caballo\n",
      "15.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "11.0\n",
      "7.0\n",
      "8.0\n",
      "8.0\n",
      "average: 9.5 \n",
      "\n",
      "días\n",
      "7.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "average: 6.166666666666667 \n",
      "\n",
      "nombre\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "9.0\n",
      "average: 8.0 \n",
      "\n",
      "mesmo\n",
      "razón\n",
      "10.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "5.0\n",
      "average: 7.125 \n",
      "\n",
      "caballo\n",
      "15.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "11.0\n",
      "7.0\n",
      "8.0\n",
      "8.0\n",
      "average: 9.5 \n",
      "\n",
      "caballero\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "average: 7.666666666666667 \n",
      "\n",
      "nombre\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "9.0\n",
      "average: 8.0 \n",
      "\n",
      "acomodársele\n",
      "manera\n",
      "7.0\n",
      "8.0\n",
      "7.0\n",
      "6.0\n",
      "average: 7.0 \n",
      "\n",
      "caballero\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "average: 7.666666666666667 \n",
      "\n",
      "razón\n",
      "10.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "5.0\n",
      "average: 7.125 \n",
      "\n",
      "señor\n",
      "9.0\n",
      "9.0\n",
      "7.0\n",
      "8.0\n",
      "8.0\n",
      "6.0\n",
      "average: 7.833333333333333 \n",
      "\n",
      "nombre\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "9.0\n",
      "average: 8.0 \n",
      "\n",
      "orden\n",
      "6.0\n",
      "9.0\n",
      "8.0\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "5.0\n",
      "6.0\n",
      "5.0\n",
      "5.0\n",
      "6.0\n",
      "average: 6.833333333333333 \n",
      "\n",
      "ejercicio\n",
      "10.0\n",
      "12.0\n",
      "8.0\n",
      "7.0\n",
      "average: 9.25 \n",
      "\n",
      "nombres\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "9.0\n",
      "average: 8.0 \n",
      "\n",
      "memoria\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "9.0\n",
      "7.0\n",
      "average: 7.25 \n",
      "\n",
      "imaginación\n",
      "11.0\n",
      "7.0\n",
      "average: 9.0 \n",
      "\n",
      "nombre\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "9.0\n",
      "average: 8.0 \n",
      "\n",
      "parecer\n",
      "8.0\n",
      "7.0\n",
      "5.0\n",
      "average: 6.666666666666667 \n",
      "\n",
      "rocín\n",
      "rocines\n",
      "mundo\n",
      "4.0\n",
      "7.0\n",
      "5.0\n",
      "9.0\n",
      "6.0\n",
      "average: 6.2 \n",
      "\n",
      "nombre\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "9.0\n",
      "average: 8.0 \n",
      "\n",
      "gusto\n",
      "7.0\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "7.0\n",
      "6.0\n",
      "7.0\n",
      "average: 7.875 \n",
      "\n",
      "caballo\n",
      "15.0\n",
      "9.0\n",
      "8.0\n",
      "10.0\n",
      "11.0\n",
      "7.0\n",
      "8.0\n",
      "8.0\n",
      "average: 9.5 \n",
      "\n",
      "pensamiento\n",
      "7.0\n",
      "9.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "11.0\n",
      "average: 7.833333333333333 \n",
      "\n",
      "días\n",
      "7.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "average: 6.166666666666667 \n",
      "\n",
      "don\n",
      "7.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "average: 8.5 \n",
      "\n",
      "ocasión\n",
      "7.0\n",
      "9.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "8.0\n",
      "average: 7.285714285714286 \n",
      "\n",
      "autores\n",
      "historia\n",
      "9.0\n",
      "5.0\n",
      "8.0\n",
      "7.0\n",
      "7.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "6.0\n",
      "average: 7.333333333333333 \n",
      "\n",
      "nombre\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "9.0\n",
      "average: 8.0 \n",
      "\n",
      "reino\n",
      "4.0\n",
      "6.0\n",
      "8.0\n",
      "7.0\n",
      "9.0\n",
      "8.0\n",
      "average: 7.0 \n",
      "\n",
      "caballero\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "average: 7.666666666666667 \n",
      "\n",
      "nombre\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "9.0\n",
      "average: 8.0 \n",
      "\n",
      "don\n",
      "7.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "average: 8.5 \n",
      "\n",
      "parecer\n",
      "8.0\n",
      "7.0\n",
      "5.0\n",
      "average: 6.666666666666667 \n",
      "\n",
      "vivo\n",
      "linaje\n",
      "8.0\n",
      "7.0\n",
      "average: 7.5 \n",
      "\n",
      "patria\n",
      "9.0\n",
      "10.0\n",
      "average: 9.5 \n",
      "\n",
      "sobrenombre\n",
      "7.0\n",
      "8.0\n",
      "average: 7.5 \n",
      "\n",
      "armas\n",
      "9.0\n",
      "11.0\n",
      "9.0\n",
      "8.0\n",
      "average: 9.25 \n",
      "\n",
      "nombre\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "9.0\n",
      "average: 8.0 \n",
      "\n",
      "rocín\n",
      "cosa\n",
      "3.0\n",
      "3.0\n",
      "7.0\n",
      "6.0\n",
      "2.0\n",
      "4.0\n",
      "11.0\n",
      "9.0\n",
      "6.0\n",
      "6.0\n",
      "6.0\n",
      "average: 5.7272727272727275 \n",
      "\n",
      "dama\n",
      "8.0\n",
      "7.0\n",
      "average: 7.5 \n",
      "\n",
      "caballero\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "average: 7.666666666666667 \n",
      "\n",
      "amores\n",
      "árbol\n",
      "10.0\n",
      "7.0\n",
      "average: 8.5 \n",
      "\n",
      "hojas\n",
      "10.0\n",
      "7.0\n",
      "8.0\n",
      "8.0\n",
      "average: 8.25 \n",
      "\n",
      "fruto\n",
      "8.0\n",
      "9.0\n",
      "average: 8.5 \n",
      "\n",
      "cuerpo\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "7.0\n",
      "10.0\n",
      "5.0\n",
      "5.0\n",
      "8.0\n",
      "9.0\n",
      "5.0\n",
      "6.0\n",
      "average: 6.7272727272727275 \n",
      "\n",
      "alma\n",
      "4.0\n",
      "8.0\n",
      "9.0\n",
      "6.0\n",
      "6.0\n",
      "average: 6.6 \n",
      "\n",
      "malos\n",
      "pecados\n",
      "9.0\n",
      "7.0\n",
      "average: 8.0 \n",
      "\n",
      "suerte\n",
      "4.0\n",
      "6.0\n",
      "average: 5.0 \n",
      "\n",
      "gigante\n",
      "8.0\n",
      "7.0\n",
      "9.0\n",
      "6.0\n",
      "6.0\n",
      "average: 7.2 \n",
      "\n",
      "ordinario\n",
      "8.0\n",
      "average: 8.0 \n",
      "\n",
      "caballeros\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "average: 7.666666666666667 \n",
      "\n",
      "derribo\n",
      "11.0\n",
      "9.0\n",
      "average: 10.0 \n",
      "\n",
      "encuentro\n",
      "10.0\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "7.0\n",
      "7.0\n",
      "8.0\n",
      "7.0\n",
      "average: 7.75 \n",
      "\n",
      "cuerpo\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "7.0\n",
      "10.0\n",
      "5.0\n",
      "5.0\n",
      "8.0\n",
      "9.0\n",
      "5.0\n",
      "6.0\n",
      "average: 6.7272727272727275 \n",
      "\n",
      "señora\n",
      "9.0\n",
      "average: 9.0 \n",
      "\n",
      "gigante\n",
      "8.0\n",
      "7.0\n",
      "9.0\n",
      "6.0\n",
      "6.0\n",
      "average: 7.2 \n",
      "\n",
      "señor\n",
      "9.0\n",
      "9.0\n",
      "7.0\n",
      "8.0\n",
      "8.0\n",
      "6.0\n",
      "average: 7.833333333333333 \n",
      "\n",
      "ínsula\n",
      "batalla\n",
      "8.0\n",
      "7.0\n",
      "6.0\n",
      "8.0\n",
      "average: 7.25 \n",
      "\n",
      "caballero\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "average: 7.666666666666667 \n",
      "\n",
      "don\n",
      "7.0\n",
      "9.0\n",
      "10.0\n",
      "8.0\n",
      "average: 8.5 \n",
      "\n",
      "grandeza\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "7.0\n",
      "average: 7.142857142857143 \n",
      "\n",
      "talante\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "average: 6.333333333333333 \n",
      "\n",
      "caballero\n",
      "8.0\n",
      "7.0\n",
      "8.0\n",
      "average: 7.666666666666667 \n",
      "\n",
      "discurso\n",
      "6.0\n",
      "8.0\n",
      "7.0\n",
      "7.0\n",
      "average: 7.0 \n",
      "\n",
      "nombre\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "9.0\n",
      "average: 8.0 \n",
      "\n",
      "dama\n",
      "8.0\n",
      "7.0\n",
      "average: 7.5 \n",
      "\n",
      "lugar\n",
      "4.0\n",
      "10.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "6.0\n",
      "8.0\n",
      "7.0\n",
      "5.0\n",
      "average: 6.166666666666667 \n",
      "\n",
      "moza\n",
      "6.0\n",
      "average: 6.0 \n",
      "\n",
      "parecer\n",
      "8.0\n",
      "7.0\n",
      "5.0\n",
      "average: 6.666666666666667 \n",
      "\n",
      "tiempo\n",
      "4.0\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "average: 5.75 \n",
      "\n",
      "cata\n",
      "título\n",
      "9.0\n",
      "8.0\n",
      "8.0\n",
      "7.0\n",
      "11.0\n",
      "8.0\n",
      "8.0\n",
      "8.0\n",
      "average: 8.375 \n",
      "\n",
      "señora\n",
      "9.0\n",
      "average: 9.0 \n",
      "\n",
      "pensamientos\n",
      "7.0\n",
      "9.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "11.0\n",
      "average: 7.833333333333333 \n",
      "\n",
      "nombre\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "9.0\n",
      "average: 8.0 \n",
      "\n",
      "princesa\n",
      "8.0\n",
      "7.0\n",
      "average: 7.5 \n",
      "\n",
      "señora\n",
      "9.0\n",
      "average: 9.0 \n",
      "\n",
      "nombre\n",
      "10.0\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "6.0\n",
      "7.0\n",
      "7.0\n",
      "9.0\n",
      "average: 8.0 \n",
      "\n",
      "parecer\n",
      "8.0\n",
      "7.0\n",
      "5.0\n",
      "average: 6.666666666666667 \n",
      "\n",
      "músico\n",
      "7.0\n",
      "7.0\n",
      "average: 7.0 \n",
      "\n",
      "peregrino\n",
      "6.0\n",
      "average: 6.0 \n",
      "\n",
      "cosas\n",
      "6.0\n",
      "3.0\n",
      "3.0\n",
      "7.0\n",
      "6.0\n",
      "2.0\n",
      "4.0\n",
      "11.0\n",
      "9.0\n",
      "6.0\n",
      "6.0\n",
      "6.0\n",
      "average: 5.75 \n",
      "\n",
      "Bruce force: 7.628218710023009\n",
      "Implementation: 7.628218710023012\n",
      "\n",
      "Bruce force: 5.0\n",
      "Implementation: 5.0\n"
     ]
    }
   ],
   "source": [
    "text = corpus['A2'][0]['content']\n",
    "print(text)\n",
    "\n",
    "tp = text_processor(text)\n",
    "text_nouns = []\n",
    "text_levels = []\n",
    "\n",
    "\n",
    "for i_sent, sent in enumerate(tp.tags):\n",
    "    for i_tag, tag in enumerate(sent):\n",
    "        if tag == 'NOUN':\n",
    "            token_level = 0\n",
    "            num_senses = 0\n",
    "            print(tp.tokens[i_sent][i_tag])\n",
    "            \n",
    "            synsets = wncr.synsets(tp.tokens[i_sent][i_tag])\n",
    "            for synset in synsets:\n",
    "                if synset.name().split('.')[1]=='n': # noun\n",
    "                    try:\n",
    "                        token_level += 1.0/synset.path_similarity(top_synset)\n",
    "                        num_senses += 1\n",
    "                        print(1.0/synset.path_similarity(top_synset))\n",
    "                    except:\n",
    "                        pass\n",
    "            if num_senses > 0:\n",
    "                text_nouns.append(tp.tokens[i_sent][i_tag])\n",
    "                text_levels.append(token_level/num_senses)        \n",
    "                print(f'average: {token_level/num_senses} \\n')\n",
    "\n",
    "avg, min_level = degree_of_abstraction(text)\n",
    "print(f'Bruce force: {np.mean(text_levels)}')\n",
    "print(f'Implementation: {avg}\\n')\n",
    "\n",
    "print(f'Bruce force: {min(text_levels)}')\n",
    "print(f'Implementation: {min_level}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "instrumental-female",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark está de viaje de negocios en Barcelona. Hoy tuvo un día libre y salió a visitar la ciudad. Primero, caminó por La Rambla, la calle más famosa de Barcelona, llena de gente, tiendas y restaurantes. Se dirigió al Barrio Gótico, uno de los sitios más antiguos y bellos de la ciudad. En la Plaza Sant Jaume observó dos de los edificios más importantes: El Palacio de la Generalitat de Catalunya y el Ayuntamiento. Volvió a La Rambla. Mark tenía hambre y se detuvo a comer unas tapas y beber una cerveza. Continuó hasta la grande y hermosa Plaza de Catalunya. Avanzó por el Paseo de Gràcia hasta llegar a un edificios fuera de lo común Casa Batlló y luego a Casa Milà, diseños del arquitecto Antoni Gaudí. Quiso saber más sobre este famoso arquitecto y se dirigió al Park Güell, donde tomó muchas fotografías. El día se acababa pero antes de volver al hotel, Mark tomó un taxi hacia la Fuente Mágica y disfrutó de un espectáculo de agua y luces. Mark quedó sorprendido con esta gran ciudad y sintió que le faltó tiempo para conocer más lugares interesantes. Se prometió regresar para tomar unas vacaciones con su familia.\n",
      "viaje\n",
      "9.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "11.0\n",
      "7.0\n",
      "6.0\n",
      "9.0\n",
      "average: 9.0 \n",
      "\n",
      "negocios\n",
      "8.0\n",
      "11.0\n",
      "7.0\n",
      "6.0\n",
      "7.0\n",
      "5.0\n",
      "average: 7.333333333333333 \n",
      "\n",
      "día\n",
      "7.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "average: 6.166666666666667 \n",
      "\n",
      "ciudad\n",
      "9.0\n",
      "8.0\n",
      "average: 8.5 \n",
      "\n",
      "calle\n",
      "9.0\n",
      "9.0\n",
      "8.0\n",
      "9.0\n",
      "7.0\n",
      "average: 8.4 \n",
      "\n",
      "gente\n",
      "4.0\n",
      "4.0\n",
      "average: 4.0 \n",
      "\n",
      "tiendas\n",
      "10.0\n",
      "8.0\n",
      "average: 9.0 \n",
      "\n",
      "restaurantes\n",
      "8.0\n",
      "average: 8.0 \n",
      "\n",
      "sitios\n",
      "10.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "6.0\n",
      "8.0\n",
      "5.0\n",
      "average: 6.285714285714286 \n",
      "\n",
      "ciudad\n",
      "9.0\n",
      "8.0\n",
      "average: 8.5 \n",
      "\n",
      "edificios\n",
      "7.0\n",
      "average: 7.0 \n",
      "\n",
      "hambre\n",
      "8.0\n",
      "9.0\n",
      "7.0\n",
      "8.0\n",
      "8.0\n",
      "average: 8.0 \n",
      "\n",
      "tapas\n",
      "cerveza\n",
      "9.0\n",
      "average: 9.0 \n",
      "\n",
      "edificios\n",
      "7.0\n",
      "average: 7.0 \n",
      "\n",
      "diseños\n",
      "9.0\n",
      "7.0\n",
      "10.0\n",
      "7.0\n",
      "8.0\n",
      "average: 8.2 \n",
      "\n",
      "arquitecto\n",
      "6.0\n",
      "average: 6.0 \n",
      "\n",
      "arquitecto\n",
      "6.0\n",
      "average: 6.0 \n",
      "\n",
      "fotografías\n",
      "8.0\n",
      "9.0\n",
      "8.0\n",
      "8.0\n",
      "4.0\n",
      "average: 7.4 \n",
      "\n",
      "día\n",
      "7.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "average: 6.166666666666667 \n",
      "\n",
      "hotel\n",
      "8.0\n",
      "average: 8.0 \n",
      "\n",
      "taxi\n",
      "12.0\n",
      "average: 12.0 \n",
      "\n",
      "espectáculo\n",
      "9.0\n",
      "9.0\n",
      "6.0\n",
      "6.0\n",
      "9.0\n",
      "average: 7.8 \n",
      "\n",
      "agua\n",
      "7.0\n",
      "6.0\n",
      "4.0\n",
      "6.0\n",
      "6.0\n",
      "8.0\n",
      "average: 6.166666666666667 \n",
      "\n",
      "luces\n",
      "8.0\n",
      "average: 8.0 \n",
      "\n",
      "ciudad\n",
      "9.0\n",
      "8.0\n",
      "average: 8.5 \n",
      "\n",
      "tiempo\n",
      "4.0\n",
      "7.0\n",
      "6.0\n",
      "6.0\n",
      "average: 5.75 \n",
      "\n",
      "lugares\n",
      "vacaciones\n",
      "8.0\n",
      "average: 8.0 \n",
      "\n",
      "familia\n",
      "5.0\n",
      "6.0\n",
      "8.0\n",
      "5.0\n",
      "7.0\n",
      "6.0\n",
      "average: 6.166666666666667 \n",
      "\n",
      "Bruce force: 7.511989795918367\n",
      "Implementation: 7.511989795918367\n",
      "\n",
      "Bruce force: 4.0\n",
      "Implementation: 4.0\n"
     ]
    }
   ],
   "source": [
    "text = corpus['B1'][0]['content']\n",
    "print(text)\n",
    "\n",
    "tp = text_processor(text)\n",
    "text_nouns = []\n",
    "text_levels = []\n",
    "\n",
    "\n",
    "for i_sent, sent in enumerate(tp.tags):\n",
    "    for i_tag, tag in enumerate(sent):\n",
    "        if tag == 'NOUN':\n",
    "            token_level = 0\n",
    "            num_senses = 0\n",
    "            print(tp.tokens[i_sent][i_tag])\n",
    "            \n",
    "            synsets = wncr.synsets(tp.tokens[i_sent][i_tag])\n",
    "            for synset in synsets:\n",
    "                if synset.name().split('.')[1]=='n': # noun\n",
    "                    try:\n",
    "                        token_level += 1.0/synset.path_similarity(top_synset)\n",
    "                        num_senses += 1\n",
    "                        print(1.0/synset.path_similarity(top_synset))\n",
    "                    except:\n",
    "                        pass\n",
    "            if num_senses > 0:\n",
    "                text_nouns.append(tp.tokens[i_sent][i_tag])\n",
    "                text_levels.append(token_level/num_senses)        \n",
    "                print(f'average: {token_level/num_senses} \\n')\n",
    "\n",
    "avg, min_level = degree_of_abstraction(text)\n",
    "print(f'Bruce force: {np.mean(text_levels)}')\n",
    "print(f'Implementation: {avg}\\n')\n",
    "\n",
    "print(f'Bruce force: {min(text_levels)}')\n",
    "print(f'Implementation: {min_level}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-handle",
   "metadata": {},
   "source": [
    "##### Run through Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "enclosed-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_list = ['A1', 'A2', 'B', 'B1']\n",
    "level_degree_abstraction = defaultdict(list)\n",
    "for level in level_list:\n",
    "    corpus_item = corpus[level]\n",
    "    for i, text_item in enumerate(corpus_item):\n",
    "        try: \n",
    "            da_avg, da_min = degree_of_abstraction(text_item['content'])\n",
    "            level_degree_abstraction['level'].append(level)\n",
    "            level_degree_abstraction['abstraction_min'].append(da_min)    # pick out the most abstract word in each text \n",
    "            level_degree_abstraction['abstraction_avg'].append(da_avg)    # average abstractness in each text \n",
    "            \n",
    "        except:\n",
    "            print(f'Error', level, i)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-buffer",
   "metadata": {},
   "source": [
    "##### Show the distribution by level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "dutch-nancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-72a8a81aaa89433a98fa01eb04c98297\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-72a8a81aaa89433a98fa01eb04c98297\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-72a8a81aaa89433a98fa01eb04c98297\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-82087d56ddc8559b10f2dc887640ee4a\"}, \"facet\": {\"type\": \"nominal\", \"field\": \"level\"}, \"spec\": {\"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"bin\": {\"maxbins\": 30}, \"field\": \"abstraction_min\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"count\"}}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-82087d56ddc8559b10f2dc887640ee4a\": [{\"level\": \"A1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.824419308290276}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.712613636363637}, {\"level\": \"A1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 8.623626893939395}, {\"level\": \"A1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.856126110731374}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.6208086785009845}, {\"level\": \"A1\", \"abstraction_min\": 5.5, \"abstraction_avg\": 9.716150005805185}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.877360817477097}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 9.254808575263125}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 8.232344187455789}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.449260042283298}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.817379911194789}, {\"level\": \"A1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.381490275893261}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.075384615384616}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.641086096616016}, {\"level\": \"A1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.850514155982904}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.519517075517076}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.323695181081543}, {\"level\": \"A1\", \"abstraction_min\": 5.5, \"abstraction_avg\": 8.369168728097302}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.44030303030303}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.638511794953516}, {\"level\": \"A1\", \"abstraction_min\": 5.2, \"abstraction_avg\": 8.415432900432902}, {\"level\": \"A1\", \"abstraction_min\": 5.5, \"abstraction_avg\": 8.246103896103897}, {\"level\": \"A1\", \"abstraction_min\": 5.833333333333333, \"abstraction_avg\": 7.976282051282052}, {\"level\": \"A1\", \"abstraction_min\": 5.7272727272727275, \"abstraction_avg\": 7.656218024399842}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.8248701298701295}, {\"level\": \"A1\", \"abstraction_min\": 6.666666666666667, \"abstraction_avg\": 8.29357349000206}, {\"level\": \"A1\", \"abstraction_min\": 6.666666666666667, \"abstraction_avg\": 7.161904761904762}, {\"level\": \"A1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 8.172080327080327}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 8.160651755651754}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 8.015157365157364}, {\"level\": \"A1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.625870657120658}, {\"level\": \"A1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.327053140096618}, {\"level\": \"A1\", \"abstraction_min\": 5.7272727272727275, \"abstraction_avg\": 7.392973692973694}, {\"level\": \"A1\", \"abstraction_min\": 6.333333333333333, \"abstraction_avg\": 8.581972789115648}, {\"level\": \"A1\", \"abstraction_min\": 6.944444444444445, \"abstraction_avg\": 8.206646825396826}, {\"level\": \"A1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.012500000000001}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.291666666666667}, {\"level\": \"A1\", \"abstraction_min\": 5.2, \"abstraction_avg\": 7.728287337662338}, {\"level\": \"A1\", \"abstraction_min\": 7.0, \"abstraction_avg\": 8.146875}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.193253968253968}, {\"level\": \"A1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.918376068376068}, {\"level\": \"A1\", \"abstraction_min\": 6.166666666666667, \"abstraction_avg\": 7.3013683847017194}, {\"level\": \"A1\", \"abstraction_min\": 6.571428571428571, \"abstraction_avg\": 8.457671957671957}, {\"level\": \"A1\", \"abstraction_min\": 5.7272727272727275, \"abstraction_avg\": 7.930308334422258}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.981567646912475}, {\"level\": \"A1\", \"abstraction_min\": 6.166666666666667, \"abstraction_avg\": 7.453962703962703}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 6.784598734598733}, {\"level\": \"A1\", \"abstraction_min\": 5.833333333333333, \"abstraction_avg\": 7.478661616161617}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 8.08325449835883}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.842341042341044}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.631901885333257}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 8.027872306557857}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 8.391645293023972}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 8.87888408353772}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 8.176739838176006}, {\"level\": \"A1\", \"abstraction_min\": 5.2, \"abstraction_avg\": 7.474111278394043}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.653813744343462}, {\"level\": \"A1\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.499737878608844}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.66301486902082}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.5335466063959355}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 8.032152840243828}, {\"level\": \"A1\", \"abstraction_min\": 5.2, \"abstraction_avg\": 7.154740446719056}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.591717929506386}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.6033229775084905}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.428038088132229}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.845609093211774}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.049059202061425}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.943939663156081}, {\"level\": \"A1\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.424319727891158}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.656836645852665}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.44632034632034}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.16560044286422}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.554802955665028}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.2446744569984}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.214978150504469}, {\"level\": \"A1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.557122028298498}, {\"level\": \"A1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.67845721529932}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.540122377622377}, {\"level\": \"A1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.309454545454545}, {\"level\": \"A1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.293001397851211}, {\"level\": \"A1\", \"abstraction_min\": 5.2, \"abstraction_avg\": 7.686787149358576}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.396644482297653}, {\"level\": \"A1\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.578240227416054}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.501983900097108}, {\"level\": \"A1\", \"abstraction_min\": 5.333333333333333, \"abstraction_avg\": 7.6576242692881324}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.283439231838626}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.599541636554016}, {\"level\": \"A1\", \"abstraction_min\": 5.2, \"abstraction_avg\": 7.499770776609014}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.557914324825682}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.420626515416069}, {\"level\": \"A1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.413421245421246}, {\"level\": \"A1\", \"abstraction_min\": 5.857142857142857, \"abstraction_avg\": 7.358565621370499}, {\"level\": \"A1\", \"abstraction_min\": 5.333333333333333, \"abstraction_avg\": 7.417101284958428}, {\"level\": \"A1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.135260770975056}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.628218710023012}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.856085200027893}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.711680667874109}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.757457083629458}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.534874026027209}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.812439963902606}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.724128286129522}, {\"level\": \"A2\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.511306964853676}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.331408446734848}, {\"level\": \"A2\", \"abstraction_min\": 5.666666666666667, \"abstraction_avg\": 7.494791666666668}, {\"level\": \"A2\", \"abstraction_min\": 5.2, \"abstraction_avg\": 7.90107101232101}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.244949494949495}, {\"level\": \"A2\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.889641700019059}, {\"level\": \"A2\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.49290601871247}, {\"level\": \"A2\", \"abstraction_min\": 5.75, \"abstraction_avg\": 8.12676008202324}, {\"level\": \"A2\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.629465709728868}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.687121212121213}, {\"level\": \"A2\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.220643939393938}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.600922350922351}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.329497053872055}, {\"level\": \"A2\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.862408147408146}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.8725}, {\"level\": \"A2\", \"abstraction_min\": 6.25, \"abstraction_avg\": 7.8322023809523795}, {\"level\": \"A2\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.356796451914099}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.753462039752363}, {\"level\": \"A2\", \"abstraction_min\": 6.166666666666667, \"abstraction_avg\": 7.811403508771931}, {\"level\": \"A2\", \"abstraction_min\": 7.0, \"abstraction_avg\": 7.868421052631579}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.65978260869565}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 6.97561553030303}, {\"level\": \"A2\", \"abstraction_min\": 6.0, \"abstraction_avg\": 6.479166666666667}, {\"level\": \"A2\", \"abstraction_min\": 5.75, \"abstraction_avg\": 6.8928571428571415}, {\"level\": \"A2\", \"abstraction_min\": 6.0, \"abstraction_avg\": 6.888888888888889}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.551440329218108}, {\"level\": \"A2\", \"abstraction_min\": 5.555555555555555, \"abstraction_avg\": 7.7003513609678}, {\"level\": \"A2\", \"abstraction_min\": 5.833333333333333, \"abstraction_avg\": 7.571654040404041}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.695561434450324}, {\"level\": \"A2\", \"abstraction_min\": 5.7272727272727275, \"abstraction_avg\": 7.664882450882454}, {\"level\": \"A2\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.49688764394647}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.588981300282669}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.253198653198653}, {\"level\": \"A2\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.635459314696602}, {\"level\": \"A2\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.3989503328213}, {\"level\": \"A2\", \"abstraction_min\": 6.0, \"abstraction_avg\": 8.093055555555555}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.514711862211862}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 8.486025039386382}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.799529871757317}, {\"level\": \"A2\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.241855631141345}, {\"level\": \"A2\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.487910997732426}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.373780710030711}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.907786718112806}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.509806301369775}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.915371654860291}, {\"level\": \"A2\", \"abstraction_min\": 5.2, \"abstraction_avg\": 7.533650278293134}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.531405585155584}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.616895286815726}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.297200291330983}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.371832788139177}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.762875457875458}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.685753076075659}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.547572556551429}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.733798934878073}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.733231652114631}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.503973960957692}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.103295220371655}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.284239667052166}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.786337868480726}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.569942470092135}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.506421629351041}, {\"level\": \"B\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.809803921568626}, {\"level\": \"B\", \"abstraction_min\": 4.25, \"abstraction_avg\": 7.556440974095922}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.966285381285383}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.950687830687831}, {\"level\": \"B\", \"abstraction_min\": 4.666666666666667, \"abstraction_avg\": 7.2422004201537105}, {\"level\": \"B\", \"abstraction_min\": 3.0, \"abstraction_avg\": 7.536576518273785}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.873482817504556}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.379824935353258}, {\"level\": \"B\", \"abstraction_min\": 4.666666666666667, \"abstraction_avg\": 7.532293780423399}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.561712908303818}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.706835454407274}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.470789198723981}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.578292267373148}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.1631481481481485}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.769902538263196}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.536590992986342}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.68952380952381}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.506060606060607}, {\"level\": \"B\", \"abstraction_min\": 5.2, \"abstraction_avg\": 7.38041906622789}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 8.146}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 8.662820512820513}, {\"level\": \"B\", \"abstraction_min\": 7.0, \"abstraction_avg\": 7.25}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.348281008207479}, {\"level\": \"B\", \"abstraction_min\": 6.7272727272727275, \"abstraction_avg\": 7.664267676767676}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.166666666666667}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.631845238095238}, {\"level\": \"B\", \"abstraction_min\": 5.857142857142857, \"abstraction_avg\": 7.095518207282914}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.7936507936507935}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.70952380952381}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.514327485380116}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.256932773109243}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.511696301782509}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 8.07665154150807}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.610831713637833}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.678384679556554}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.636061285580517}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.488369576160273}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.876830171800581}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.535654761904762}, {\"level\": \"B\", \"abstraction_min\": 5.75, \"abstraction_avg\": 8.104197297375638}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.775184676434677}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.663366485834839}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.809966422466422}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 8.210537918871253}, {\"level\": \"B\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.492593110912075}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.94351851851852}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.637491483910166}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.610482228729026}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.384750566893424}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 8.2}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 8.079608225108224}, {\"level\": \"B\", \"abstraction_min\": 4.666666666666667, \"abstraction_avg\": 7.368834688346881}, {\"level\": \"B\", \"abstraction_min\": 6.5, \"abstraction_avg\": 7.613659147869675}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.584837578442231}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.364777869189633}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.401531974969476}, {\"level\": \"B\", \"abstraction_min\": 3.0, \"abstraction_avg\": 7.211607142857143}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.3751984126984125}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.169696969696971}, {\"level\": \"B\", \"abstraction_min\": 5.666666666666667, \"abstraction_avg\": 7.177272727272728}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.486575543797766}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.621716186591186}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.262279541446209}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.759986772486773}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.63191192865106}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.343518518518518}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.381312695873865}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.440513983371127}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.260786435786435}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.741846361185983}, {\"level\": \"B\", \"abstraction_min\": 5.666666666666667, \"abstraction_avg\": 7.540623003123002}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.201789292578767}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.400002398731212}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.301179138321994}, {\"level\": \"B\", \"abstraction_min\": 5.6, \"abstraction_avg\": 7.711341823551125}, {\"level\": \"B\", \"abstraction_min\": 5.7272727272727275, \"abstraction_avg\": 8.300307026307026}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.464582429844056}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.506517206241469}, {\"level\": \"B\", \"abstraction_min\": 4.666666666666667, \"abstraction_avg\": 7.293358837544884}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.027136752136752}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 8.23501984126984}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.644033841247218}, {\"level\": \"B\", \"abstraction_min\": 5.857142857142857, \"abstraction_avg\": 7.611921347386466}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.398841158841159}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.402139971139971}, {\"level\": \"B\", \"abstraction_min\": 5.666666666666667, \"abstraction_avg\": 6.988359788359789}, {\"level\": \"B\", \"abstraction_min\": 4.666666666666667, \"abstraction_avg\": 7.407576803228976}, {\"level\": \"B\", \"abstraction_min\": 5.7272727272727275, \"abstraction_avg\": 7.748183364254794}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 8.00391414141414}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.7885507321677565}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.436448016094033}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.626709401709402}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.433333333333334}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 9.157142857142857}, {\"level\": \"B\", \"abstraction_min\": 6.6, \"abstraction_avg\": 6.8}, {\"level\": \"B\", \"abstraction_min\": 6.5, \"abstraction_avg\": 8.69090909090909}, {\"level\": \"B\", \"abstraction_min\": 7.0, \"abstraction_avg\": 8.0}, {\"level\": \"B\", \"abstraction_min\": 6.2, \"abstraction_avg\": 6.933333333333334}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 8.033333333333335}, {\"level\": \"B\", \"abstraction_min\": 7.0, \"abstraction_avg\": 8.0}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.537857142857144}, {\"level\": \"B\", \"abstraction_min\": 6.4, \"abstraction_avg\": 7.973754578754578}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 6.549166666666667}, {\"level\": \"B\", \"abstraction_min\": 7.666666666666667, \"abstraction_avg\": 8.458333333333334}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.511989795918367}, {\"level\": \"B1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 8.307637600494742}, {\"level\": \"B1\", \"abstraction_min\": 6.166666666666667, \"abstraction_avg\": 7.571422558922559}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.777944862155389}, {\"level\": \"B1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.303367592003956}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.144024834814309}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.607463524130191}, {\"level\": \"B1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.643876262626262}, {\"level\": \"B1\", \"abstraction_min\": 5.666666666666667, \"abstraction_avg\": 8.1626267143709}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.670781893004117}, {\"level\": \"B1\", \"abstraction_min\": 5.833333333333333, \"abstraction_avg\": 7.91047619047619}, {\"level\": \"B1\", \"abstraction_min\": 5.5, \"abstraction_avg\": 8.528070626955415}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.656402015210741}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.386924871586443}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.511675899857719}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.827287529230806}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.327906823330869}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.5868658645647296}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.845137997432605}, {\"level\": \"B1\", \"abstraction_min\": 5.2, \"abstraction_avg\": 7.818587321547849}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.786344694372765}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 8.040455477813431}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.461357907958678}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.706542594639538}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.731595534095533}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.477056101334955}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.732448536622526}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.74253593489997}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.780611441066207}, {\"level\": \"B1\", \"abstraction_min\": 5.833333333333333, \"abstraction_avg\": 8.024305555555555}, {\"level\": \"B1\", \"abstraction_min\": 6.166666666666667, \"abstraction_avg\": 7.821818181818182}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.5496336996337}, {\"level\": \"B1\", \"abstraction_min\": 6.166666666666667, \"abstraction_avg\": 7.416666666666667}, {\"level\": \"B1\", \"abstraction_min\": 6.333333333333333, \"abstraction_avg\": 7.572222222222222}, {\"level\": \"B1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.642783899602082}, {\"level\": \"B1\", \"abstraction_min\": 4.666666666666667, \"abstraction_avg\": 7.402499198332531}, {\"level\": \"B1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.399080086580088}, {\"level\": \"B1\", \"abstraction_min\": 6.166666666666667, \"abstraction_avg\": 8.973076923076924}, {\"level\": \"B1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.343674067867616}, {\"level\": \"B1\", \"abstraction_min\": 5.666666666666667, \"abstraction_avg\": 7.079166666666667}, {\"level\": \"B1\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.450382819794585}, {\"level\": \"B1\", \"abstraction_min\": 6.166666666666667, \"abstraction_avg\": 7.750359032501891}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_da_df = pd.DataFrame(level_degree_abstraction)\n",
    "alt.Chart(level_da_df).mark_bar().encode(x = alt.X('abstraction_min', bin = alt.Bin(maxbins = 30)), y = 'count()').facet('level')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "protected-davis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-1f52addfadc34c16ad2e3c42942a5121\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-1f52addfadc34c16ad2e3c42942a5121\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-1f52addfadc34c16ad2e3c42942a5121\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-82087d56ddc8559b10f2dc887640ee4a\"}, \"facet\": {\"type\": \"nominal\", \"field\": \"level\"}, \"spec\": {\"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"bin\": {\"maxbins\": 30}, \"field\": \"abstraction_avg\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"count\"}}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-82087d56ddc8559b10f2dc887640ee4a\": [{\"level\": \"A1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.824419308290276}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.712613636363637}, {\"level\": \"A1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 8.623626893939395}, {\"level\": \"A1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.856126110731374}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.6208086785009845}, {\"level\": \"A1\", \"abstraction_min\": 5.5, \"abstraction_avg\": 9.716150005805185}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.877360817477097}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 9.254808575263125}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 8.232344187455789}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.449260042283298}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.817379911194789}, {\"level\": \"A1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.381490275893261}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.075384615384616}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.641086096616016}, {\"level\": \"A1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.850514155982904}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.519517075517076}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.323695181081543}, {\"level\": \"A1\", \"abstraction_min\": 5.5, \"abstraction_avg\": 8.369168728097302}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.44030303030303}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.638511794953516}, {\"level\": \"A1\", \"abstraction_min\": 5.2, \"abstraction_avg\": 8.415432900432902}, {\"level\": \"A1\", \"abstraction_min\": 5.5, \"abstraction_avg\": 8.246103896103897}, {\"level\": \"A1\", \"abstraction_min\": 5.833333333333333, \"abstraction_avg\": 7.976282051282052}, {\"level\": \"A1\", \"abstraction_min\": 5.7272727272727275, \"abstraction_avg\": 7.656218024399842}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.8248701298701295}, {\"level\": \"A1\", \"abstraction_min\": 6.666666666666667, \"abstraction_avg\": 8.29357349000206}, {\"level\": \"A1\", \"abstraction_min\": 6.666666666666667, \"abstraction_avg\": 7.161904761904762}, {\"level\": \"A1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 8.172080327080327}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 8.160651755651754}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 8.015157365157364}, {\"level\": \"A1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.625870657120658}, {\"level\": \"A1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.327053140096618}, {\"level\": \"A1\", \"abstraction_min\": 5.7272727272727275, \"abstraction_avg\": 7.392973692973694}, {\"level\": \"A1\", \"abstraction_min\": 6.333333333333333, \"abstraction_avg\": 8.581972789115648}, {\"level\": \"A1\", \"abstraction_min\": 6.944444444444445, \"abstraction_avg\": 8.206646825396826}, {\"level\": \"A1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.012500000000001}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.291666666666667}, {\"level\": \"A1\", \"abstraction_min\": 5.2, \"abstraction_avg\": 7.728287337662338}, {\"level\": \"A1\", \"abstraction_min\": 7.0, \"abstraction_avg\": 8.146875}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.193253968253968}, {\"level\": \"A1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.918376068376068}, {\"level\": \"A1\", \"abstraction_min\": 6.166666666666667, \"abstraction_avg\": 7.3013683847017194}, {\"level\": \"A1\", \"abstraction_min\": 6.571428571428571, \"abstraction_avg\": 8.457671957671957}, {\"level\": \"A1\", \"abstraction_min\": 5.7272727272727275, \"abstraction_avg\": 7.930308334422258}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.981567646912475}, {\"level\": \"A1\", \"abstraction_min\": 6.166666666666667, \"abstraction_avg\": 7.453962703962703}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 6.784598734598733}, {\"level\": \"A1\", \"abstraction_min\": 5.833333333333333, \"abstraction_avg\": 7.478661616161617}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 8.08325449835883}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.842341042341044}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.631901885333257}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 8.027872306557857}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 8.391645293023972}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 8.87888408353772}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 8.176739838176006}, {\"level\": \"A1\", \"abstraction_min\": 5.2, \"abstraction_avg\": 7.474111278394043}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.653813744343462}, {\"level\": \"A1\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.499737878608844}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.66301486902082}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.5335466063959355}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 8.032152840243828}, {\"level\": \"A1\", \"abstraction_min\": 5.2, \"abstraction_avg\": 7.154740446719056}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.591717929506386}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.6033229775084905}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.428038088132229}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.845609093211774}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.049059202061425}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.943939663156081}, {\"level\": \"A1\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.424319727891158}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.656836645852665}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.44632034632034}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.16560044286422}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.554802955665028}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.2446744569984}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.214978150504469}, {\"level\": \"A1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.557122028298498}, {\"level\": \"A1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.67845721529932}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.540122377622377}, {\"level\": \"A1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.309454545454545}, {\"level\": \"A1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.293001397851211}, {\"level\": \"A1\", \"abstraction_min\": 5.2, \"abstraction_avg\": 7.686787149358576}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.396644482297653}, {\"level\": \"A1\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.578240227416054}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.501983900097108}, {\"level\": \"A1\", \"abstraction_min\": 5.333333333333333, \"abstraction_avg\": 7.6576242692881324}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.283439231838626}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.599541636554016}, {\"level\": \"A1\", \"abstraction_min\": 5.2, \"abstraction_avg\": 7.499770776609014}, {\"level\": \"A1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.557914324825682}, {\"level\": \"A1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.420626515416069}, {\"level\": \"A1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.413421245421246}, {\"level\": \"A1\", \"abstraction_min\": 5.857142857142857, \"abstraction_avg\": 7.358565621370499}, {\"level\": \"A1\", \"abstraction_min\": 5.333333333333333, \"abstraction_avg\": 7.417101284958428}, {\"level\": \"A1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.135260770975056}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.628218710023012}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.856085200027893}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.711680667874109}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.757457083629458}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.534874026027209}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.812439963902606}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.724128286129522}, {\"level\": \"A2\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.511306964853676}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.331408446734848}, {\"level\": \"A2\", \"abstraction_min\": 5.666666666666667, \"abstraction_avg\": 7.494791666666668}, {\"level\": \"A2\", \"abstraction_min\": 5.2, \"abstraction_avg\": 7.90107101232101}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.244949494949495}, {\"level\": \"A2\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.889641700019059}, {\"level\": \"A2\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.49290601871247}, {\"level\": \"A2\", \"abstraction_min\": 5.75, \"abstraction_avg\": 8.12676008202324}, {\"level\": \"A2\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.629465709728868}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.687121212121213}, {\"level\": \"A2\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.220643939393938}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.600922350922351}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.329497053872055}, {\"level\": \"A2\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.862408147408146}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.8725}, {\"level\": \"A2\", \"abstraction_min\": 6.25, \"abstraction_avg\": 7.8322023809523795}, {\"level\": \"A2\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.356796451914099}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.753462039752363}, {\"level\": \"A2\", \"abstraction_min\": 6.166666666666667, \"abstraction_avg\": 7.811403508771931}, {\"level\": \"A2\", \"abstraction_min\": 7.0, \"abstraction_avg\": 7.868421052631579}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.65978260869565}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 6.97561553030303}, {\"level\": \"A2\", \"abstraction_min\": 6.0, \"abstraction_avg\": 6.479166666666667}, {\"level\": \"A2\", \"abstraction_min\": 5.75, \"abstraction_avg\": 6.8928571428571415}, {\"level\": \"A2\", \"abstraction_min\": 6.0, \"abstraction_avg\": 6.888888888888889}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.551440329218108}, {\"level\": \"A2\", \"abstraction_min\": 5.555555555555555, \"abstraction_avg\": 7.7003513609678}, {\"level\": \"A2\", \"abstraction_min\": 5.833333333333333, \"abstraction_avg\": 7.571654040404041}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.695561434450324}, {\"level\": \"A2\", \"abstraction_min\": 5.7272727272727275, \"abstraction_avg\": 7.664882450882454}, {\"level\": \"A2\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.49688764394647}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.588981300282669}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.253198653198653}, {\"level\": \"A2\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.635459314696602}, {\"level\": \"A2\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.3989503328213}, {\"level\": \"A2\", \"abstraction_min\": 6.0, \"abstraction_avg\": 8.093055555555555}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.514711862211862}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 8.486025039386382}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.799529871757317}, {\"level\": \"A2\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.241855631141345}, {\"level\": \"A2\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.487910997732426}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.373780710030711}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.907786718112806}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.509806301369775}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.915371654860291}, {\"level\": \"A2\", \"abstraction_min\": 5.2, \"abstraction_avg\": 7.533650278293134}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.531405585155584}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.616895286815726}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.297200291330983}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.371832788139177}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.762875457875458}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.685753076075659}, {\"level\": \"A2\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.547572556551429}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.733798934878073}, {\"level\": \"A2\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.733231652114631}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.503973960957692}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.103295220371655}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.284239667052166}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.786337868480726}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.569942470092135}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.506421629351041}, {\"level\": \"B\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.809803921568626}, {\"level\": \"B\", \"abstraction_min\": 4.25, \"abstraction_avg\": 7.556440974095922}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.966285381285383}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.950687830687831}, {\"level\": \"B\", \"abstraction_min\": 4.666666666666667, \"abstraction_avg\": 7.2422004201537105}, {\"level\": \"B\", \"abstraction_min\": 3.0, \"abstraction_avg\": 7.536576518273785}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.873482817504556}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.379824935353258}, {\"level\": \"B\", \"abstraction_min\": 4.666666666666667, \"abstraction_avg\": 7.532293780423399}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.561712908303818}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.706835454407274}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.470789198723981}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.578292267373148}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.1631481481481485}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.769902538263196}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.536590992986342}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.68952380952381}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.506060606060607}, {\"level\": \"B\", \"abstraction_min\": 5.2, \"abstraction_avg\": 7.38041906622789}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 8.146}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 8.662820512820513}, {\"level\": \"B\", \"abstraction_min\": 7.0, \"abstraction_avg\": 7.25}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.348281008207479}, {\"level\": \"B\", \"abstraction_min\": 6.7272727272727275, \"abstraction_avg\": 7.664267676767676}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.166666666666667}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.631845238095238}, {\"level\": \"B\", \"abstraction_min\": 5.857142857142857, \"abstraction_avg\": 7.095518207282914}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.7936507936507935}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.70952380952381}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.514327485380116}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.256932773109243}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.511696301782509}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 8.07665154150807}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.610831713637833}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.678384679556554}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.636061285580517}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.488369576160273}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.876830171800581}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.535654761904762}, {\"level\": \"B\", \"abstraction_min\": 5.75, \"abstraction_avg\": 8.104197297375638}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.775184676434677}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.663366485834839}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.809966422466422}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 8.210537918871253}, {\"level\": \"B\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.492593110912075}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.94351851851852}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.637491483910166}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.610482228729026}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.384750566893424}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 8.2}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 8.079608225108224}, {\"level\": \"B\", \"abstraction_min\": 4.666666666666667, \"abstraction_avg\": 7.368834688346881}, {\"level\": \"B\", \"abstraction_min\": 6.5, \"abstraction_avg\": 7.613659147869675}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.584837578442231}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.364777869189633}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.401531974969476}, {\"level\": \"B\", \"abstraction_min\": 3.0, \"abstraction_avg\": 7.211607142857143}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.3751984126984125}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.169696969696971}, {\"level\": \"B\", \"abstraction_min\": 5.666666666666667, \"abstraction_avg\": 7.177272727272728}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.486575543797766}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.621716186591186}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.262279541446209}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.759986772486773}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.63191192865106}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.343518518518518}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.381312695873865}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.440513983371127}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.260786435786435}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.741846361185983}, {\"level\": \"B\", \"abstraction_min\": 5.666666666666667, \"abstraction_avg\": 7.540623003123002}, {\"level\": \"B\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.201789292578767}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.400002398731212}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.301179138321994}, {\"level\": \"B\", \"abstraction_min\": 5.6, \"abstraction_avg\": 7.711341823551125}, {\"level\": \"B\", \"abstraction_min\": 5.7272727272727275, \"abstraction_avg\": 8.300307026307026}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.464582429844056}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.506517206241469}, {\"level\": \"B\", \"abstraction_min\": 4.666666666666667, \"abstraction_avg\": 7.293358837544884}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.027136752136752}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 8.23501984126984}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.644033841247218}, {\"level\": \"B\", \"abstraction_min\": 5.857142857142857, \"abstraction_avg\": 7.611921347386466}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.398841158841159}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.402139971139971}, {\"level\": \"B\", \"abstraction_min\": 5.666666666666667, \"abstraction_avg\": 6.988359788359789}, {\"level\": \"B\", \"abstraction_min\": 4.666666666666667, \"abstraction_avg\": 7.407576803228976}, {\"level\": \"B\", \"abstraction_min\": 5.7272727272727275, \"abstraction_avg\": 7.748183364254794}, {\"level\": \"B\", \"abstraction_min\": 5.5, \"abstraction_avg\": 8.00391414141414}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.7885507321677565}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.436448016094033}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.626709401709402}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.433333333333334}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 9.157142857142857}, {\"level\": \"B\", \"abstraction_min\": 6.6, \"abstraction_avg\": 6.8}, {\"level\": \"B\", \"abstraction_min\": 6.5, \"abstraction_avg\": 8.69090909090909}, {\"level\": \"B\", \"abstraction_min\": 7.0, \"abstraction_avg\": 8.0}, {\"level\": \"B\", \"abstraction_min\": 6.2, \"abstraction_avg\": 6.933333333333334}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 8.033333333333335}, {\"level\": \"B\", \"abstraction_min\": 7.0, \"abstraction_avg\": 8.0}, {\"level\": \"B\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.537857142857144}, {\"level\": \"B\", \"abstraction_min\": 6.4, \"abstraction_avg\": 7.973754578754578}, {\"level\": \"B\", \"abstraction_min\": 5.0, \"abstraction_avg\": 6.549166666666667}, {\"level\": \"B\", \"abstraction_min\": 7.666666666666667, \"abstraction_avg\": 8.458333333333334}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.511989795918367}, {\"level\": \"B1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 8.307637600494742}, {\"level\": \"B1\", \"abstraction_min\": 6.166666666666667, \"abstraction_avg\": 7.571422558922559}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.777944862155389}, {\"level\": \"B1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.303367592003956}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.144024834814309}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.607463524130191}, {\"level\": \"B1\", \"abstraction_min\": 5.75, \"abstraction_avg\": 7.643876262626262}, {\"level\": \"B1\", \"abstraction_min\": 5.666666666666667, \"abstraction_avg\": 8.1626267143709}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.670781893004117}, {\"level\": \"B1\", \"abstraction_min\": 5.833333333333333, \"abstraction_avg\": 7.91047619047619}, {\"level\": \"B1\", \"abstraction_min\": 5.5, \"abstraction_avg\": 8.528070626955415}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.656402015210741}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.386924871586443}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.511675899857719}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.827287529230806}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.327906823330869}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.5868658645647296}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.845137997432605}, {\"level\": \"B1\", \"abstraction_min\": 5.2, \"abstraction_avg\": 7.818587321547849}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.786344694372765}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 8.040455477813431}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.461357907958678}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.706542594639538}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.731595534095533}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.477056101334955}, {\"level\": \"B1\", \"abstraction_min\": 4.0, \"abstraction_avg\": 7.732448536622526}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.74253593489997}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.780611441066207}, {\"level\": \"B1\", \"abstraction_min\": 5.833333333333333, \"abstraction_avg\": 8.024305555555555}, {\"level\": \"B1\", \"abstraction_min\": 6.166666666666667, \"abstraction_avg\": 7.821818181818182}, {\"level\": \"B1\", \"abstraction_min\": 5.0, \"abstraction_avg\": 7.5496336996337}, {\"level\": \"B1\", \"abstraction_min\": 6.166666666666667, \"abstraction_avg\": 7.416666666666667}, {\"level\": \"B1\", \"abstraction_min\": 6.333333333333333, \"abstraction_avg\": 7.572222222222222}, {\"level\": \"B1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.642783899602082}, {\"level\": \"B1\", \"abstraction_min\": 4.666666666666667, \"abstraction_avg\": 7.402499198332531}, {\"level\": \"B1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.399080086580088}, {\"level\": \"B1\", \"abstraction_min\": 6.166666666666667, \"abstraction_avg\": 8.973076923076924}, {\"level\": \"B1\", \"abstraction_min\": 6.0, \"abstraction_avg\": 7.343674067867616}, {\"level\": \"B1\", \"abstraction_min\": 5.666666666666667, \"abstraction_avg\": 7.079166666666667}, {\"level\": \"B1\", \"abstraction_min\": 5.5, \"abstraction_avg\": 7.450382819794585}, {\"level\": \"B1\", \"abstraction_min\": 6.166666666666667, \"abstraction_avg\": 7.750359032501891}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(level_da_df).mark_bar().encode(x = alt.X('abstraction_avg', bin = alt.Bin(maxbins = 30)), y = 'count()').facet('level')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-sheep",
   "metadata": {},
   "source": [
    "## Degree of Ambiguation\n",
    "\n",
    "This feature is based one of the measures suggested in the paper: \"Coh-Metrix: Analysis of text on cohesion and language\". \n",
    "The authors of the paper suggest that the degree of ambiguation can be measured by number of senses in WordNet. Words having many senses tend to be more ambiguous. \n",
    "\n",
    "We calculate the degree of ambiguousness of a Spanish text by taking the average over the number of senses in the tokens in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "refined-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polysemy_ambiguation(text):\n",
    "    ''' This function measures degree of ambiguation of a text by counting the number of senses of each token.\n",
    "    ------------------------------------------------\n",
    "    Argument: text (str)\n",
    "    Returns: the degree of ambiguation as mean over all tokens (the higher, more ambiguous)\n",
    "             the degree of ambiguation as mean over all content tokens \n",
    "    '''\n",
    "    \n",
    "    sent_tokens, sent_senses = [], []\n",
    "    sent_cont_tokens, sent_cont_senses = [], []\n",
    "    num_senses, num_cont_senses = 0, 0\n",
    "\n",
    "    \n",
    "    tp = text_processor(text)\n",
    "    for i_sent, sent in enumerate(tp.tokens):\n",
    "        for i_token, token in enumerate(sent):\n",
    "            token = token.lower()\n",
    "            synsets = wncr.synsets(token)\n",
    "            if len(synsets) > 0 :                                                                                              # all words \n",
    "                num_senses += len(synsets) \n",
    "                sent_tokens.append(token)                # for debugging purpose \n",
    "                sent_senses.append(len(synsets))\n",
    "            if tp.tags[i_sent][i_token] in {\"VERB\", \"NOUN\", \"PROPN\", \"ADP\", \"ADJ\", \"ADV\"} and len(synsets) > 0:                # function workds \n",
    "                num_cont_senses  += len(synsets)\n",
    "                sent_cont_tokens.append(token)\n",
    "                sent_cont_senses.append(len(synsets))\n",
    "            \n",
    "    if sent_senses == []:\n",
    "        return 0, 0\n",
    "    \n",
    "    return np.mean(sent_senses), np.mean(sent_cont_senses)            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-offering",
   "metadata": {},
   "source": [
    "### Tests\n",
    "\n",
    "Shown below are test cases for polysemy_ambiguatoin function. Please note that these codes are written to make debugging easier and they are not written for efficiency.\n",
    "The codes print a lot of redundant information for debugging purpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-emergency",
   "metadata": {},
   "source": [
    "#### Boundary Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "subject-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "assert polysemy_ambiguation(text) == (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "quarterly-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '1.'\n",
    "assert polysemy_ambiguation(text) == (0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-carol",
   "metadata": {},
   "source": [
    "#### Bruce force vs. Implementation (Simple Cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "better-courage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voy VERB []\n",
      "a ADP [Synset('a.n.01')]\n",
      "la DET [Synset('la.n.01')]\n",
      "escuela NOUN [Synset('colegio.n.01'), Synset('escuela.n.02'), Synset('colegio.n.02'), Synset('colegio.n.03')]\n",
      ". PUNCT []\n",
      "voy VERB []\n",
      "a ADP [Synset('a.n.01')]\n",
      "la DET [Synset('la.n.01')]\n",
      "escuela NOUN [Synset('colegio.n.01'), Synset('escuela.n.02'), Synset('colegio.n.02'), Synset('colegio.n.03')]\n",
      "el DET []\n",
      "lunes NOUN [Synset('lunes.n.01')]\n",
      ", PUNCT []\n",
      "el DET []\n",
      "martes NOUN [Synset('martes.n.01'), Synset('marte.n.01')]\n",
      ", PUNCT []\n",
      "el DET []\n",
      "miércoles NOUN [Synset('miércoles.n.01')]\n",
      ", PUNCT []\n",
      "el DET []\n",
      "jueves NOUN [Synset('jueves.n.01')]\n",
      "y CCONJ [Synset('y.n.01')]\n",
      "el DET []\n",
      "viernes NOUN [Synset('viernes.n.01')]\n",
      ". PUNCT []\n",
      "1.5833333333333333 1.7777777777777777\n",
      "(1.5833333333333333, 1.7777777777777777)\n"
     ]
    }
   ],
   "source": [
    "text = 'Voy a la escuela. Voy a la escuela el lunes, el martes, el miércoles, el jueves y el viernes.'\n",
    "tp = text_processor(text)\n",
    "num_synsets, num_tokens = 0, 0\n",
    "num_cont_synsets, num_cont_tokens = 0, 0\n",
    "\n",
    "for i_sent, sent in enumerate(tp.tokens):\n",
    "    for i_token, token in enumerate(sent):\n",
    "        token = token.lower()\n",
    "        synsets = wncr.synsets(token)\n",
    "        print(token, tp.tags[i_sent][i_token], synsets)\n",
    "        if len(synsets) > 0:                                                                                 # all words \n",
    "            num_synsets += len(synsets)\n",
    "            num_tokens += 1\n",
    "        if tp.tags[i_sent][i_token] in {\"VERB\", \"NOUN\", \"PROPN\", \"ADP\", \"ADJ\", \"ADV\"} and len(synsets) > 0:   # function words \n",
    "            num_cont_synsets += len(synsets)\n",
    "            num_cont_tokens += 1\n",
    "\n",
    "print(num_synsets/num_tokens, num_cont_synsets/num_cont_tokens)  # Make sure that these are the same as the function returns. \n",
    "print(polysemy_ambiguation(text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "automated-teddy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me llamo María Pérez y tengo diecinueve años. Nací en Málaga, pero vivo en Granada. Soy estudiante de primer curso de Periodismo. De lunes a viernes me levanto a las siete y media, desayuno y camino hasta la universidad. Entro en clase a las nueve y salgo a la una. Al medio día, como en mi casa y veo la televisión. Por la tarde, estudio hasta las siete y después quedo con mis amigas. A nosotras nos gusta mucho el cine, el teatro y la música. Los viernes por la noche cenamos pizza y bailamos en la discoteca. Todos los sábados visito a mi familia en Málaga. El domingo por la tarde regreso a Granada y, si hace sol, salgo con mi perro a dar un paseo. ¡Me encantan los animales!\n",
      "me PRON []\n",
      "llamo VERB []\n",
      "maría PROPN [Synset('chocolate.n.01'), Synset('mary_jane.n.01'), Synset('hierba.n.07'), Synset('maría.n.04')]\n",
      "pérez PROPN []\n",
      "y CCONJ [Synset('y.n.01')]\n",
      "tengo VERB []\n",
      "diecinueve NUM []\n",
      "años NOUN [Synset('años.n.01'), Synset('año.n.01'), Synset('año.n.02'), Synset('año.n.03')]\n",
      ". PUNCT []\n",
      "nací PROPN []\n",
      "en ADP [Synset('en.n.01')]\n",
      "málaga PROPN []\n",
      ", PUNCT []\n",
      "pero CCONJ []\n",
      "vivo VERB [Synset('vivo.a.01'), Synset('viviente.a.01'), Synset('animado.a.01'), Synset('intenso.a.01'), Synset('vivo.a.05'), Synset('brillante.a.06'), Synset('intenso.a.03'), Synset('vivo.a.08'), Synset('viviente.a.02'), Synset('despierto.a.03'), Synset('vivo.a.11')]\n",
      "en ADP [Synset('en.n.01')]\n",
      "granada PROPN [Synset('granada.n.01'), Synset('granada.n.02'), Synset('granada.n.03')]\n",
      ". PUNCT []\n",
      "soy AUX []\n",
      "estudiante NOUN [Synset('alumna.n.01')]\n",
      "de ADP [Synset('de.n.01')]\n",
      "primer ADJ []\n",
      "curso NOUN [Synset('asignatura.n.01'), Synset('clase.n.11'), Synset('curso.n.03'), Synset('corriente.n.06'), Synset('camino.n.10'), Synset('curso.n.06'), Synset('camino.n.11'), Synset('curso.n.08'), Synset('camino.n.12')]\n",
      "de ADP [Synset('de.n.01')]\n",
      "periodismo PROPN [Synset('periodismo.n.01'), Synset('medios_periodísticos.n.01')]\n",
      ". PUNCT []\n",
      "de ADP [Synset('de.n.01')]\n",
      "lunes NOUN [Synset('lunes.n.01')]\n",
      "a ADP [Synset('a.n.01')]\n",
      "viernes NOUN [Synset('viernes.n.01')]\n",
      "me PRON []\n",
      "levanto VERB []\n",
      "a ADP [Synset('a.n.01')]\n",
      "las DET [Synset('la.n.01')]\n",
      "siete NUM []\n",
      "y CCONJ [Synset('y.n.01')]\n",
      "media NUM [Synset('media.n.01'), Synset('media.n.02'), Synset('media.n.03')]\n",
      ", PUNCT []\n",
      "desayuno NOUN [Synset('almuerzo.n.02'), Synset('almuerzo.n.04')]\n",
      "y CCONJ [Synset('y.n.01')]\n",
      "camino NOUN [Synset('camino.n.01'), Synset('camino.n.02'), Synset('camino.n.03'), Synset('camino.n.04'), Synset('camino.n.05'), Synset('camino.n.06'), Synset('camino.n.07'), Synset('camino.n.08'), Synset('camino.n.09'), Synset('camino.n.10'), Synset('camino.n.11'), Synset('camino.n.12')]\n",
      "hasta ADP []\n",
      "la DET [Synset('la.n.01')]\n",
      "universidad NOUN [Synset('universidad.n.01'), Synset('universidad.n.02'), Synset('universidad.n.03')]\n",
      ". PUNCT []\n",
      "entro VERB []\n",
      "en ADP [Synset('en.n.01')]\n",
      "clase NOUN [Synset('asignatura.n.01'), Synset('clase.n.02'), Synset('clase.n.03'), Synset('clase.n.04'), Synset('clase.n.05'), Synset('clase.n.06'), Synset('clase.n.07'), Synset('clase.n.08'), Synset('categoría.n.02'), Synset('clase.n.10'), Synset('clase.n.11')]\n",
      "a ADP [Synset('a.n.01')]\n",
      "las DET [Synset('la.n.01')]\n",
      "nueve NOUN [Synset('nueve.n.01')]\n",
      "y CCONJ [Synset('y.n.01')]\n",
      "salgo VERB []\n",
      "a ADP [Synset('a.n.01')]\n",
      "la DET [Synset('la.n.01')]\n",
      "una PRON []\n",
      ". PUNCT []\n",
      "al ADP [Synset('al.n.01')]\n",
      "medio NUM [Synset('medio.n.01'), Synset('manera.n.03'), Synset('centro.n.04'), Synset('medio.n.04'), Synset('medio.n.05'), Synset('centro.n.06'), Synset('centro.n.07'), Synset('medio.n.08'), Synset('entorno.n.02'), Synset('corazón.n.08'), Synset('entorno.n.03'), Synset('alrededores.n.01'), Synset('medio.n.13'), Synset('medio.n.14'), Synset('medio.a.01'), Synset('medio.a.02'), Synset('medio.a.03'), Synset('intermedio.a.05'), Synset('intermedio.a.06'), Synset('medio.a.06')]\n",
      "día NOUN [Synset('día.n.01'), Synset('día.n.02'), Synset('día.n.03'), Synset('día.n.04'), Synset('día.n.05'), Synset('día.n.06')]\n",
      ", PUNCT []\n",
      "como SCONJ []\n",
      "en ADP [Synset('en.n.01')]\n",
      "mi DET []\n",
      "casa NOUN [Synset('alojamiento.n.02'), Synset('casa.n.02'), Synset('casa.n.03'), Synset('casa.n.04'), Synset('casa.n.05'), Synset('casa.n.06'), Synset('casa.n.07'), Synset('casa.n.08'), Synset('casa.n.09'), Synset('casa.n.10'), Synset('casa.n.11')]\n",
      "y CCONJ [Synset('y.n.01')]\n",
      "veo VERB []\n",
      "la DET [Synset('la.n.01')]\n",
      "televisión NOUN [Synset('televisión.n.01'), Synset('tele.n.01'), Synset('tele.n.02')]\n",
      ". PUNCT []\n",
      "por ADP [Synset('por.r.01')]\n",
      "la DET [Synset('la.n.01')]\n",
      "tarde NOUN [Synset('tarde.n.01')]\n",
      ", PUNCT []\n",
      "estudio NOUN [Synset('estudio.n.01'), Synset('apart_hotel.n.01'), Synset('estudio.n.03'), Synset('estudio.n.04'), Synset('estudio.n.05'), Synset('estudio.n.06'), Synset('estudio.n.07'), Synset('cogitación.n.01'), Synset('atención.n.10'), Synset('area_de_jurisdicción.n.01'), Synset('estudio.n.11'), Synset('estudio.n.12')]\n",
      "hasta ADP []\n",
      "las DET [Synset('la.n.01')]\n",
      "siete NUM []\n",
      "y CCONJ [Synset('y.n.01')]\n",
      "después ADV []\n",
      "quedo VERB []\n",
      "con ADP []\n",
      "mis DET []\n",
      "amigas NOUN [Synset('aficionado.n.04'), Synset('amiga.n.02'), Synset('amante.n.04'), Synset('aficionado.n.08')]\n",
      ". PUNCT []\n",
      "a ADP [Synset('a.n.01')]\n",
      "nosotras INTJ []\n",
      "nos PRON [Synset('no.n.01')]\n",
      "gusta VERB []\n",
      "mucho ADV [Synset('mucho.r.01'), Synset('muchisimo.r.01'), Synset('intensamente.r.01')]\n",
      "el DET []\n",
      "cine NOUN [Synset('cine.n.01'), Synset('celuloide.n.01')]\n",
      ", PUNCT []\n",
      "el DET []\n",
      "teatro NOUN [Synset('teatro.n.01'), Synset('escenario.n.02'), Synset('teatro.n.03'), Synset('espectáculo.n.03'), Synset('teatro.n.05'), Synset('escena.n.03')]\n",
      "y CCONJ [Synset('y.n.01')]\n",
      "la DET [Synset('la.n.01')]\n",
      "música NOUN [Synset('música.n.01'), Synset('música.n.02'), Synset('música.n.03')]\n",
      ". PUNCT []\n",
      "los DET []\n",
      "viernes NOUN [Synset('viernes.n.01')]\n",
      "por ADP [Synset('por.r.01')]\n",
      "la DET [Synset('la.n.01')]\n",
      "noche NOUN [Synset('nox.n.01'), Synset('noche.n.02'), Synset('noche.n.03'), Synset('anochecer.n.01'), Synset('noche.n.05'), Synset('noche.n.06'), Synset('noche.n.07'), Synset('noche.n.08')]\n",
      "cenamos VERB []\n",
      "pizza NOUN []\n",
      "y CCONJ [Synset('y.n.01')]\n",
      "bailamos VERB []\n",
      "en ADP [Synset('en.n.01')]\n",
      "la DET [Synset('la.n.01')]\n",
      "discoteca NOUN [Synset('disco.n.01')]\n",
      ". PUNCT []\n",
      "todos DET [Synset('todo.n.01'), Synset('camas_y_petacas.n.01')]\n",
      "los DET []\n",
      "sábados NOUN [Synset('sábado.n.01')]\n",
      "visito AUX []\n",
      "a ADP [Synset('a.n.01')]\n",
      "mi DET []\n",
      "familia NOUN [Synset('clan.n.01'), Synset('familia.n.02'), Synset('familia.n.03'), Synset('categoría.n.02'), Synset('casa.n.07'), Synset('familia.n.06')]\n",
      "en ADP [Synset('en.n.01')]\n",
      "málaga PROPN []\n",
      ". PUNCT []\n",
      "el DET []\n",
      "domingo NOUN [Synset('domingo.n.01')]\n",
      "por ADP [Synset('por.r.01')]\n",
      "la DET [Synset('la.n.01')]\n",
      "tarde NOUN [Synset('tarde.n.01')]\n",
      "regreso NOUN [Synset('regreso.n.01'), Synset('regreso.n.02'), Synset('regreso.n.03')]\n",
      "a ADP [Synset('a.n.01')]\n",
      "granada PROPN [Synset('granada.n.01'), Synset('granada.n.02'), Synset('granada.n.03')]\n",
      "y CCONJ [Synset('y.n.01')]\n",
      ", PUNCT []\n",
      "si SCONJ [Synset('si.n.01')]\n",
      "hace VERB []\n",
      "sol NOUN [Synset('sol.n.01'), Synset('sol.n.02'), Synset('sol.n.03'), Synset('amado.n.01'), Synset('sol.n.05'), Synset('luz_del_sol.n.01'), Synset('sol.n.07')]\n",
      ", PUNCT []\n",
      "salgo VERB []\n",
      "con ADP []\n",
      "mi DET []\n",
      "perro NOUN [Synset('can.n.01'), Synset('perro.n.02')]\n",
      "a ADP [Synset('a.n.01')]\n",
      "dar VERB [Synset('dar.v.01'), Synset('dar.v.02'), Synset('dar.v.03'), Synset('dar.v.04'), Synset('dar.v.05'), Synset('dar.v.06'), Synset('dar.v.07'), Synset('dar.v.08'), Synset('dar.v.09'), Synset('dar.v.10'), Synset('arrojar.v.10'), Synset('dar.v.12'), Synset('dar.v.13'), Synset('dar.v.14'), Synset('dar.v.15'), Synset('dar.v.16'), Synset('dar.v.17'), Synset('dar.v.18'), Synset('dar.v.19'), Synset('ceder.v.06'), Synset('dar.v.21'), Synset('dar.v.22'), Synset('conceder.v.09'), Synset('dar.v.24'), Synset('aplicar.v.03'), Synset('causar.v.06'), Synset('conceder.v.12'), Synset('dar.v.28'), Synset('dar.v.29'), Synset('dar.v.30'), Synset('dar.v.31'), Synset('dar.v.32')]\n",
      "un DET []\n",
      "paseo NOUN [Synset('paseo.n.01'), Synset('paseo.n.02'), Synset('caminata.n.01'), Synset('paseo.n.04'), Synset('caminata.n.02'), Synset('alameda.n.01'), Synset('camino.n.07')]\n",
      ". PUNCT []\n",
      "¡ PUNCT []\n",
      "me PRON []\n",
      "encantan AUX []\n",
      "los DET []\n",
      "animales NOUN [Synset('animales.n.01')]\n",
      "! PUNCT []\n",
      "3.0714285714285716 3.5762711864406778\n",
      "(3.0714285714285716, 3.5762711864406778)\n"
     ]
    }
   ],
   "source": [
    "text = corpus['A1'][0]['content']\n",
    "print(text)\n",
    "\n",
    "tp = text_processor(text)\n",
    "num_synsets, num_tokens = 0, 0\n",
    "num_cont_synsets, num_cont_tokens = 0, 0\n",
    "\n",
    "for i_sent, sent in enumerate(tp.tokens):\n",
    "    for i_token, token in enumerate(sent):\n",
    "        token = token.lower()\n",
    "        synsets = wncr.synsets(token)\n",
    "        print(token, tp.tags[i_sent][i_token], synsets)\n",
    "        if len(synsets) > 0:\n",
    "            num_synsets += len(synsets)\n",
    "            num_tokens += 1\n",
    "        if tp.tags[i_sent][i_token] in {\"VERB\", \"NOUN\", \"PROPN\", \"ADP\", \"ADJ\", \"ADV\"} and len(synsets) > 0:\n",
    "            num_cont_synsets += len(synsets)\n",
    "            num_cont_tokens += 1\n",
    "\n",
    "print(num_synsets/num_tokens, num_cont_synsets/num_cont_tokens) # Make sure that these are the same as the function returns. \n",
    "print(polysemy_ambiguation(text)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-acceptance",
   "metadata": {},
   "source": [
    "#### Test on small examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "satellite-sacramento",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capítulo I \n",
      "Que trata de la condición y ejercicio del famoso\n",
      "hi dalgo don Quijote de la Mancha  \n",
      " de cuyo nombre no \n",
      "quiero acordarme, no ha mucho tiempo que vivía un hidalgo \n",
      "de los de lanza en astillero, adarga antigua, rocín flaco y galgo \n",
      "corredor. Una olla de algo más vaca que carnero, salpicón las \n",
      "más noches, duelos y quebrantos los sábados, lentejas los \n",
      "viernes, algún palomino de añadidura los domingos, consumían \n",
      "las tres partes de su hacienda. El resto della concluían sayo \n",
      "de velarte, calzas de velludo para las fiestas, con sus pantu-\n",
      "flos de lo mesmo, y los días de entresemana se honraba consu \n",
      "vellorí de lo más fino. Tenía en su casa una ama que pasaba  \n",
      "de los cuarenta, y una sobrina que no llegaba a los veinte, y \n",
      "un mozo de campo y plaza, que así ensillaba el rocín como \n",
      "tomaba la podadera. Frisaba la edad de nuestro hidalgo con los  \n",
      "cincuenta años; era de complexión recia, seco de carnes, enjuto de \n",
      "rostro, gran madrugador y amigo de la caza. Quieren decir que tenía el \n",
      "sobrenombre \n",
      "de Quijada o \n",
      "Quesada, que en \n",
      "esto hay alguna  \n",
      "diferencia en los\n",
      "autores que deste \n",
      "caso escriben; \n",
      "aunque, por\n",
      "conjeturas vero-\n",
      "símiles, se deja \n",
      "entender que se \n",
      "lla maba Quejana. \n",
      "Pero esto importa\n",
      "poco a nuestro  \n",
      "cuento; basta \n",
      "que en la narra-\n",
      "ción dél no se \n",
      "salga un punto \n",
      "de la verdad.  Es, pues, de saber que este sobredicho hidalgo, los ratos que \n",
      "estaba ocioso, que eran los más del año, se daba a leer libros de \n",
      "caballerías, con tanta afición y gusto, que olvidó casi de todo \n",
      "punto el ejercicio de la caza y aun la administración de su       \n",
      "hacienda; y llegó a tanto su curiosidad y desatino en esto, que \n",
      "vendió muchas hanegas de tierra de sembradura para  \n",
      "comprar libros de caballerías en que leer, y así, llevó a su casa  \n",
      "todos cuantos pudo haber dellos; y de todos, ningunos le parecían \n",
      "tan bien como los que compuso el famoso Feliciano de Silva, porque \n",
      "la claridad de su prosa y aquellas entricadas razones suyas le parecían \n",
      "de perlas, y más cuando llegaba a leer aquellos requiebros y cartas \n",
      "de desafíos, donde en muchas partes hallaba escrito: La razón de la \n",
      "sinrazón que a mi razón se hace, de tal manera mi razón enflaquece, que \n",
      "con razón me quejo de la vuestra fermosura*. Y también cuando leía; \n",
      "…los altos cielos que de vuestra divinidad divinamente con las estrellas os \n",
      "fortifican y os hacen merecedora del merecimiento que merece la vuestra  \n",
      "grandeza. \n",
      " Con estas razones perdía el pobre caballero el juicio, \n",
      "y desvelábase por entenderlas y desentrañarles el sentido,  \n",
      "que no se lo sacara ni las entendiera el mesmo Aristóteles, si     \n",
      "resucitara para sólo ello. No estaba muy bien con las heridas \n",
      "que don Belianís daba y recebía, porque se imaginaba que, por  \n",
      "grandes maestros que le hubiesen curado, no dejaría de tener  \n",
      "el rostro y todo el cuerpo lleno de cicatrices y señales. Pero, \n",
      "con todo, alababa en su autor aquel acabar su libro con la   \n",
      "promesa de aquella inacabable aventura, y muchas veces le vino \n",
      "deseo de tomar la pluma y dalle fin al pie de la letra, como allí \n",
      "se promete; y sin duda alguna lo hiciera, y aun saliera con ello, \n",
      "si otros mayores y continuos pensamientos no se lo estorbaran. \n",
      "Tuvo muchas veces competencia con el cura de su lugar —que \n",
      "era hombre docto, graduado en Sigüenza—, sobre cuál había \n",
      "sido mejor caballero; Palmerín de Inglaterra o Amadís de \n",
      "Gaula; mas maese Nicolás, barbero del mesmo pueblo, decía \n",
      "que     ninguno llegaba al Caballero del Febo, y que si alguno se le  \n",
      "podía comparar, era don Galaor, hermano de Amadís de Gaula, \n",
      "porque tenía muy acomodada condición para todo; que no eracaballero melindroso, ni tan llorón como su hermano, y que en lo \n",
      "de la valentía no le iba en zaga. \n",
      " En resolución, él se enfrascó tanto en su lectura, que se \n",
      "le pasaban las noches leyendo de claro en claro, y los días de     \n",
      "turbio en turbio; y así, del poco dormir y del mucho leer, se  \n",
      "le secó el cerebro, de manera que vino a perder el juicio.      \n",
      "Llenósele la fantasía de todo aquello que leía en los libros, así de \n",
      "encantamientos como de pendencias, batallas, desafíos, heridas, \n",
      "requiebros, amores, tormentas y disparates imposibles; y \n",
      "asentósele de tal modo en la imaginación que era verdad toda \n",
      "aquella máquina de aquellas sonadas soñadas invenciones que \n",
      "leía, que para él no había otra historia más cierta en el mundo. \n",
      "Decía él que el Cid Ruy Díaz había sido muy buen caballero,   \n",
      "pero que no tenía que ver con el Caballero de la Ardiente \n",
      "Espada, que de sólo un revés había partido por medio dos  \n",
      "fieros y descomunales gigantes. Mejor estaba con Bernardo \n",
      "del Carpio, porque en Roncesvalles había muerto a Roldán, el \n",
      "encantado, valiéndose de la industria de Hércules, cuando ahogó \n",
      "a Anteo, el hijo de la Tierra, entre los brazos. Decía mucho bien \n",
      "del gigante Morgante, porque, con ser de aquella generación  \n",
      "gigantea, que todos son soberbios y descomedidos, él solo  \n",
      "era afable y bien criado. Pero, sobre todos, estaba bien con   \n",
      "Reinaldos de Montalbán, y más cuando le veía salir de su castillo \n",
      "y robar cuantos topaba, y cuando en allende robó aquel ídolo  \n",
      "de Mahoma que era todo de oro, según dice su historia. Diera él, \n",
      "por dar una mano de coces al traidor de Galalón, al ama que tenía \n",
      "y aun a su sobrina de añadidura. \n",
      " En efecto, rematado ya su juicio, vino a dar en el más       \n",
      "extraño pensamiento que jamás dio loco en el mundo; y fue \n",
      "que le pareció convenible y necesario, así para el aumento de \n",
      "su honra como para el servicio de su república, hacerse caballero \n",
      "andante, e irse por todo el mundo con sus armas y caballo \n",
      "a buscar las aventuras y a ejercitarse en todo aquello que él  \n",
      "había leído que los caballeros andantes se ejercitaban, deshaciendo todo género de agravio, y poniéndose en ocasiones y peligros \n",
      "donde, acabándolos, cobrase eterno nombre y fama Imaginábase el \n",
      "pobre ya coronado por el valor de su brazo, por lo menos, del imperio \n",
      "de Trapisonda; y así, con estos tan agradables pensamientos, llevado \n",
      "del extraño gusto que en ellos sentía, se dio priesa a poner en efeto \n",
      "lo que deseaba. \n",
      " Y lo primero que hizo fue limpiar unas armas que habían \n",
      "sido de sus bisabuelos, que, tomadas de orín y llenas de moho, \n",
      "luengos siglos había que estaban puestas y olvidadas en un \n",
      "rincón. Limpiólas y aderezólas lo mejor que pudo, pero \n",
      "vio que tenían una gran falta, y era que no tenían celada de        \n",
      "encaje, sino morrión simple (era solamente media celada);  \n",
      "mas a esto suplió su industria, porque de cartones hizo un  \n",
      "modo de media celada (una visera), que, encajada con el         \n",
      "morrión, hacían una apariencia de celada entera. Es verdad que  \n",
      "para probar si era fuerte y podía estar al riesgo de una  \n",
      "cuchillada, sacó su espada y le dio dos golpes, y con el primero  \n",
      "y en un punto deshizo lo que había hecho en una semana; y no \n",
      "dejó de parecerle mal la facilidad con que la había hecho       \n",
      "pedazos, y, por asegurarse deste peligro, la tornó a hacer de nuevo, \n",
      "poniéndole unas barras de hierro por de dentro, de tal manera \n",
      "que él quedó satisfecho de su fortaleza; y, sin querer hacer nueva \n",
      "experiencia della, la diputó y tuvo por celada finísima de encaje. \n",
      " Fue luego a ver su rocín, y, aunque tenía más cuartos que un \n",
      "real y más tachas que el caballo de Gonela, que tantum pellis et \n",
      "ossa fuit, le pareció que ni el Bucéfalo de Alejandro ni Babieca el \n",
      "del Cid con él se igualaban. Cuatro días se le pasaron en imaginar \n",
      "qué nombre le pondría; porque, según se decía él a sí mesmo, no \n",
      "era razón que caballo de caballero tan famoso, y tan bueno él por sí, \n",
      "estuviese sin nombre conocido; y ansí, procuraba acomodársele de \n",
      "manera que declarase quién había sido, antes que fuese de caballero \n",
      "andante, y lo que era  entonces; pues estaba muy puesto en razón que, \n",
      "mudando su señor estado, mudase él también el nombre, y cobrase   \n",
      "famoso y de estruendo, como convenía a la nueva orden y al nuevo \n",
      "ejercicio que ya profesaba. Y así, después de muchos nombres  \n",
      "que formó, borró y quitó, añadió, deshizo y tornó a hacer en   \n",
      "su memoria e imaginación, al fin le vino a llamar Rocinante, \n",
      "nombre, a su parecer, alto, sonoro y significativo de lo que  \n",
      "había sido cuando fue rocín, antes de lo que ahora era, que era \n",
      "antes y primero de todos los rocines del mundo. Puesto nombre, y tan a su gusto, a su caballo, quiso  \n",
      "ponérsele a sí mismo, y en este pensamiento duró otros ocho  \n",
      "días, y al cabo se vino a llamar don Quijote; de donde, como  \n",
      "queda dicho, tomaron ocasión los autores desta tan verdadera \n",
      "historia que, sin duda, se debía de llamar Quijada, y no Quesada, \n",
      "como otros quisieron decir. Pero, acordándose que el valeroso  \n",
      "Amadís no sólo se había contentado con llamarse Amadís  \n",
      "a secas, sino que añadió el nombre de su reino y patria, por \n",
      "Hepila famosa, y se llamó Amadís de Gaula, así quiso, como \n",
      "buen caballero, añadir al suyo el nombre de la suya y llamarse \n",
      "don Quijote de la Mancha, con que, a su parecer, declaraba muy al  \n",
      "vivo su linaje y patria, y la honraba con tomar el sobrenombre  \n",
      "della. \n",
      " Limpias, pues, sus armas, hecho del morrión celada, \n",
      "puesto nombre a su rocín y confirmándose a sí mismo, se dio a \n",
      "entender que no le faltaba otra cosa sino buscar una dama de quien \n",
      "enamorarse; porque el caballero andante sin amores era árbol sin \n",
      "hojas y sin fruto y cuerpo sin alma. Decíase él: \n",
      " —Si yo, por malos de mis pecados, o por mi buena suerte, \n",
      "me encuentro por ahí con algún gigante, como de ordinario les  \n",
      "acontece a los caballeros andantes, y le derribo de un encuentro, o le \n",
      "parto por mitad del cuerpo, o, finalmente, le venzo y le rindo, \n",
      "rendido.\n",
      " —Yo, señora, soy el gigante Caraculiambro, señor de la \n",
      "ínsula Malindrania, a quien venció en singular batalla el jamás  \n",
      "como se debe alabado caballero don Quijote de la Mancha, el  \n",
      "cual me mandó que me presentase ante vuestra merced, para  \n",
      "que la vuestra grandeza disponga de mí a su talante. Capítulo 2  \n",
      " ¡Oh, cómo se holgó nuestro buen caballero cuando hubo \n",
      "hecho este discurso, y más cuando halló a quien dar nombre de \n",
      "su dama! Y fue, a lo que se cree, que en un lugar cerca del suyo \n",
      "había una moza labradora de muy buen parecer, de quien él un \n",
      "tiempo anduvo enamorado, aunque, según se entiende, ella jamás \n",
      "lo supo, ni le dio cata dello. Llamábase Aldonza Lorenzo, y a \n",
      "ésta le pareció ser bien darle título de señora de sus pensamientos; \n",
      "y, buscándole nombre que no desdijese mucho del suyo, y que \n",
      "tirase y se encaminase al de princesa y gran señora, vino a lla marla \n",
      "Dulcinea del Toboso, porque era natural del Toboso; nombre, a su \n",
      "parecer, músico y peregrino y significativo, como todos los demás \n",
      "que a él y a sus cosas había puesto. \n",
      "3.5323974082073435 4.126062322946176\n",
      "(3.5323974082073435, 4.126062322946176)\n"
     ]
    }
   ],
   "source": [
    "text = corpus['A2'][0]['content']\n",
    "print(text)\n",
    "\n",
    "tp = text_processor(text)\n",
    "num_synsets, num_tokens = 0, 0\n",
    "num_cont_synsets, num_cont_tokens = 0, 0\n",
    "\n",
    "for i_sent, sent in enumerate(tp.tokens):\n",
    "    for i_token, token in enumerate(sent):\n",
    "        token = token.lower()\n",
    "        synsets = wncr.synsets(token)\n",
    "        #print(token, tp.tags[i_sent][i_token], synsets)\n",
    "        if len(synsets) > 0:\n",
    "            num_synsets += len(synsets)\n",
    "            num_tokens += 1\n",
    "        if tp.tags[i_sent][i_token] in {\"VERB\", \"NOUN\", \"PROPN\", \"ADP\", \"ADJ\", \"ADV\"} and len(synsets) > 0:\n",
    "            num_cont_synsets += len(synsets)\n",
    "            num_cont_tokens += 1\n",
    "\n",
    "print(num_synsets/num_tokens, num_cont_synsets/num_cont_tokens) # Make sure that these are the same as the function returns. \n",
    "print(polysemy_ambiguation(text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "collect-money",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mark está de viaje de negocios en Barcelona. Hoy tuvo un día libre y salió a visitar la ciudad. Primero, caminó por La Rambla, la calle más famosa de Barcelona, llena de gente, tiendas y restaurantes. Se dirigió al Barrio Gótico, uno de los sitios más antiguos y bellos de la ciudad. En la Plaza Sant Jaume observó dos de los edificios más importantes: El Palacio de la Generalitat de Catalunya y el Ayuntamiento. Volvió a La Rambla. Mark tenía hambre y se detuvo a comer unas tapas y beber una cerveza. Continuó hasta la grande y hermosa Plaza de Catalunya. Avanzó por el Paseo de Gràcia hasta llegar a un edificios fuera de lo común Casa Batlló y luego a Casa Milà, diseños del arquitecto Antoni Gaudí. Quiso saber más sobre este famoso arquitecto y se dirigió al Park Güell, donde tomó muchas fotografías. El día se acababa pero antes de volver al hotel, Mark tomó un taxi hacia la Fuente Mágica y disfrutó de un espectáculo de agua y luces. Mark quedó sorprendido con esta gran ciudad y sintió que le faltó tiempo para conocer más lugares interesantes. Se prometió regresar para tomar unas vacaciones con su familia.\n",
      "3.045045045045045 3.5681818181818183\n",
      "(3.045045045045045, 3.5681818181818183)\n"
     ]
    }
   ],
   "source": [
    "text = corpus['B1'][0]['content']\n",
    "print(text)\n",
    "\n",
    "tp = text_processor(text)\n",
    "num_synsets, num_tokens = 0, 0\n",
    "num_cont_synsets, num_cont_tokens = 0, 0\n",
    "\n",
    "for i_sent, sent in enumerate(tp.tokens):\n",
    "    for i_token, token in enumerate(sent):\n",
    "        token = token.lower()\n",
    "        synsets = wncr.synsets(token)\n",
    "        #print(token, tp.tags[i_sent][i_token], synsets)\n",
    "        if len(synsets) > 0:\n",
    "            num_synsets += len(synsets)\n",
    "            num_tokens += 1\n",
    "        if tp.tags[i_sent][i_token] in {\"VERB\", \"NOUN\", \"PROPN\", \"ADP\", \"ADJ\", \"ADV\"} and len(synsets) > 0:\n",
    "            num_cont_synsets += len(synsets)\n",
    "            num_cont_tokens += 1\n",
    "\n",
    "print(num_synsets/num_tokens, num_cont_synsets/num_cont_tokens) # Make sure that these are the same as the function returns. \n",
    "print(polysemy_ambiguation(text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "czech-prince",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un día hablaba el Conde Lucanor con Patronio, su consejero, y le decía:\n",
      "\n",
      "—Patronio, un pariente mío me ha dicho que lo quieren casar con una mujer muy rica, y aunque es más honrada que él, el casamiento sería muy bueno para él si no fuera por un embargo que ahí hay, y el embargo es éste:  Me dijo que le dijeron otros que aquella mujer era la más fuerte y la más brava cosa del mundo, y ahora ruego a vos que me aconsejéis si le mande que case con aquella mujer—pues sabe de cual manera es—, o si le mande que lo no haga.\n",
      "\n",
      "—Señor conde Lucanor —dijo Patronio— si él fuera tal como fue un hijo de un hombre bueno que era moro, aconsejadle que case con ella; más si no fuere tal, no se lo aconseja. Y el conde le rogó que le dijera cómo era aquello.\n",
      "\n",
      "Patronio le dijo que en una villa vivía un moro honrado que vivía con un hijo, el mejor mancebo que en el mundo podría ser, pero no era tan rico que pudiese cumplir varios proyectos que quería hacer.  Por eso el moro estaba muy preocupado, porque tenía la voluntad y no tenía el poder.\n",
      "\n",
      "En aquella misma villa vivió otro moro mucho más honrado y más rico que el padre del mancebo, que sólo tenía una hija, y era de carácter muy distinto al de aquel mancebo, que cuanto en él había de buenas maneras, tanto lo tenía aquella hija del hombre bueno de malas, por lo cual ningún hombre en el mundo querría casarse con aquel diablo.\n",
      "\n",
      "Aquel mancebo tan bueno fue un día a su padre y le dijo que bien sabía él que no era tan rico que pudiese darle con que él pudiese vivir a su honra, y que pues le convenía o pasar miseria y pobreza o irse de aquella tierra. Por lo tanto, le preguntaba si a él le parecía que era más inteligente buscar algún casamiento con el que pudiese mantenerse y pasar la vida. El padre le dijo que le placería mucho poder hallarle un matrimonio ventajoso.\n",
      "\n",
      "Le dijo el hijo a su padre que, si él quería, podía arreglar que aquel hombre bueno, que tenía aquella hija tan mala, se la diese por esposa. Y cuando el padre oyó esto fue muy maravillado y le dijo que cómo podía pensar en tal cosa, que no había hombre que la conociese que, por pobre que fuera, quisiera casarse con ella. El hijo le dijo que hiciese el favor de concertar aquel matrimonio. Tanto le insistió que, aunque al padre le pareció algo muy extraño, le dijo que lo haría.\n",
      "\n",
      "Marchó luego a casa de aquel buen hombre, del que era muy amigo, y le contó todo lo que había pasado con su hijo, que se atrevía a casarse con su hija, que le gustaba, y que se la diera en matrimonio. Cuando el buen hombre oyó hablar así a su amigo, le contestó:\n",
      "\n",
      "—Por Dios, amigo, si yo autorizara esa boda sería vuestro peor amigo, pues tratándose de vuestro hijo, que es muy bueno, yo pensaría que le hacía grave daño al consentir su perjuicio o su muerte, porque estoy seguro de que, si se casa con mi hija, morirá, o su vida con ella será peor que la misma muerte. Mas no penséis que os digo esto por no aceptar vuestra petición, pues, si la queréis como esposa de vuestro hijo, a mí mucho me contentará entregarla a él o a cualquiera que se la lleve de esta casa.\n",
      "\n",
      "Su amigo le respondió que le agradecería mucho su advertencia, pero, como su hijo insistía en casarse con ella, le volvía a pedir su consentimiento.\n",
      "\n",
      "El casamiento fue hecho, llevaron a la novia a casa de su marido y, como eran moros, siguiendo sus costumbres les prepararon la cena, les pusieron la mesa y los dejaron solos hasta la mañana siguiente. Pero los padres y las madres y parientes del novio y de la novia estaban con mucho miedo, pues pensaban que al día siguiente encontrarían al joven muerto o muy mal herido.\n",
      "\n",
      "Al quedarse los novios solos en su casa, se sentaron a la mesa y, antes de que ella pudiese decir nada, miró el novio a una y otra parte de la mesa y, al ver un perro, le dijo ya bastante airado:\n",
      "\n",
      "—¡Perro, danos agua para las manos!\n",
      "\n",
      "El perro no lo hizo. El mancebo comenzó a enfadarse y le dijo más bravamente que les trajese agua para las manos. Pero el perro no lo hizo. Viendo que el perro no lo hacía, el joven se levantó muy enfadado de la mesa y agarró la espada y fue directo hacia el perro. Y cuando el perro lo vio venir hacia sí, comenzó a huir, y él en pos del perro, saltando ambos por la ropa y por la mesa, y por el fuego, y tanto anduvo en pos de él hasta alcanzarlo. Lo sujetó y le cortó la cabeza, las patas y las manos, haciéndolo pedazos y ensangrentando toda la casa, la ropa y la mesa.\n",
      "\n",
      "Después, muy enojado y ensangrentado, volvió a sentarse a la mesa y miró en derredor. Vio un gato, al que mandó que trajese agua para las manos; y porque el gato no lo hacía, le gritó:\n",
      "\n",
      "—¡Cómo, falso traidor! ¿No viste lo que hice con el perro por no obedecerme? Yo prometo que, si un punto más disputas conmigo, que tendrás el mismo destino que el perro.\n",
      "\n",
      "El gato no lo hizo y así se levantó el mancebo, lo cogió por las patas y lo estrelló contra la pared, haciendo de él más de cien pedazos y demostrando con él mayor ensañamiento que con el perro.\n",
      "\n",
      "Así, enfadado y colérico, volvió a la mesa y miró a todas partes. La mujer, al verlo hacer todo esto, pensó que se había vuelto loco y no decía nada.\n",
      "\n",
      "Después de mirar por todas partes, vio a su caballo, que estaba en la casa y, aunque era el único que tenía, le dijo muy bravamente que les trajese agua para las manos; pero el caballo no le hizo. Al ver que no lo hacía, le dijo:\n",
      "\n",
      "—¡Cómo, don caballo! Solamente porque no hay otro caballo, ¿por eso os dejaré si no hacéis lo que yo os mande?…tan mala muerte os daré como a los otros, y no hay cosa viva en el mundo que no haga lo que yo mande, que eso mismo no le haré.\n",
      "\n",
      "El caballo estuvo quieto. Cuando el mancebo vio que el caballo no le obedecía, se acercó a él, le cortó la cabeza con mucha rabia y luego lo hizo pedazos.\n",
      "\n",
      "Cuando la mujer vio que mataba al caballo, aunque no tenía otro, y que decía que haría lo mismo con quien no le obedeciese, pensó que no se trataba de una broma y le entró tantísimo miedo que no sabía si estaba viva o muerta.\n",
      "\n",
      "Él, así—bravo, furioso y ensangrentado—, volvió a la mesa, jurando que, si mil caballos, hombres o mujeres hubiera en su casa que no le hicieran caso, los mataría a todos. Se sentó y miró a un lado y a otro, con la espada llena de sangre en el regazo; cuando hubo mirado muy bien, al no ver a ningún ser vivo sino a su mujer, volvió la mirada hacia ella muy bravamente y le dijo con muchísima furia, mostrándole la espada en su mano:\n",
      "\n",
      "—Levantados y dadme agua para las manos.\n",
      "\n",
      "La mujer, que no esperaba otra cosa sino que la despedazara toda, se levantó muy apriesa y le dio el agua para las manos. Él le dijo:\n",
      "\n",
      "—¡Ah! ¡Cuánto agradezco a Dios porque habéis hecho lo que os mandé! Porque de otra guisa, habría hecho con vos lo mismo que con ellos.\n",
      "\n",
      "Después le mandó que le sirviese la comida y ella lo hizo, y con tal son se lo decía que ella ya pensaba que su cabeza era ida por el polvo. Y así pasó el hecho entre ellos aquella noche.\n",
      "\n",
      "Así ocurrió entre los dos aquella noche, y nunca hablaba ella sino que se limitaba a obedecer a su marido. Cuando ya habían dormido un rato, le dijo él:\n",
      "\n",
      "—Con tanta ira como tuve esta noche, no puedo dormir bien. Procurad que mañana no me despierte nadie y preparadme un buen desayuno.\n",
      "\n",
      "Cuando aún era muy temprano, los padres, madres y parientes se acercaron a la puerta y, como no se oía a nadie, pensaron que el novio estaba muerto o herido. Y vieron entre las puertas a la novia y no al novio, y  su temor se hizo muy grande.\n",
      "\n",
      "Ella, al verlos junto a la puerta, se les acercó muy despacio y, con gran miedo, comenzó a decirles:\n",
      "\n",
      "—¡Ingratos! ¡Qué hacéis! ¿Qué hacéis ahí? ¿Cómo os atrevéis a llegar a esta puerta? ¿No os da miedo hablar? ¡Callaos, si no, todos moriremos, vosotros y yo!\n",
      "\n",
      "Al oírla decir esto, fueron muy maravillados. Cuando supieron lo ocurrido entre ellos aquella noche, sintieron gran estima por el mancebo porque sabía imponer su autoridad y hacerse él con el gobierno de su casa. Desde aquel día en adelante, fue su mujer muy obediente y llevaron muy buena vida.\n",
      "\n",
      "Pasados unos días, quiso su suegro hacer lo mismo que su yerno, y por aquella manera mató un gallo. Su mujer le dijo:\n",
      "\n",
      "—A la fe, don Fulano, tarde vos acordáis que ya bien nos conocemos.\n",
      "\n",
      "Y concluyó Patronio:\n",
      "\n",
      "—Vos, señor conde, si vuestro pariente quiere casarse con esa mujer y tiene el carácter de aquel mancebo, aconsejadle que lo haga, pues sabrá mandar en su casa; pero si no es así y no puede hacer todo lo necesario, debe dejar pasar esa oportunidad. También os aconsejo a vos que, cuando habéis de tratar con los demás hombres, les deis a entender desde el principio cómo han de portarse con vos.\n",
      "\n",
      "El conde vio que éste era un buen consejo, obró según él y le fue muy bien.\n",
      "\n",
      "Como don Juan comprobó que el cuento era bueno, lo mandó escribir en este libro e hizo estos versos que dicen así:\n",
      "3.662387676508344 4.460884353741497\n",
      "(3.662387676508344, 4.460884353741497)\n"
     ]
    }
   ],
   "source": [
    "text = corpus['B'][0]['content']\n",
    "print(text)\n",
    "\n",
    "tp = text_processor(text)\n",
    "num_synsets, num_tokens = 0, 0\n",
    "num_cont_synsets, num_cont_tokens = 0, 0\n",
    "\n",
    "for i_sent, sent in enumerate(tp.tokens):\n",
    "    for i_token, token in enumerate(sent):\n",
    "        token = token.lower()\n",
    "        synsets = wncr.synsets(token)\n",
    "        #print(token, tp.tags[i_sent][i_token], synsets)\n",
    "        if len(synsets) > 0:\n",
    "            num_synsets += len(synsets)\n",
    "            num_tokens += 1\n",
    "        if tp.tags[i_sent][i_token] in {\"VERB\", \"NOUN\", \"PROPN\", \"ADP\", \"ADJ\", \"ADV\"} and len(synsets) > 0:\n",
    "            num_cont_synsets += len(synsets)\n",
    "            num_cont_tokens += 1\n",
    "\n",
    "print(num_synsets/num_tokens, num_cont_synsets/num_cont_tokens) # Make sure that these are the same as the function returns. \n",
    "print(polysemy_ambiguation(text)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-prescription",
   "metadata": {},
   "source": [
    "### Run through Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "american-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_list = ['A1', 'A2', 'B', 'B1']\n",
    "level_degree_ambig = defaultdict(list)\n",
    "for level in level_list:\n",
    "    corpus_item = corpus[level]\n",
    "    for i, text_item in enumerate(corpus_item):\n",
    "        try: \n",
    "            am_avg, am_cont = polysemy_ambiguation(text_item['content'])\n",
    "            level_degree_ambig['level'].append(level)\n",
    "            level_degree_ambig['ambiguity_avg'].append(am_avg)    # pick out the most abstract word in each text \n",
    "            level_degree_ambig['ambiguity_cont'].append(am_cont)    # average abstractness in each text \n",
    "            \n",
    "        except:\n",
    "            print(f'Error', level, i)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-retailer",
   "metadata": {},
   "source": [
    "### Plot Distributions\n",
    "\n",
    "We plot the ambiguity distribution by text difficulty level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "closed-birmingham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-ca3fb7e834e84f0aa3e2ab6759ef6cec\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-ca3fb7e834e84f0aa3e2ab6759ef6cec\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-ca3fb7e834e84f0aa3e2ab6759ef6cec\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-027e704a06acf56bf8649009343713fd\"}, \"facet\": {\"type\": \"nominal\", \"field\": \"level\"}, \"spec\": {\"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"bin\": {\"maxbins\": 30}, \"field\": \"ambiguity_avg\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"count\"}}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-027e704a06acf56bf8649009343713fd\": [{\"level\": \"A1\", \"ambiguity_avg\": 3.0714285714285716, \"ambiguity_cont\": 3.5762711864406778}, {\"level\": \"A1\", \"ambiguity_avg\": 3.2403846153846154, \"ambiguity_cont\": 3.987012987012987}, {\"level\": \"A1\", \"ambiguity_avg\": 3.7948717948717947, \"ambiguity_cont\": 4.444444444444445}, {\"level\": \"A1\", \"ambiguity_avg\": 3.1666666666666665, \"ambiguity_cont\": 3.3870967741935485}, {\"level\": \"A1\", \"ambiguity_avg\": 4.388888888888889, \"ambiguity_cont\": 4.961038961038961}, {\"level\": \"A1\", \"ambiguity_avg\": 3.3451776649746194, \"ambiguity_cont\": 4.006535947712418}, {\"level\": \"A1\", \"ambiguity_avg\": 3.832579185520362, \"ambiguity_cont\": 4.44}, {\"level\": \"A1\", \"ambiguity_avg\": 3.415492957746479, \"ambiguity_cont\": 4.0186335403726705}, {\"level\": \"A1\", \"ambiguity_avg\": 3.0373626373626372, \"ambiguity_cont\": 3.59375}, {\"level\": \"A1\", \"ambiguity_avg\": 3.1512605042016806, \"ambiguity_cont\": 3.597826086956522}, {\"level\": \"A1\", \"ambiguity_avg\": 2.9541284403669725, \"ambiguity_cont\": 3.3295880149812733}, {\"level\": \"A1\", \"ambiguity_avg\": 3.0669642857142856, \"ambiguity_cont\": 3.5864197530864197}, {\"level\": \"A1\", \"ambiguity_avg\": 2.6029411764705883, \"ambiguity_cont\": 3.216494845360825}, {\"level\": \"A1\", \"ambiguity_avg\": 2.824263038548753, \"ambiguity_cont\": 3.3808049535603715}, {\"level\": \"A1\", \"ambiguity_avg\": 3.725, \"ambiguity_cont\": 4.204545454545454}, {\"level\": \"A1\", \"ambiguity_avg\": 3.278969957081545, \"ambiguity_cont\": 4.083832335329341}, {\"level\": \"A1\", \"ambiguity_avg\": 3.046747967479675, \"ambiguity_cont\": 3.4207792207792207}, {\"level\": \"A1\", \"ambiguity_avg\": 3.1867816091954024, \"ambiguity_cont\": 3.6486486486486487}, {\"level\": \"A1\", \"ambiguity_avg\": 2.8877551020408165, \"ambiguity_cont\": 3.769230769230769}, {\"level\": \"A1\", \"ambiguity_avg\": 2.984198645598194, \"ambiguity_cont\": 3.3626062322946177}, {\"level\": \"A1\", \"ambiguity_avg\": 3.6710526315789473, \"ambiguity_cont\": 4.35}, {\"level\": \"A1\", \"ambiguity_avg\": 3.9375, \"ambiguity_cont\": 4.805555555555555}, {\"level\": \"A1\", \"ambiguity_avg\": 2.5714285714285716, \"ambiguity_cont\": 2.7857142857142856}, {\"level\": \"A1\", \"ambiguity_avg\": 2.76, \"ambiguity_cont\": 3.3877551020408165}, {\"level\": \"A1\", \"ambiguity_avg\": 4.3076923076923075, \"ambiguity_cont\": 4.898305084745763}, {\"level\": \"A1\", \"ambiguity_avg\": 2.3225806451612905, \"ambiguity_cont\": 2.90625}, {\"level\": \"A1\", \"ambiguity_avg\": 1.962962962962963, \"ambiguity_cont\": 2.3157894736842106}, {\"level\": \"A1\", \"ambiguity_avg\": 2.782178217821782, \"ambiguity_cont\": 3.6865671641791047}, {\"level\": \"A1\", \"ambiguity_avg\": 3.109090909090909, \"ambiguity_cont\": 3.9240506329113924}, {\"level\": \"A1\", \"ambiguity_avg\": 3.2982456140350878, \"ambiguity_cont\": 4.421052631578948}, {\"level\": \"A1\", \"ambiguity_avg\": 2.8947368421052633, \"ambiguity_cont\": 3.515151515151515}, {\"level\": \"A1\", \"ambiguity_avg\": 2.9775280898876404, \"ambiguity_cont\": 3.8852459016393444}, {\"level\": \"A1\", \"ambiguity_avg\": 3.15609756097561, \"ambiguity_cont\": 4.092198581560283}, {\"level\": \"A1\", \"ambiguity_avg\": 2.1773049645390072, \"ambiguity_cont\": 2.5161290322580645}, {\"level\": \"A1\", \"ambiguity_avg\": 2.2783505154639174, \"ambiguity_cont\": 2.888888888888889}, {\"level\": \"A1\", \"ambiguity_avg\": 2.4177215189873418, \"ambiguity_cont\": 2.774193548387097}, {\"level\": \"A1\", \"ambiguity_avg\": 2.337078651685393, \"ambiguity_cont\": 2.757575757575758}, {\"level\": \"A1\", \"ambiguity_avg\": 3.86144578313253, \"ambiguity_cont\": 4.466165413533835}, {\"level\": \"A1\", \"ambiguity_avg\": 2.78125, \"ambiguity_cont\": 3.1363636363636362}, {\"level\": \"A1\", \"ambiguity_avg\": 2.4411764705882355, \"ambiguity_cont\": 2.92}, {\"level\": \"A1\", \"ambiguity_avg\": 2.4782608695652173, \"ambiguity_cont\": 3.392857142857143}, {\"level\": \"A1\", \"ambiguity_avg\": 2.8222222222222224, \"ambiguity_cont\": 3.125}, {\"level\": \"A1\", \"ambiguity_avg\": 1.8095238095238095, \"ambiguity_cont\": 2.32}, {\"level\": \"A1\", \"ambiguity_avg\": 3.6830601092896176, \"ambiguity_cont\": 5.315315315315315}, {\"level\": \"A1\", \"ambiguity_avg\": 2.9655172413793105, \"ambiguity_cont\": 3.6689419795221845}, {\"level\": \"A1\", \"ambiguity_avg\": 4.794871794871795, \"ambiguity_cont\": 5.032258064516129}, {\"level\": \"A1\", \"ambiguity_avg\": 2.841463414634146, \"ambiguity_cont\": 3.7115384615384617}, {\"level\": \"A1\", \"ambiguity_avg\": 3.671641791044776, \"ambiguity_cont\": 4.22}, {\"level\": \"A1\", \"ambiguity_avg\": 3.2662632375189107, \"ambiguity_cont\": 3.8636363636363638}, {\"level\": \"A1\", \"ambiguity_avg\": 3.33245382585752, \"ambiguity_cont\": 3.8562091503267975}, {\"level\": \"A1\", \"ambiguity_avg\": 3.183266932270916, \"ambiguity_cont\": 3.728}, {\"level\": \"A1\", \"ambiguity_avg\": 3.2717391304347827, \"ambiguity_cont\": 3.7890625}, {\"level\": \"A1\", \"ambiguity_avg\": 3.3784860557768925, \"ambiguity_cont\": 3.957912457912458}, {\"level\": \"A1\", \"ambiguity_avg\": 2.897746967071057, \"ambiguity_cont\": 3.4251781472684084}, {\"level\": \"A1\", \"ambiguity_avg\": 3.1964601769911503, \"ambiguity_cont\": 3.6328828828828827}, {\"level\": \"A1\", \"ambiguity_avg\": 3.515702479338843, \"ambiguity_cont\": 4.2091954022988505}, {\"level\": \"A1\", \"ambiguity_avg\": 3.408333333333333, \"ambiguity_cont\": 4.023861171366594}, {\"level\": \"A1\", \"ambiguity_avg\": 3.1946902654867255, \"ambiguity_cont\": 3.6628175519630486}, {\"level\": \"A1\", \"ambiguity_avg\": 3.4402777777777778, \"ambiguity_cont\": 4.066427289048474}, {\"level\": \"A1\", \"ambiguity_avg\": 3.4354136429608126, \"ambiguity_cont\": 4.057581573896353}, {\"level\": \"A1\", \"ambiguity_avg\": 3.8633288227334237, \"ambiguity_cont\": 4.558510638297872}, {\"level\": \"A1\", \"ambiguity_avg\": 3.3805309734513274, \"ambiguity_cont\": 3.8700564971751414}, {\"level\": \"A1\", \"ambiguity_avg\": 3.2058432934926957, \"ambiguity_cont\": 3.75}, {\"level\": \"A1\", \"ambiguity_avg\": 3.5453277545327753, \"ambiguity_cont\": 4.089416058394161}, {\"level\": \"A1\", \"ambiguity_avg\": 3.489082969432314, \"ambiguity_cont\": 4.033755274261603}, {\"level\": \"A1\", \"ambiguity_avg\": 3.574105621805792, \"ambiguity_cont\": 4.212860310421286}, {\"level\": \"A1\", \"ambiguity_avg\": 3.21285140562249, \"ambiguity_cont\": 3.6526315789473682}, {\"level\": \"A1\", \"ambiguity_avg\": 3.3480662983425415, \"ambiguity_cont\": 4.104477611940299}, {\"level\": \"A1\", \"ambiguity_avg\": 3.7510204081632654, \"ambiguity_cont\": 4.348484848484849}, {\"level\": \"A1\", \"ambiguity_avg\": 3.032876712328767, \"ambiguity_cont\": 3.4869565217391303}, {\"level\": \"A1\", \"ambiguity_avg\": 3.4869431643625193, \"ambiguity_cont\": 4.073929961089494}, {\"level\": \"A1\", \"ambiguity_avg\": 3.0848329048843186, \"ambiguity_cont\": 3.696245733788396}, {\"level\": \"A1\", \"ambiguity_avg\": 3.326530612244898, \"ambiguity_cont\": 3.9285714285714284}, {\"level\": \"A1\", \"ambiguity_avg\": 3.412037037037037, \"ambiguity_cont\": 3.7403314917127073}, {\"level\": \"A1\", \"ambiguity_avg\": 3.0, \"ambiguity_cont\": 3.0778688524590163}, {\"level\": \"A1\", \"ambiguity_avg\": 3.2295719844357977, \"ambiguity_cont\": 3.419512195121951}, {\"level\": \"A1\", \"ambiguity_avg\": 3.0528846153846154, \"ambiguity_cont\": 3.5555555555555554}, {\"level\": \"A1\", \"ambiguity_avg\": 3.752212389380531, \"ambiguity_cont\": 4.384615384615385}, {\"level\": \"A1\", \"ambiguity_avg\": 3.3222222222222224, \"ambiguity_cont\": 3.8970588235294117}, {\"level\": \"A1\", \"ambiguity_avg\": 3.223463687150838, \"ambiguity_cont\": 3.7611940298507465}, {\"level\": \"A1\", \"ambiguity_avg\": 3.33596837944664, \"ambiguity_cont\": 3.7945544554455446}, {\"level\": \"A1\", \"ambiguity_avg\": 3.5455904334828103, \"ambiguity_cont\": 4.129343629343629}, {\"level\": \"A1\", \"ambiguity_avg\": 3.320689655172414, \"ambiguity_cont\": 3.6709401709401708}, {\"level\": \"A1\", \"ambiguity_avg\": 3.1338912133891212, \"ambiguity_cont\": 3.6575342465753424}, {\"level\": \"A1\", \"ambiguity_avg\": 3.2595419847328246, \"ambiguity_cont\": 3.72069825436409}, {\"level\": \"A1\", \"ambiguity_avg\": 3.4269230769230767, \"ambiguity_cont\": 3.8660287081339715}, {\"level\": \"A1\", \"ambiguity_avg\": 3.0112079701120797, \"ambiguity_cont\": 3.5342237061769617}, {\"level\": \"A1\", \"ambiguity_avg\": 3.4273789649415694, \"ambiguity_cont\": 4.119638826185102}, {\"level\": \"A1\", \"ambiguity_avg\": 3.229768786127168, \"ambiguity_cont\": 3.7145557655954633}, {\"level\": \"A1\", \"ambiguity_avg\": 3.875, \"ambiguity_cont\": 4.341269841269841}, {\"level\": \"A1\", \"ambiguity_avg\": 3.935064935064935, \"ambiguity_cont\": 4.515625}, {\"level\": \"A1\", \"ambiguity_avg\": 3.8818897637795278, \"ambiguity_cont\": 4.724489795918367}, {\"level\": \"A1\", \"ambiguity_avg\": 3.676190476190476, \"ambiguity_cont\": 4.193181818181818}, {\"level\": \"A1\", \"ambiguity_avg\": 3.171875, \"ambiguity_cont\": 3.6226415094339623}, {\"level\": \"A2\", \"ambiguity_avg\": 3.5323974082073435, \"ambiguity_cont\": 4.126062322946176}, {\"level\": \"A2\", \"ambiguity_avg\": 3.1983317886932343, \"ambiguity_cont\": 3.841387856257745}, {\"level\": \"A2\", \"ambiguity_avg\": 3.0336206896551725, \"ambiguity_cont\": 3.6310904872389793}, {\"level\": \"A2\", \"ambiguity_avg\": 3.153973509933775, \"ambiguity_cont\": 3.7073446327683617}, {\"level\": \"A2\", \"ambiguity_avg\": 3.234126984126984, \"ambiguity_cont\": 3.7814113597246126}, {\"level\": \"A2\", \"ambiguity_avg\": 3.182674199623352, \"ambiguity_cont\": 3.6574225122349104}, {\"level\": \"A2\", \"ambiguity_avg\": 3.0412371134020617, \"ambiguity_cont\": 3.610404624277457}, {\"level\": \"A2\", \"ambiguity_avg\": 3.8074074074074074, \"ambiguity_cont\": 4.562814070351759}, {\"level\": \"A2\", \"ambiguity_avg\": 3.394431554524362, \"ambiguity_cont\": 3.942227516378797}, {\"level\": \"A2\", \"ambiguity_avg\": 3.3076923076923075, \"ambiguity_cont\": 3.525}, {\"level\": \"A2\", \"ambiguity_avg\": 4.076923076923077, \"ambiguity_cont\": 4.96}, {\"level\": \"A2\", \"ambiguity_avg\": 2.97196261682243, \"ambiguity_cont\": 3.6184210526315788}, {\"level\": \"A2\", \"ambiguity_avg\": 3.7880794701986753, \"ambiguity_cont\": 4.581196581196581}, {\"level\": \"A2\", \"ambiguity_avg\": 3.9135802469135803, \"ambiguity_cont\": 5.105263157894737}, {\"level\": \"A2\", \"ambiguity_avg\": 3.609271523178808, \"ambiguity_cont\": 4.455357142857143}, {\"level\": \"A2\", \"ambiguity_avg\": 3.054054054054054, \"ambiguity_cont\": 3.4918032786885247}, {\"level\": \"A2\", \"ambiguity_avg\": 2.1904761904761907, \"ambiguity_cont\": 2.536842105263158}, {\"level\": \"A2\", \"ambiguity_avg\": 5.98989898989899, \"ambiguity_cont\": 7.35064935064935}, {\"level\": \"A2\", \"ambiguity_avg\": 2.8900343642611683, \"ambiguity_cont\": 4.0}, {\"level\": \"A2\", \"ambiguity_avg\": 3.6507936507936507, \"ambiguity_cont\": 4.134615384615385}, {\"level\": \"A2\", \"ambiguity_avg\": 3.066326530612245, \"ambiguity_cont\": 3.6490066225165565}, {\"level\": \"A2\", \"ambiguity_avg\": 2.671232876712329, \"ambiguity_cont\": 2.982142857142857}, {\"level\": \"A2\", \"ambiguity_avg\": 2.793103448275862, \"ambiguity_cont\": 3.1714285714285713}, {\"level\": \"A2\", \"ambiguity_avg\": 3.763157894736842, \"ambiguity_cont\": 4.818181818181818}, {\"level\": \"A2\", \"ambiguity_avg\": 3.5542168674698793, \"ambiguity_cont\": 4.0285714285714285}, {\"level\": \"A2\", \"ambiguity_avg\": 2.4918032786885247, \"ambiguity_cont\": 2.977777777777778}, {\"level\": \"A2\", \"ambiguity_avg\": 1.9791666666666667, \"ambiguity_cont\": 2.0930232558139537}, {\"level\": \"A2\", \"ambiguity_avg\": 3.326530612244898, \"ambiguity_cont\": 3.707317073170732}, {\"level\": \"A2\", \"ambiguity_avg\": 2.9863013698630136, \"ambiguity_cont\": 3.482142857142857}, {\"level\": \"A2\", \"ambiguity_avg\": 2.8636363636363638, \"ambiguity_cont\": 3.0}, {\"level\": \"A2\", \"ambiguity_avg\": 3.4761904761904763, \"ambiguity_cont\": 3.6140350877192984}, {\"level\": \"A2\", \"ambiguity_avg\": 2.3461538461538463, \"ambiguity_cont\": 2.5217391304347827}, {\"level\": \"A2\", \"ambiguity_avg\": 3.1097560975609757, \"ambiguity_cont\": 3.7244094488188977}, {\"level\": \"A2\", \"ambiguity_avg\": 3.4942263279445727, \"ambiguity_cont\": 4.114803625377643}, {\"level\": \"A2\", \"ambiguity_avg\": 2.8115942028985508, \"ambiguity_cont\": 3.2962962962962963}, {\"level\": \"A2\", \"ambiguity_avg\": 3.029304029304029, \"ambiguity_cont\": 3.626794258373206}, {\"level\": \"A2\", \"ambiguity_avg\": 3.417525773195876, \"ambiguity_cont\": 4.129824561403509}, {\"level\": \"A2\", \"ambiguity_avg\": 3.5283018867924527, \"ambiguity_cont\": 3.874015748031496}, {\"level\": \"A2\", \"ambiguity_avg\": 3.473684210526316, \"ambiguity_cont\": 3.857843137254902}, {\"level\": \"A2\", \"ambiguity_avg\": 3.6627906976744184, \"ambiguity_cont\": 4.154929577464789}, {\"level\": \"A2\", \"ambiguity_avg\": 2.7333333333333334, \"ambiguity_cont\": 3.206611570247934}, {\"level\": \"A2\", \"ambiguity_avg\": 2.8376068376068377, \"ambiguity_cont\": 3.6455696202531644}, {\"level\": \"A2\", \"ambiguity_avg\": 2.433333333333333, \"ambiguity_cont\": 2.8297872340425534}, {\"level\": \"A2\", \"ambiguity_avg\": 2.5714285714285716, \"ambiguity_cont\": 3.079136690647482}, {\"level\": \"A2\", \"ambiguity_avg\": 3.0278164116828927, \"ambiguity_cont\": 3.4861878453038675}, {\"level\": \"A2\", \"ambiguity_avg\": 3.317135549872123, \"ambiguity_cont\": 3.895424836601307}, {\"level\": \"A2\", \"ambiguity_avg\": 3.427480916030534, \"ambiguity_cont\": 4.0495049504950495}, {\"level\": \"A2\", \"ambiguity_avg\": 2.7662337662337664, \"ambiguity_cont\": 3.2758620689655173}, {\"level\": \"A2\", \"ambiguity_avg\": 3.1589285714285715, \"ambiguity_cont\": 3.815347721822542}, {\"level\": \"A2\", \"ambiguity_avg\": 3.238853503184713, \"ambiguity_cont\": 3.631048387096774}, {\"level\": \"A2\", \"ambiguity_avg\": 3.412298387096774, \"ambiguity_cont\": 3.8343949044585988}, {\"level\": \"A2\", \"ambiguity_avg\": 2.5724907063197024, \"ambiguity_cont\": 2.925373134328358}, {\"level\": \"A2\", \"ambiguity_avg\": 3.0686695278969958, \"ambiguity_cont\": 3.458100558659218}, {\"level\": \"A2\", \"ambiguity_avg\": 3.263157894736842, \"ambiguity_cont\": 3.986159169550173}, {\"level\": \"A2\", \"ambiguity_avg\": 3.2248322147651005, \"ambiguity_cont\": 3.903669724770642}, {\"level\": \"A2\", \"ambiguity_avg\": 3.4741035856573705, \"ambiguity_cont\": 4.115183246073299}, {\"level\": \"A2\", \"ambiguity_avg\": 2.9404761904761907, \"ambiguity_cont\": 3.282442748091603}, {\"level\": \"A2\", \"ambiguity_avg\": 2.739130434782609, \"ambiguity_cont\": 3.0242424242424244}, {\"level\": \"A2\", \"ambiguity_avg\": 3.5155709342560555, \"ambiguity_cont\": 3.986666666666667}, {\"level\": \"A2\", \"ambiguity_avg\": 3.1908212560386473, \"ambiguity_cont\": 3.68693009118541}, {\"level\": \"A2\", \"ambiguity_avg\": 3.316742081447964, \"ambiguity_cont\": 3.878260869565217}, {\"level\": \"A2\", \"ambiguity_avg\": 3.3846153846153846, \"ambiguity_cont\": 3.916923076923077}, {\"level\": \"B\", \"ambiguity_avg\": 3.662387676508344, \"ambiguity_cont\": 4.460884353741497}, {\"level\": \"B\", \"ambiguity_avg\": 2.93232044198895, \"ambiguity_cont\": 3.6025390625}, {\"level\": \"B\", \"ambiguity_avg\": 3.03486529318542, \"ambiguity_cont\": 3.8339140534262484}, {\"level\": \"B\", \"ambiguity_avg\": 3.6853932584269664, \"ambiguity_cont\": 4.414285714285715}, {\"level\": \"B\", \"ambiguity_avg\": 3.4217221135029354, \"ambiguity_cont\": 4.1302048909451425}, {\"level\": \"B\", \"ambiguity_avg\": 2.914942528735632, \"ambiguity_cont\": 3.5080128205128207}, {\"level\": \"B\", \"ambiguity_avg\": 3.106382978723404, \"ambiguity_cont\": 4.233333333333333}, {\"level\": \"B\", \"ambiguity_avg\": 3.177293934681182, \"ambiguity_cont\": 3.7651209677419355}, {\"level\": \"B\", \"ambiguity_avg\": 2.9683544303797467, \"ambiguity_cont\": 3.353448275862069}, {\"level\": \"B\", \"ambiguity_avg\": 3.6666666666666665, \"ambiguity_cont\": 3.9722222222222223}, {\"level\": \"B\", \"ambiguity_avg\": 3.238578680203046, \"ambiguity_cont\": 3.9192399049881237}, {\"level\": \"B\", \"ambiguity_avg\": 2.977535301668806, \"ambiguity_cont\": 3.3632585203657523}, {\"level\": \"B\", \"ambiguity_avg\": 3.1041666666666665, \"ambiguity_cont\": 3.6463414634146343}, {\"level\": \"B\", \"ambiguity_avg\": 3.6203544882790166, \"ambiguity_cont\": 4.196549137284321}, {\"level\": \"B\", \"ambiguity_avg\": 2.8720445062586926, \"ambiguity_cont\": 3.464576802507837}, {\"level\": \"B\", \"ambiguity_avg\": 3.077319587628866, \"ambiguity_cont\": 3.7222222222222223}, {\"level\": \"B\", \"ambiguity_avg\": 3.7586206896551726, \"ambiguity_cont\": 4.068376068376068}, {\"level\": \"B\", \"ambiguity_avg\": 3.578727841501564, \"ambiguity_cont\": 4.108638743455497}, {\"level\": \"B\", \"ambiguity_avg\": 3.3717357910906296, \"ambiguity_cont\": 3.8099009900990097}, {\"level\": \"B\", \"ambiguity_avg\": 3.3714285714285714, \"ambiguity_cont\": 3.53125}, {\"level\": \"B\", \"ambiguity_avg\": 3.2222222222222223, \"ambiguity_cont\": 3.7563025210084033}, {\"level\": \"B\", \"ambiguity_avg\": 3.7222222222222223, \"ambiguity_cont\": 4.362068965517241}, {\"level\": \"B\", \"ambiguity_avg\": 3.1481481481481484, \"ambiguity_cont\": 3.697674418604651}, {\"level\": \"B\", \"ambiguity_avg\": 2.9767441860465116, \"ambiguity_cont\": 3.342857142857143}, {\"level\": \"B\", \"ambiguity_avg\": 4.5131578947368425, \"ambiguity_cont\": 5.454545454545454}, {\"level\": \"B\", \"ambiguity_avg\": 3.057142857142857, \"ambiguity_cont\": 3.618181818181818}, {\"level\": \"B\", \"ambiguity_avg\": 1.9230769230769231, \"ambiguity_cont\": 2.7}, {\"level\": \"B\", \"ambiguity_avg\": 2.6666666666666665, \"ambiguity_cont\": 2.6666666666666665}, {\"level\": \"B\", \"ambiguity_avg\": 3.8383838383838382, \"ambiguity_cont\": 4.50625}, {\"level\": \"B\", \"ambiguity_avg\": 5.090909090909091, \"ambiguity_cont\": 5.615384615384615}, {\"level\": \"B\", \"ambiguity_avg\": 3.0555555555555554, \"ambiguity_cont\": 3.3125}, {\"level\": \"B\", \"ambiguity_avg\": 4.129032258064516, \"ambiguity_cont\": 4.730769230769231}, {\"level\": \"B\", \"ambiguity_avg\": 4.191780821917808, \"ambiguity_cont\": 4.932203389830509}, {\"level\": \"B\", \"ambiguity_avg\": 2.6153846153846154, \"ambiguity_cont\": 3.1904761904761907}, {\"level\": \"B\", \"ambiguity_avg\": 3.4150943396226414, \"ambiguity_cont\": 3.6808510638297873}, {\"level\": \"B\", \"ambiguity_avg\": 3.0, \"ambiguity_cont\": 3.4285714285714284}, {\"level\": \"B\", \"ambiguity_avg\": 3.311688311688312, \"ambiguity_cont\": 3.5873015873015874}, {\"level\": \"B\", \"ambiguity_avg\": 3.102493074792244, \"ambiguity_cont\": 3.8452380952380953}, {\"level\": \"B\", \"ambiguity_avg\": 2.99039780521262, \"ambiguity_cont\": 3.5413533834586466}, {\"level\": \"B\", \"ambiguity_avg\": 2.9965156794425085, \"ambiguity_cont\": 3.5821596244131455}, {\"level\": \"B\", \"ambiguity_avg\": 2.761904761904762, \"ambiguity_cont\": 3.5704697986577183}, {\"level\": \"B\", \"ambiguity_avg\": 3.217391304347826, \"ambiguity_cont\": 3.6742209631728047}, {\"level\": \"B\", \"ambiguity_avg\": 3.153284671532847, \"ambiguity_cont\": 3.6132075471698113}, {\"level\": \"B\", \"ambiguity_avg\": 3.1973969631236443, \"ambiguity_cont\": 3.6629834254143647}, {\"level\": \"B\", \"ambiguity_avg\": 2.58, \"ambiguity_cont\": 3.0}, {\"level\": \"B\", \"ambiguity_avg\": 3.43343653250774, \"ambiguity_cont\": 4.02020202020202}, {\"level\": \"B\", \"ambiguity_avg\": 3.102803738317757, \"ambiguity_cont\": 3.635294117647059}, {\"level\": \"B\", \"ambiguity_avg\": 2.9743589743589745, \"ambiguity_cont\": 3.6363636363636362}, {\"level\": \"B\", \"ambiguity_avg\": 2.8776978417266186, \"ambiguity_cont\": 3.4326923076923075}, {\"level\": \"B\", \"ambiguity_avg\": 2.42, \"ambiguity_cont\": 2.871559633027523}, {\"level\": \"B\", \"ambiguity_avg\": 3.4751552795031055, \"ambiguity_cont\": 4.048780487804878}, {\"level\": \"B\", \"ambiguity_avg\": 4.0, \"ambiguity_cont\": 5.205128205128205}, {\"level\": \"B\", \"ambiguity_avg\": 3.3017408123791103, \"ambiguity_cont\": 3.9558823529411766}, {\"level\": \"B\", \"ambiguity_avg\": 3.041407867494824, \"ambiguity_cont\": 3.5026315789473683}, {\"level\": \"B\", \"ambiguity_avg\": 3.1016949152542375, \"ambiguity_cont\": 3.630434782608696}, {\"level\": \"B\", \"ambiguity_avg\": 4.083333333333333, \"ambiguity_cont\": 4.363636363636363}, {\"level\": \"B\", \"ambiguity_avg\": 2.9135802469135803, \"ambiguity_cont\": 3.6785714285714284}, {\"level\": \"B\", \"ambiguity_avg\": 3.45021645021645, \"ambiguity_cont\": 3.7945945945945945}, {\"level\": \"B\", \"ambiguity_avg\": 2.5555555555555554, \"ambiguity_cont\": 3.076923076923077}, {\"level\": \"B\", \"ambiguity_avg\": 3.2327586206896552, \"ambiguity_cont\": 3.865168539325843}, {\"level\": \"B\", \"ambiguity_avg\": 3.5677083333333335, \"ambiguity_cont\": 4.401459854014599}, {\"level\": \"B\", \"ambiguity_avg\": 3.746556473829201, \"ambiguity_cont\": 4.408450704225352}, {\"level\": \"B\", \"ambiguity_avg\": 4.0, \"ambiguity_cont\": 4.4576271186440675}, {\"level\": \"B\", \"ambiguity_avg\": 3.8275862068965516, \"ambiguity_cont\": 4.242424242424242}, {\"level\": \"B\", \"ambiguity_avg\": 5.032258064516129, \"ambiguity_cont\": 5.62962962962963}, {\"level\": \"B\", \"ambiguity_avg\": 2.4242424242424243, \"ambiguity_cont\": 3.0434782608695654}, {\"level\": \"B\", \"ambiguity_avg\": 3.3149038461538463, \"ambiguity_cont\": 3.897106109324759}, {\"level\": \"B\", \"ambiguity_avg\": 3.286153846153846, \"ambiguity_cont\": 3.9036144578313254}, {\"level\": \"B\", \"ambiguity_avg\": 3.0930232558139537, \"ambiguity_cont\": 3.870967741935484}, {\"level\": \"B\", \"ambiguity_avg\": 3.46875, \"ambiguity_cont\": 4.714285714285714}, {\"level\": \"B\", \"ambiguity_avg\": 3.4444444444444446, \"ambiguity_cont\": 4.666666666666667}, {\"level\": \"B\", \"ambiguity_avg\": 3.1228070175438596, \"ambiguity_cont\": 3.7045454545454546}, {\"level\": \"B\", \"ambiguity_avg\": 3.3014440433212995, \"ambiguity_cont\": 3.9877450980392157}, {\"level\": \"B\", \"ambiguity_avg\": 2.7794117647058822, \"ambiguity_cont\": 3.6222222222222222}, {\"level\": \"B\", \"ambiguity_avg\": 3.1376146788990824, \"ambiguity_cont\": 3.810126582278481}, {\"level\": \"B\", \"ambiguity_avg\": 2.7710843373493974, \"ambiguity_cont\": 3.396551724137931}, {\"level\": \"B\", \"ambiguity_avg\": 3.075268817204301, \"ambiguity_cont\": 3.4313725490196076}, {\"level\": \"B\", \"ambiguity_avg\": 3.2346938775510203, \"ambiguity_cont\": 3.8275862068965516}, {\"level\": \"B\", \"ambiguity_avg\": 3.754491017964072, \"ambiguity_cont\": 4.7478991596638656}, {\"level\": \"B\", \"ambiguity_avg\": 3.736842105263158, \"ambiguity_cont\": 5.031746031746032}, {\"level\": \"B\", \"ambiguity_avg\": 2.9344262295081966, \"ambiguity_cont\": 3.297872340425532}, {\"level\": \"B\", \"ambiguity_avg\": 3.064935064935065, \"ambiguity_cont\": 3.3728813559322033}, {\"level\": \"B\", \"ambiguity_avg\": 3.238993710691824, \"ambiguity_cont\": 3.8583333333333334}, {\"level\": \"B\", \"ambiguity_avg\": 3.7057291666666665, \"ambiguity_cont\": 4.404761904761905}, {\"level\": \"B\", \"ambiguity_avg\": 3.596774193548387, \"ambiguity_cont\": 4.21978021978022}, {\"level\": \"B\", \"ambiguity_avg\": 3.12, \"ambiguity_cont\": 3.4358974358974357}, {\"level\": \"B\", \"ambiguity_avg\": 3.893617021276596, \"ambiguity_cont\": 4.333333333333333}, {\"level\": \"B\", \"ambiguity_avg\": 3.3839285714285716, \"ambiguity_cont\": 4.0552147239263805}, {\"level\": \"B\", \"ambiguity_avg\": 3.9026548672566372, \"ambiguity_cont\": 4.963414634146342}, {\"level\": \"B\", \"ambiguity_avg\": 4.095238095238095, \"ambiguity_cont\": 5.223684210526316}, {\"level\": \"B\", \"ambiguity_avg\": 3.419642857142857, \"ambiguity_cont\": 3.987577639751553}, {\"level\": \"B\", \"ambiguity_avg\": 3.2830188679245285, \"ambiguity_cont\": 3.975}, {\"level\": \"B\", \"ambiguity_avg\": 3.387434554973822, \"ambiguity_cont\": 4.020833333333333}, {\"level\": \"B\", \"ambiguity_avg\": 3.141304347826087, \"ambiguity_cont\": 3.8296296296296295}, {\"level\": \"B\", \"ambiguity_avg\": 2.275, \"ambiguity_cont\": 2.84}, {\"level\": \"B\", \"ambiguity_avg\": 2.90929203539823, \"ambiguity_cont\": 3.588957055214724}, {\"level\": \"B\", \"ambiguity_avg\": 3.392739273927393, \"ambiguity_cont\": 3.911392405063291}, {\"level\": \"B\", \"ambiguity_avg\": 3.795918367346939, \"ambiguity_cont\": 4.341463414634147}, {\"level\": \"B\", \"ambiguity_avg\": 4.428571428571429, \"ambiguity_cont\": 7.0}, {\"level\": \"B\", \"ambiguity_avg\": 4.625, \"ambiguity_cont\": 5.0}, {\"level\": \"B\", \"ambiguity_avg\": 5.0, \"ambiguity_cont\": 5.8}, {\"level\": \"B\", \"ambiguity_avg\": 4.6, \"ambiguity_cont\": 5.7368421052631575}, {\"level\": \"B\", \"ambiguity_avg\": 2.230769230769231, \"ambiguity_cont\": 2.875}, {\"level\": \"B\", \"ambiguity_avg\": 3.8, \"ambiguity_cont\": 4.5}, {\"level\": \"B\", \"ambiguity_avg\": 1.5714285714285714, \"ambiguity_cont\": 1.8}, {\"level\": \"B\", \"ambiguity_avg\": 2.375, \"ambiguity_cont\": 3.0625}, {\"level\": \"B\", \"ambiguity_avg\": 3.0, \"ambiguity_cont\": 3.372093023255814}, {\"level\": \"B\", \"ambiguity_avg\": 3.259259259259259, \"ambiguity_cont\": 3.95}, {\"level\": \"B\", \"ambiguity_avg\": 2.3, \"ambiguity_cont\": 2.5}, {\"level\": \"B\", \"ambiguity_avg\": 1.8333333333333333, \"ambiguity_cont\": 2.6666666666666665}, {\"level\": \"B1\", \"ambiguity_avg\": 3.045045045045045, \"ambiguity_cont\": 3.5681818181818183}, {\"level\": \"B1\", \"ambiguity_avg\": 3.3392857142857144, \"ambiguity_cont\": 3.897727272727273}, {\"level\": \"B1\", \"ambiguity_avg\": 2.4571428571428573, \"ambiguity_cont\": 3.0}, {\"level\": \"B1\", \"ambiguity_avg\": 3.3055555555555554, \"ambiguity_cont\": 3.86046511627907}, {\"level\": \"B1\", \"ambiguity_avg\": 3.0982142857142856, \"ambiguity_cont\": 3.606741573033708}, {\"level\": \"B1\", \"ambiguity_avg\": 3.226086956521739, \"ambiguity_cont\": 3.9310344827586206}, {\"level\": \"B1\", \"ambiguity_avg\": 2.5517241379310347, \"ambiguity_cont\": 3.25}, {\"level\": \"B1\", \"ambiguity_avg\": 3.9150943396226414, \"ambiguity_cont\": 4.5813953488372094}, {\"level\": \"B1\", \"ambiguity_avg\": 3.633663366336634, \"ambiguity_cont\": 4.142857142857143}, {\"level\": \"B1\", \"ambiguity_avg\": 2.4607843137254903, \"ambiguity_cont\": 2.7948717948717947}, {\"level\": \"B1\", \"ambiguity_avg\": 2.34375, \"ambiguity_cont\": 2.72}, {\"level\": \"B1\", \"ambiguity_avg\": 3.7183308494783907, \"ambiguity_cont\": 4.488}, {\"level\": \"B1\", \"ambiguity_avg\": 2.941028858218319, \"ambiguity_cont\": 3.476510067114094}, {\"level\": \"B1\", \"ambiguity_avg\": 3.757575757575758, \"ambiguity_cont\": 4.560267857142857}, {\"level\": \"B1\", \"ambiguity_avg\": 3.3466257668711656, \"ambiguity_cont\": 4.063025210084033}, {\"level\": \"B1\", \"ambiguity_avg\": 3.322074788902292, \"ambiguity_cont\": 3.8276972624798713}, {\"level\": \"B1\", \"ambiguity_avg\": 3.69164265129683, \"ambiguity_cont\": 4.285714285714286}, {\"level\": \"B1\", \"ambiguity_avg\": 3.2006745362563236, \"ambiguity_cont\": 3.7142857142857144}, {\"level\": \"B1\", \"ambiguity_avg\": 3.281124497991968, \"ambiguity_cont\": 3.776595744680851}, {\"level\": \"B1\", \"ambiguity_avg\": 2.9153094462540716, \"ambiguity_cont\": 3.4930232558139536}, {\"level\": \"B1\", \"ambiguity_avg\": 3.2146050670640833, \"ambiguity_cont\": 3.735812133072407}, {\"level\": \"B1\", \"ambiguity_avg\": 3.135388739946381, \"ambiguity_cont\": 3.664310954063604}, {\"level\": \"B1\", \"ambiguity_avg\": 2.992343032159265, \"ambiguity_cont\": 3.6212765957446806}, {\"level\": \"B1\", \"ambiguity_avg\": 3.46890286512928, \"ambiguity_cont\": 4.009812667261373}, {\"level\": \"B1\", \"ambiguity_avg\": 3.213507625272331, \"ambiguity_cont\": 3.814814814814815}, {\"level\": \"B1\", \"ambiguity_avg\": 3.3594351732991012, \"ambiguity_cont\": 3.986111111111111}, {\"level\": \"B1\", \"ambiguity_avg\": 3.165434906196703, \"ambiguity_cont\": 3.769109535066982}, {\"level\": \"B1\", \"ambiguity_avg\": 3.174228675136116, \"ambiguity_cont\": 3.823873121869783}, {\"level\": \"B1\", \"ambiguity_avg\": 3.0245051837888783, \"ambiguity_cont\": 3.5854922279792745}, {\"level\": \"B1\", \"ambiguity_avg\": 2.3333333333333335, \"ambiguity_cont\": 2.5428571428571427}, {\"level\": \"B1\", \"ambiguity_avg\": 3.864864864864865, \"ambiguity_cont\": 4.777777777777778}, {\"level\": \"B1\", \"ambiguity_avg\": 3.0, \"ambiguity_cont\": 3.6153846153846154}, {\"level\": \"B1\", \"ambiguity_avg\": 3.3636363636363638, \"ambiguity_cont\": 3.88}, {\"level\": \"B1\", \"ambiguity_avg\": 3.0, \"ambiguity_cont\": 3.5}, {\"level\": \"B1\", \"ambiguity_avg\": 3.8, \"ambiguity_cont\": 4.1940298507462686}, {\"level\": \"B1\", \"ambiguity_avg\": 3.3854166666666665, \"ambiguity_cont\": 3.993150684931507}, {\"level\": \"B1\", \"ambiguity_avg\": 3.169491525423729, \"ambiguity_cont\": 3.717391304347826}, {\"level\": \"B1\", \"ambiguity_avg\": 3.5789473684210527, \"ambiguity_cont\": 3.7222222222222223}, {\"level\": \"B1\", \"ambiguity_avg\": 3.5476190476190474, \"ambiguity_cont\": 4.095238095238095}, {\"level\": \"B1\", \"ambiguity_avg\": 2.735294117647059, \"ambiguity_cont\": 3.4583333333333335}, {\"level\": \"B1\", \"ambiguity_avg\": 2.4615384615384617, \"ambiguity_cont\": 2.9655172413793105}, {\"level\": \"B1\", \"ambiguity_avg\": 3.2758620689655173, \"ambiguity_cont\": 4.0}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "level_amb_df = pd.DataFrame(level_degree_ambig)\n",
    "level_amb_df.head()\n",
    "alt.Chart(level_amb_df).mark_bar().encode(x = alt.X('ambiguity_avg', bin = alt.Bin(maxbins = 30)), y = 'count()').facet('level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "amended-prophet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-5304cdbbb4b2425e9576801b32848ac6\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-5304cdbbb4b2425e9576801b32848ac6\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-5304cdbbb4b2425e9576801b32848ac6\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-027e704a06acf56bf8649009343713fd\"}, \"facet\": {\"type\": \"nominal\", \"field\": \"level\"}, \"spec\": {\"mark\": \"bar\", \"encoding\": {\"x\": {\"type\": \"quantitative\", \"bin\": {\"maxbins\": 30}, \"field\": \"ambiguity_cont\"}, \"y\": {\"type\": \"quantitative\", \"aggregate\": \"count\"}}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\", \"datasets\": {\"data-027e704a06acf56bf8649009343713fd\": [{\"level\": \"A1\", \"ambiguity_avg\": 3.0714285714285716, \"ambiguity_cont\": 3.5762711864406778}, {\"level\": \"A1\", \"ambiguity_avg\": 3.2403846153846154, \"ambiguity_cont\": 3.987012987012987}, {\"level\": \"A1\", \"ambiguity_avg\": 3.7948717948717947, \"ambiguity_cont\": 4.444444444444445}, {\"level\": \"A1\", \"ambiguity_avg\": 3.1666666666666665, \"ambiguity_cont\": 3.3870967741935485}, {\"level\": \"A1\", \"ambiguity_avg\": 4.388888888888889, \"ambiguity_cont\": 4.961038961038961}, {\"level\": \"A1\", \"ambiguity_avg\": 3.3451776649746194, \"ambiguity_cont\": 4.006535947712418}, {\"level\": \"A1\", \"ambiguity_avg\": 3.832579185520362, \"ambiguity_cont\": 4.44}, {\"level\": \"A1\", \"ambiguity_avg\": 3.415492957746479, \"ambiguity_cont\": 4.0186335403726705}, {\"level\": \"A1\", \"ambiguity_avg\": 3.0373626373626372, \"ambiguity_cont\": 3.59375}, {\"level\": \"A1\", \"ambiguity_avg\": 3.1512605042016806, \"ambiguity_cont\": 3.597826086956522}, {\"level\": \"A1\", \"ambiguity_avg\": 2.9541284403669725, \"ambiguity_cont\": 3.3295880149812733}, {\"level\": \"A1\", \"ambiguity_avg\": 3.0669642857142856, \"ambiguity_cont\": 3.5864197530864197}, {\"level\": \"A1\", \"ambiguity_avg\": 2.6029411764705883, \"ambiguity_cont\": 3.216494845360825}, {\"level\": \"A1\", \"ambiguity_avg\": 2.824263038548753, \"ambiguity_cont\": 3.3808049535603715}, {\"level\": \"A1\", \"ambiguity_avg\": 3.725, \"ambiguity_cont\": 4.204545454545454}, {\"level\": \"A1\", \"ambiguity_avg\": 3.278969957081545, \"ambiguity_cont\": 4.083832335329341}, {\"level\": \"A1\", \"ambiguity_avg\": 3.046747967479675, \"ambiguity_cont\": 3.4207792207792207}, {\"level\": \"A1\", \"ambiguity_avg\": 3.1867816091954024, \"ambiguity_cont\": 3.6486486486486487}, {\"level\": \"A1\", \"ambiguity_avg\": 2.8877551020408165, \"ambiguity_cont\": 3.769230769230769}, {\"level\": \"A1\", \"ambiguity_avg\": 2.984198645598194, \"ambiguity_cont\": 3.3626062322946177}, {\"level\": \"A1\", \"ambiguity_avg\": 3.6710526315789473, \"ambiguity_cont\": 4.35}, {\"level\": \"A1\", \"ambiguity_avg\": 3.9375, \"ambiguity_cont\": 4.805555555555555}, {\"level\": \"A1\", \"ambiguity_avg\": 2.5714285714285716, \"ambiguity_cont\": 2.7857142857142856}, {\"level\": \"A1\", \"ambiguity_avg\": 2.76, \"ambiguity_cont\": 3.3877551020408165}, {\"level\": \"A1\", \"ambiguity_avg\": 4.3076923076923075, \"ambiguity_cont\": 4.898305084745763}, {\"level\": \"A1\", \"ambiguity_avg\": 2.3225806451612905, \"ambiguity_cont\": 2.90625}, {\"level\": \"A1\", \"ambiguity_avg\": 1.962962962962963, \"ambiguity_cont\": 2.3157894736842106}, {\"level\": \"A1\", \"ambiguity_avg\": 2.782178217821782, \"ambiguity_cont\": 3.6865671641791047}, {\"level\": \"A1\", \"ambiguity_avg\": 3.109090909090909, \"ambiguity_cont\": 3.9240506329113924}, {\"level\": \"A1\", \"ambiguity_avg\": 3.2982456140350878, \"ambiguity_cont\": 4.421052631578948}, {\"level\": \"A1\", \"ambiguity_avg\": 2.8947368421052633, \"ambiguity_cont\": 3.515151515151515}, {\"level\": \"A1\", \"ambiguity_avg\": 2.9775280898876404, \"ambiguity_cont\": 3.8852459016393444}, {\"level\": \"A1\", \"ambiguity_avg\": 3.15609756097561, \"ambiguity_cont\": 4.092198581560283}, {\"level\": \"A1\", \"ambiguity_avg\": 2.1773049645390072, \"ambiguity_cont\": 2.5161290322580645}, {\"level\": \"A1\", \"ambiguity_avg\": 2.2783505154639174, \"ambiguity_cont\": 2.888888888888889}, {\"level\": \"A1\", \"ambiguity_avg\": 2.4177215189873418, \"ambiguity_cont\": 2.774193548387097}, {\"level\": \"A1\", \"ambiguity_avg\": 2.337078651685393, \"ambiguity_cont\": 2.757575757575758}, {\"level\": \"A1\", \"ambiguity_avg\": 3.86144578313253, \"ambiguity_cont\": 4.466165413533835}, {\"level\": \"A1\", \"ambiguity_avg\": 2.78125, \"ambiguity_cont\": 3.1363636363636362}, {\"level\": \"A1\", \"ambiguity_avg\": 2.4411764705882355, \"ambiguity_cont\": 2.92}, {\"level\": \"A1\", \"ambiguity_avg\": 2.4782608695652173, \"ambiguity_cont\": 3.392857142857143}, {\"level\": \"A1\", \"ambiguity_avg\": 2.8222222222222224, \"ambiguity_cont\": 3.125}, {\"level\": \"A1\", \"ambiguity_avg\": 1.8095238095238095, \"ambiguity_cont\": 2.32}, {\"level\": \"A1\", \"ambiguity_avg\": 3.6830601092896176, \"ambiguity_cont\": 5.315315315315315}, {\"level\": \"A1\", \"ambiguity_avg\": 2.9655172413793105, \"ambiguity_cont\": 3.6689419795221845}, {\"level\": \"A1\", \"ambiguity_avg\": 4.794871794871795, \"ambiguity_cont\": 5.032258064516129}, {\"level\": \"A1\", \"ambiguity_avg\": 2.841463414634146, \"ambiguity_cont\": 3.7115384615384617}, {\"level\": \"A1\", \"ambiguity_avg\": 3.671641791044776, \"ambiguity_cont\": 4.22}, {\"level\": \"A1\", \"ambiguity_avg\": 3.2662632375189107, \"ambiguity_cont\": 3.8636363636363638}, {\"level\": \"A1\", \"ambiguity_avg\": 3.33245382585752, \"ambiguity_cont\": 3.8562091503267975}, {\"level\": \"A1\", \"ambiguity_avg\": 3.183266932270916, \"ambiguity_cont\": 3.728}, {\"level\": \"A1\", \"ambiguity_avg\": 3.2717391304347827, \"ambiguity_cont\": 3.7890625}, {\"level\": \"A1\", \"ambiguity_avg\": 3.3784860557768925, \"ambiguity_cont\": 3.957912457912458}, {\"level\": \"A1\", \"ambiguity_avg\": 2.897746967071057, \"ambiguity_cont\": 3.4251781472684084}, {\"level\": \"A1\", \"ambiguity_avg\": 3.1964601769911503, \"ambiguity_cont\": 3.6328828828828827}, {\"level\": \"A1\", \"ambiguity_avg\": 3.515702479338843, \"ambiguity_cont\": 4.2091954022988505}, {\"level\": \"A1\", \"ambiguity_avg\": 3.408333333333333, \"ambiguity_cont\": 4.023861171366594}, {\"level\": \"A1\", \"ambiguity_avg\": 3.1946902654867255, \"ambiguity_cont\": 3.6628175519630486}, {\"level\": \"A1\", \"ambiguity_avg\": 3.4402777777777778, \"ambiguity_cont\": 4.066427289048474}, {\"level\": \"A1\", \"ambiguity_avg\": 3.4354136429608126, \"ambiguity_cont\": 4.057581573896353}, {\"level\": \"A1\", \"ambiguity_avg\": 3.8633288227334237, \"ambiguity_cont\": 4.558510638297872}, {\"level\": \"A1\", \"ambiguity_avg\": 3.3805309734513274, \"ambiguity_cont\": 3.8700564971751414}, {\"level\": \"A1\", \"ambiguity_avg\": 3.2058432934926957, \"ambiguity_cont\": 3.75}, {\"level\": \"A1\", \"ambiguity_avg\": 3.5453277545327753, \"ambiguity_cont\": 4.089416058394161}, {\"level\": \"A1\", \"ambiguity_avg\": 3.489082969432314, \"ambiguity_cont\": 4.033755274261603}, {\"level\": \"A1\", \"ambiguity_avg\": 3.574105621805792, \"ambiguity_cont\": 4.212860310421286}, {\"level\": \"A1\", \"ambiguity_avg\": 3.21285140562249, \"ambiguity_cont\": 3.6526315789473682}, {\"level\": \"A1\", \"ambiguity_avg\": 3.3480662983425415, \"ambiguity_cont\": 4.104477611940299}, {\"level\": \"A1\", \"ambiguity_avg\": 3.7510204081632654, \"ambiguity_cont\": 4.348484848484849}, {\"level\": \"A1\", \"ambiguity_avg\": 3.032876712328767, \"ambiguity_cont\": 3.4869565217391303}, {\"level\": \"A1\", \"ambiguity_avg\": 3.4869431643625193, \"ambiguity_cont\": 4.073929961089494}, {\"level\": \"A1\", \"ambiguity_avg\": 3.0848329048843186, \"ambiguity_cont\": 3.696245733788396}, {\"level\": \"A1\", \"ambiguity_avg\": 3.326530612244898, \"ambiguity_cont\": 3.9285714285714284}, {\"level\": \"A1\", \"ambiguity_avg\": 3.412037037037037, \"ambiguity_cont\": 3.7403314917127073}, {\"level\": \"A1\", \"ambiguity_avg\": 3.0, \"ambiguity_cont\": 3.0778688524590163}, {\"level\": \"A1\", \"ambiguity_avg\": 3.2295719844357977, \"ambiguity_cont\": 3.419512195121951}, {\"level\": \"A1\", \"ambiguity_avg\": 3.0528846153846154, \"ambiguity_cont\": 3.5555555555555554}, {\"level\": \"A1\", \"ambiguity_avg\": 3.752212389380531, \"ambiguity_cont\": 4.384615384615385}, {\"level\": \"A1\", \"ambiguity_avg\": 3.3222222222222224, \"ambiguity_cont\": 3.8970588235294117}, {\"level\": \"A1\", \"ambiguity_avg\": 3.223463687150838, \"ambiguity_cont\": 3.7611940298507465}, {\"level\": \"A1\", \"ambiguity_avg\": 3.33596837944664, \"ambiguity_cont\": 3.7945544554455446}, {\"level\": \"A1\", \"ambiguity_avg\": 3.5455904334828103, \"ambiguity_cont\": 4.129343629343629}, {\"level\": \"A1\", \"ambiguity_avg\": 3.320689655172414, \"ambiguity_cont\": 3.6709401709401708}, {\"level\": \"A1\", \"ambiguity_avg\": 3.1338912133891212, \"ambiguity_cont\": 3.6575342465753424}, {\"level\": \"A1\", \"ambiguity_avg\": 3.2595419847328246, \"ambiguity_cont\": 3.72069825436409}, {\"level\": \"A1\", \"ambiguity_avg\": 3.4269230769230767, \"ambiguity_cont\": 3.8660287081339715}, {\"level\": \"A1\", \"ambiguity_avg\": 3.0112079701120797, \"ambiguity_cont\": 3.5342237061769617}, {\"level\": \"A1\", \"ambiguity_avg\": 3.4273789649415694, \"ambiguity_cont\": 4.119638826185102}, {\"level\": \"A1\", \"ambiguity_avg\": 3.229768786127168, \"ambiguity_cont\": 3.7145557655954633}, {\"level\": \"A1\", \"ambiguity_avg\": 3.875, \"ambiguity_cont\": 4.341269841269841}, {\"level\": \"A1\", \"ambiguity_avg\": 3.935064935064935, \"ambiguity_cont\": 4.515625}, {\"level\": \"A1\", \"ambiguity_avg\": 3.8818897637795278, \"ambiguity_cont\": 4.724489795918367}, {\"level\": \"A1\", \"ambiguity_avg\": 3.676190476190476, \"ambiguity_cont\": 4.193181818181818}, {\"level\": \"A1\", \"ambiguity_avg\": 3.171875, \"ambiguity_cont\": 3.6226415094339623}, {\"level\": \"A2\", \"ambiguity_avg\": 3.5323974082073435, \"ambiguity_cont\": 4.126062322946176}, {\"level\": \"A2\", \"ambiguity_avg\": 3.1983317886932343, \"ambiguity_cont\": 3.841387856257745}, {\"level\": \"A2\", \"ambiguity_avg\": 3.0336206896551725, \"ambiguity_cont\": 3.6310904872389793}, {\"level\": \"A2\", \"ambiguity_avg\": 3.153973509933775, \"ambiguity_cont\": 3.7073446327683617}, {\"level\": \"A2\", \"ambiguity_avg\": 3.234126984126984, \"ambiguity_cont\": 3.7814113597246126}, {\"level\": \"A2\", \"ambiguity_avg\": 3.182674199623352, \"ambiguity_cont\": 3.6574225122349104}, {\"level\": \"A2\", \"ambiguity_avg\": 3.0412371134020617, \"ambiguity_cont\": 3.610404624277457}, {\"level\": \"A2\", \"ambiguity_avg\": 3.8074074074074074, \"ambiguity_cont\": 4.562814070351759}, {\"level\": \"A2\", \"ambiguity_avg\": 3.394431554524362, \"ambiguity_cont\": 3.942227516378797}, {\"level\": \"A2\", \"ambiguity_avg\": 3.3076923076923075, \"ambiguity_cont\": 3.525}, {\"level\": \"A2\", \"ambiguity_avg\": 4.076923076923077, \"ambiguity_cont\": 4.96}, {\"level\": \"A2\", \"ambiguity_avg\": 2.97196261682243, \"ambiguity_cont\": 3.6184210526315788}, {\"level\": \"A2\", \"ambiguity_avg\": 3.7880794701986753, \"ambiguity_cont\": 4.581196581196581}, {\"level\": \"A2\", \"ambiguity_avg\": 3.9135802469135803, \"ambiguity_cont\": 5.105263157894737}, {\"level\": \"A2\", \"ambiguity_avg\": 3.609271523178808, \"ambiguity_cont\": 4.455357142857143}, {\"level\": \"A2\", \"ambiguity_avg\": 3.054054054054054, \"ambiguity_cont\": 3.4918032786885247}, {\"level\": \"A2\", \"ambiguity_avg\": 2.1904761904761907, \"ambiguity_cont\": 2.536842105263158}, {\"level\": \"A2\", \"ambiguity_avg\": 5.98989898989899, \"ambiguity_cont\": 7.35064935064935}, {\"level\": \"A2\", \"ambiguity_avg\": 2.8900343642611683, \"ambiguity_cont\": 4.0}, {\"level\": \"A2\", \"ambiguity_avg\": 3.6507936507936507, \"ambiguity_cont\": 4.134615384615385}, {\"level\": \"A2\", \"ambiguity_avg\": 3.066326530612245, \"ambiguity_cont\": 3.6490066225165565}, {\"level\": \"A2\", \"ambiguity_avg\": 2.671232876712329, \"ambiguity_cont\": 2.982142857142857}, {\"level\": \"A2\", \"ambiguity_avg\": 2.793103448275862, \"ambiguity_cont\": 3.1714285714285713}, {\"level\": \"A2\", \"ambiguity_avg\": 3.763157894736842, \"ambiguity_cont\": 4.818181818181818}, {\"level\": \"A2\", \"ambiguity_avg\": 3.5542168674698793, \"ambiguity_cont\": 4.0285714285714285}, {\"level\": \"A2\", \"ambiguity_avg\": 2.4918032786885247, \"ambiguity_cont\": 2.977777777777778}, {\"level\": \"A2\", \"ambiguity_avg\": 1.9791666666666667, \"ambiguity_cont\": 2.0930232558139537}, {\"level\": \"A2\", \"ambiguity_avg\": 3.326530612244898, \"ambiguity_cont\": 3.707317073170732}, {\"level\": \"A2\", \"ambiguity_avg\": 2.9863013698630136, \"ambiguity_cont\": 3.482142857142857}, {\"level\": \"A2\", \"ambiguity_avg\": 2.8636363636363638, \"ambiguity_cont\": 3.0}, {\"level\": \"A2\", \"ambiguity_avg\": 3.4761904761904763, \"ambiguity_cont\": 3.6140350877192984}, {\"level\": \"A2\", \"ambiguity_avg\": 2.3461538461538463, \"ambiguity_cont\": 2.5217391304347827}, {\"level\": \"A2\", \"ambiguity_avg\": 3.1097560975609757, \"ambiguity_cont\": 3.7244094488188977}, {\"level\": \"A2\", \"ambiguity_avg\": 3.4942263279445727, \"ambiguity_cont\": 4.114803625377643}, {\"level\": \"A2\", \"ambiguity_avg\": 2.8115942028985508, \"ambiguity_cont\": 3.2962962962962963}, {\"level\": \"A2\", \"ambiguity_avg\": 3.029304029304029, \"ambiguity_cont\": 3.626794258373206}, {\"level\": \"A2\", \"ambiguity_avg\": 3.417525773195876, \"ambiguity_cont\": 4.129824561403509}, {\"level\": \"A2\", \"ambiguity_avg\": 3.5283018867924527, \"ambiguity_cont\": 3.874015748031496}, {\"level\": \"A2\", \"ambiguity_avg\": 3.473684210526316, \"ambiguity_cont\": 3.857843137254902}, {\"level\": \"A2\", \"ambiguity_avg\": 3.6627906976744184, \"ambiguity_cont\": 4.154929577464789}, {\"level\": \"A2\", \"ambiguity_avg\": 2.7333333333333334, \"ambiguity_cont\": 3.206611570247934}, {\"level\": \"A2\", \"ambiguity_avg\": 2.8376068376068377, \"ambiguity_cont\": 3.6455696202531644}, {\"level\": \"A2\", \"ambiguity_avg\": 2.433333333333333, \"ambiguity_cont\": 2.8297872340425534}, {\"level\": \"A2\", \"ambiguity_avg\": 2.5714285714285716, \"ambiguity_cont\": 3.079136690647482}, {\"level\": \"A2\", \"ambiguity_avg\": 3.0278164116828927, \"ambiguity_cont\": 3.4861878453038675}, {\"level\": \"A2\", \"ambiguity_avg\": 3.317135549872123, \"ambiguity_cont\": 3.895424836601307}, {\"level\": \"A2\", \"ambiguity_avg\": 3.427480916030534, \"ambiguity_cont\": 4.0495049504950495}, {\"level\": \"A2\", \"ambiguity_avg\": 2.7662337662337664, \"ambiguity_cont\": 3.2758620689655173}, {\"level\": \"A2\", \"ambiguity_avg\": 3.1589285714285715, \"ambiguity_cont\": 3.815347721822542}, {\"level\": \"A2\", \"ambiguity_avg\": 3.238853503184713, \"ambiguity_cont\": 3.631048387096774}, {\"level\": \"A2\", \"ambiguity_avg\": 3.412298387096774, \"ambiguity_cont\": 3.8343949044585988}, {\"level\": \"A2\", \"ambiguity_avg\": 2.5724907063197024, \"ambiguity_cont\": 2.925373134328358}, {\"level\": \"A2\", \"ambiguity_avg\": 3.0686695278969958, \"ambiguity_cont\": 3.458100558659218}, {\"level\": \"A2\", \"ambiguity_avg\": 3.263157894736842, \"ambiguity_cont\": 3.986159169550173}, {\"level\": \"A2\", \"ambiguity_avg\": 3.2248322147651005, \"ambiguity_cont\": 3.903669724770642}, {\"level\": \"A2\", \"ambiguity_avg\": 3.4741035856573705, \"ambiguity_cont\": 4.115183246073299}, {\"level\": \"A2\", \"ambiguity_avg\": 2.9404761904761907, \"ambiguity_cont\": 3.282442748091603}, {\"level\": \"A2\", \"ambiguity_avg\": 2.739130434782609, \"ambiguity_cont\": 3.0242424242424244}, {\"level\": \"A2\", \"ambiguity_avg\": 3.5155709342560555, \"ambiguity_cont\": 3.986666666666667}, {\"level\": \"A2\", \"ambiguity_avg\": 3.1908212560386473, \"ambiguity_cont\": 3.68693009118541}, {\"level\": \"A2\", \"ambiguity_avg\": 3.316742081447964, \"ambiguity_cont\": 3.878260869565217}, {\"level\": \"A2\", \"ambiguity_avg\": 3.3846153846153846, \"ambiguity_cont\": 3.916923076923077}, {\"level\": \"B\", \"ambiguity_avg\": 3.662387676508344, \"ambiguity_cont\": 4.460884353741497}, {\"level\": \"B\", \"ambiguity_avg\": 2.93232044198895, \"ambiguity_cont\": 3.6025390625}, {\"level\": \"B\", \"ambiguity_avg\": 3.03486529318542, \"ambiguity_cont\": 3.8339140534262484}, {\"level\": \"B\", \"ambiguity_avg\": 3.6853932584269664, \"ambiguity_cont\": 4.414285714285715}, {\"level\": \"B\", \"ambiguity_avg\": 3.4217221135029354, \"ambiguity_cont\": 4.1302048909451425}, {\"level\": \"B\", \"ambiguity_avg\": 2.914942528735632, \"ambiguity_cont\": 3.5080128205128207}, {\"level\": \"B\", \"ambiguity_avg\": 3.106382978723404, \"ambiguity_cont\": 4.233333333333333}, {\"level\": \"B\", \"ambiguity_avg\": 3.177293934681182, \"ambiguity_cont\": 3.7651209677419355}, {\"level\": \"B\", \"ambiguity_avg\": 2.9683544303797467, \"ambiguity_cont\": 3.353448275862069}, {\"level\": \"B\", \"ambiguity_avg\": 3.6666666666666665, \"ambiguity_cont\": 3.9722222222222223}, {\"level\": \"B\", \"ambiguity_avg\": 3.238578680203046, \"ambiguity_cont\": 3.9192399049881237}, {\"level\": \"B\", \"ambiguity_avg\": 2.977535301668806, \"ambiguity_cont\": 3.3632585203657523}, {\"level\": \"B\", \"ambiguity_avg\": 3.1041666666666665, \"ambiguity_cont\": 3.6463414634146343}, {\"level\": \"B\", \"ambiguity_avg\": 3.6203544882790166, \"ambiguity_cont\": 4.196549137284321}, {\"level\": \"B\", \"ambiguity_avg\": 2.8720445062586926, \"ambiguity_cont\": 3.464576802507837}, {\"level\": \"B\", \"ambiguity_avg\": 3.077319587628866, \"ambiguity_cont\": 3.7222222222222223}, {\"level\": \"B\", \"ambiguity_avg\": 3.7586206896551726, \"ambiguity_cont\": 4.068376068376068}, {\"level\": \"B\", \"ambiguity_avg\": 3.578727841501564, \"ambiguity_cont\": 4.108638743455497}, {\"level\": \"B\", \"ambiguity_avg\": 3.3717357910906296, \"ambiguity_cont\": 3.8099009900990097}, {\"level\": \"B\", \"ambiguity_avg\": 3.3714285714285714, \"ambiguity_cont\": 3.53125}, {\"level\": \"B\", \"ambiguity_avg\": 3.2222222222222223, \"ambiguity_cont\": 3.7563025210084033}, {\"level\": \"B\", \"ambiguity_avg\": 3.7222222222222223, \"ambiguity_cont\": 4.362068965517241}, {\"level\": \"B\", \"ambiguity_avg\": 3.1481481481481484, \"ambiguity_cont\": 3.697674418604651}, {\"level\": \"B\", \"ambiguity_avg\": 2.9767441860465116, \"ambiguity_cont\": 3.342857142857143}, {\"level\": \"B\", \"ambiguity_avg\": 4.5131578947368425, \"ambiguity_cont\": 5.454545454545454}, {\"level\": \"B\", \"ambiguity_avg\": 3.057142857142857, \"ambiguity_cont\": 3.618181818181818}, {\"level\": \"B\", \"ambiguity_avg\": 1.9230769230769231, \"ambiguity_cont\": 2.7}, {\"level\": \"B\", \"ambiguity_avg\": 2.6666666666666665, \"ambiguity_cont\": 2.6666666666666665}, {\"level\": \"B\", \"ambiguity_avg\": 3.8383838383838382, \"ambiguity_cont\": 4.50625}, {\"level\": \"B\", \"ambiguity_avg\": 5.090909090909091, \"ambiguity_cont\": 5.615384615384615}, {\"level\": \"B\", \"ambiguity_avg\": 3.0555555555555554, \"ambiguity_cont\": 3.3125}, {\"level\": \"B\", \"ambiguity_avg\": 4.129032258064516, \"ambiguity_cont\": 4.730769230769231}, {\"level\": \"B\", \"ambiguity_avg\": 4.191780821917808, \"ambiguity_cont\": 4.932203389830509}, {\"level\": \"B\", \"ambiguity_avg\": 2.6153846153846154, \"ambiguity_cont\": 3.1904761904761907}, {\"level\": \"B\", \"ambiguity_avg\": 3.4150943396226414, \"ambiguity_cont\": 3.6808510638297873}, {\"level\": \"B\", \"ambiguity_avg\": 3.0, \"ambiguity_cont\": 3.4285714285714284}, {\"level\": \"B\", \"ambiguity_avg\": 3.311688311688312, \"ambiguity_cont\": 3.5873015873015874}, {\"level\": \"B\", \"ambiguity_avg\": 3.102493074792244, \"ambiguity_cont\": 3.8452380952380953}, {\"level\": \"B\", \"ambiguity_avg\": 2.99039780521262, \"ambiguity_cont\": 3.5413533834586466}, {\"level\": \"B\", \"ambiguity_avg\": 2.9965156794425085, \"ambiguity_cont\": 3.5821596244131455}, {\"level\": \"B\", \"ambiguity_avg\": 2.761904761904762, \"ambiguity_cont\": 3.5704697986577183}, {\"level\": \"B\", \"ambiguity_avg\": 3.217391304347826, \"ambiguity_cont\": 3.6742209631728047}, {\"level\": \"B\", \"ambiguity_avg\": 3.153284671532847, \"ambiguity_cont\": 3.6132075471698113}, {\"level\": \"B\", \"ambiguity_avg\": 3.1973969631236443, \"ambiguity_cont\": 3.6629834254143647}, {\"level\": \"B\", \"ambiguity_avg\": 2.58, \"ambiguity_cont\": 3.0}, {\"level\": \"B\", \"ambiguity_avg\": 3.43343653250774, \"ambiguity_cont\": 4.02020202020202}, {\"level\": \"B\", \"ambiguity_avg\": 3.102803738317757, \"ambiguity_cont\": 3.635294117647059}, {\"level\": \"B\", \"ambiguity_avg\": 2.9743589743589745, \"ambiguity_cont\": 3.6363636363636362}, {\"level\": \"B\", \"ambiguity_avg\": 2.8776978417266186, \"ambiguity_cont\": 3.4326923076923075}, {\"level\": \"B\", \"ambiguity_avg\": 2.42, \"ambiguity_cont\": 2.871559633027523}, {\"level\": \"B\", \"ambiguity_avg\": 3.4751552795031055, \"ambiguity_cont\": 4.048780487804878}, {\"level\": \"B\", \"ambiguity_avg\": 4.0, \"ambiguity_cont\": 5.205128205128205}, {\"level\": \"B\", \"ambiguity_avg\": 3.3017408123791103, \"ambiguity_cont\": 3.9558823529411766}, {\"level\": \"B\", \"ambiguity_avg\": 3.041407867494824, \"ambiguity_cont\": 3.5026315789473683}, {\"level\": \"B\", \"ambiguity_avg\": 3.1016949152542375, \"ambiguity_cont\": 3.630434782608696}, {\"level\": \"B\", \"ambiguity_avg\": 4.083333333333333, \"ambiguity_cont\": 4.363636363636363}, {\"level\": \"B\", \"ambiguity_avg\": 2.9135802469135803, \"ambiguity_cont\": 3.6785714285714284}, {\"level\": \"B\", \"ambiguity_avg\": 3.45021645021645, \"ambiguity_cont\": 3.7945945945945945}, {\"level\": \"B\", \"ambiguity_avg\": 2.5555555555555554, \"ambiguity_cont\": 3.076923076923077}, {\"level\": \"B\", \"ambiguity_avg\": 3.2327586206896552, \"ambiguity_cont\": 3.865168539325843}, {\"level\": \"B\", \"ambiguity_avg\": 3.5677083333333335, \"ambiguity_cont\": 4.401459854014599}, {\"level\": \"B\", \"ambiguity_avg\": 3.746556473829201, \"ambiguity_cont\": 4.408450704225352}, {\"level\": \"B\", \"ambiguity_avg\": 4.0, \"ambiguity_cont\": 4.4576271186440675}, {\"level\": \"B\", \"ambiguity_avg\": 3.8275862068965516, \"ambiguity_cont\": 4.242424242424242}, {\"level\": \"B\", \"ambiguity_avg\": 5.032258064516129, \"ambiguity_cont\": 5.62962962962963}, {\"level\": \"B\", \"ambiguity_avg\": 2.4242424242424243, \"ambiguity_cont\": 3.0434782608695654}, {\"level\": \"B\", \"ambiguity_avg\": 3.3149038461538463, \"ambiguity_cont\": 3.897106109324759}, {\"level\": \"B\", \"ambiguity_avg\": 3.286153846153846, \"ambiguity_cont\": 3.9036144578313254}, {\"level\": \"B\", \"ambiguity_avg\": 3.0930232558139537, \"ambiguity_cont\": 3.870967741935484}, {\"level\": \"B\", \"ambiguity_avg\": 3.46875, \"ambiguity_cont\": 4.714285714285714}, {\"level\": \"B\", \"ambiguity_avg\": 3.4444444444444446, \"ambiguity_cont\": 4.666666666666667}, {\"level\": \"B\", \"ambiguity_avg\": 3.1228070175438596, \"ambiguity_cont\": 3.7045454545454546}, {\"level\": \"B\", \"ambiguity_avg\": 3.3014440433212995, \"ambiguity_cont\": 3.9877450980392157}, {\"level\": \"B\", \"ambiguity_avg\": 2.7794117647058822, \"ambiguity_cont\": 3.6222222222222222}, {\"level\": \"B\", \"ambiguity_avg\": 3.1376146788990824, \"ambiguity_cont\": 3.810126582278481}, {\"level\": \"B\", \"ambiguity_avg\": 2.7710843373493974, \"ambiguity_cont\": 3.396551724137931}, {\"level\": \"B\", \"ambiguity_avg\": 3.075268817204301, \"ambiguity_cont\": 3.4313725490196076}, {\"level\": \"B\", \"ambiguity_avg\": 3.2346938775510203, \"ambiguity_cont\": 3.8275862068965516}, {\"level\": \"B\", \"ambiguity_avg\": 3.754491017964072, \"ambiguity_cont\": 4.7478991596638656}, {\"level\": \"B\", \"ambiguity_avg\": 3.736842105263158, \"ambiguity_cont\": 5.031746031746032}, {\"level\": \"B\", \"ambiguity_avg\": 2.9344262295081966, \"ambiguity_cont\": 3.297872340425532}, {\"level\": \"B\", \"ambiguity_avg\": 3.064935064935065, \"ambiguity_cont\": 3.3728813559322033}, {\"level\": \"B\", \"ambiguity_avg\": 3.238993710691824, \"ambiguity_cont\": 3.8583333333333334}, {\"level\": \"B\", \"ambiguity_avg\": 3.7057291666666665, \"ambiguity_cont\": 4.404761904761905}, {\"level\": \"B\", \"ambiguity_avg\": 3.596774193548387, \"ambiguity_cont\": 4.21978021978022}, {\"level\": \"B\", \"ambiguity_avg\": 3.12, \"ambiguity_cont\": 3.4358974358974357}, {\"level\": \"B\", \"ambiguity_avg\": 3.893617021276596, \"ambiguity_cont\": 4.333333333333333}, {\"level\": \"B\", \"ambiguity_avg\": 3.3839285714285716, \"ambiguity_cont\": 4.0552147239263805}, {\"level\": \"B\", \"ambiguity_avg\": 3.9026548672566372, \"ambiguity_cont\": 4.963414634146342}, {\"level\": \"B\", \"ambiguity_avg\": 4.095238095238095, \"ambiguity_cont\": 5.223684210526316}, {\"level\": \"B\", \"ambiguity_avg\": 3.419642857142857, \"ambiguity_cont\": 3.987577639751553}, {\"level\": \"B\", \"ambiguity_avg\": 3.2830188679245285, \"ambiguity_cont\": 3.975}, {\"level\": \"B\", \"ambiguity_avg\": 3.387434554973822, \"ambiguity_cont\": 4.020833333333333}, {\"level\": \"B\", \"ambiguity_avg\": 3.141304347826087, \"ambiguity_cont\": 3.8296296296296295}, {\"level\": \"B\", \"ambiguity_avg\": 2.275, \"ambiguity_cont\": 2.84}, {\"level\": \"B\", \"ambiguity_avg\": 2.90929203539823, \"ambiguity_cont\": 3.588957055214724}, {\"level\": \"B\", \"ambiguity_avg\": 3.392739273927393, \"ambiguity_cont\": 3.911392405063291}, {\"level\": \"B\", \"ambiguity_avg\": 3.795918367346939, \"ambiguity_cont\": 4.341463414634147}, {\"level\": \"B\", \"ambiguity_avg\": 4.428571428571429, \"ambiguity_cont\": 7.0}, {\"level\": \"B\", \"ambiguity_avg\": 4.625, \"ambiguity_cont\": 5.0}, {\"level\": \"B\", \"ambiguity_avg\": 5.0, \"ambiguity_cont\": 5.8}, {\"level\": \"B\", \"ambiguity_avg\": 4.6, \"ambiguity_cont\": 5.7368421052631575}, {\"level\": \"B\", \"ambiguity_avg\": 2.230769230769231, \"ambiguity_cont\": 2.875}, {\"level\": \"B\", \"ambiguity_avg\": 3.8, \"ambiguity_cont\": 4.5}, {\"level\": \"B\", \"ambiguity_avg\": 1.5714285714285714, \"ambiguity_cont\": 1.8}, {\"level\": \"B\", \"ambiguity_avg\": 2.375, \"ambiguity_cont\": 3.0625}, {\"level\": \"B\", \"ambiguity_avg\": 3.0, \"ambiguity_cont\": 3.372093023255814}, {\"level\": \"B\", \"ambiguity_avg\": 3.259259259259259, \"ambiguity_cont\": 3.95}, {\"level\": \"B\", \"ambiguity_avg\": 2.3, \"ambiguity_cont\": 2.5}, {\"level\": \"B\", \"ambiguity_avg\": 1.8333333333333333, \"ambiguity_cont\": 2.6666666666666665}, {\"level\": \"B1\", \"ambiguity_avg\": 3.045045045045045, \"ambiguity_cont\": 3.5681818181818183}, {\"level\": \"B1\", \"ambiguity_avg\": 3.3392857142857144, \"ambiguity_cont\": 3.897727272727273}, {\"level\": \"B1\", \"ambiguity_avg\": 2.4571428571428573, \"ambiguity_cont\": 3.0}, {\"level\": \"B1\", \"ambiguity_avg\": 3.3055555555555554, \"ambiguity_cont\": 3.86046511627907}, {\"level\": \"B1\", \"ambiguity_avg\": 3.0982142857142856, \"ambiguity_cont\": 3.606741573033708}, {\"level\": \"B1\", \"ambiguity_avg\": 3.226086956521739, \"ambiguity_cont\": 3.9310344827586206}, {\"level\": \"B1\", \"ambiguity_avg\": 2.5517241379310347, \"ambiguity_cont\": 3.25}, {\"level\": \"B1\", \"ambiguity_avg\": 3.9150943396226414, \"ambiguity_cont\": 4.5813953488372094}, {\"level\": \"B1\", \"ambiguity_avg\": 3.633663366336634, \"ambiguity_cont\": 4.142857142857143}, {\"level\": \"B1\", \"ambiguity_avg\": 2.4607843137254903, \"ambiguity_cont\": 2.7948717948717947}, {\"level\": \"B1\", \"ambiguity_avg\": 2.34375, \"ambiguity_cont\": 2.72}, {\"level\": \"B1\", \"ambiguity_avg\": 3.7183308494783907, \"ambiguity_cont\": 4.488}, {\"level\": \"B1\", \"ambiguity_avg\": 2.941028858218319, \"ambiguity_cont\": 3.476510067114094}, {\"level\": \"B1\", \"ambiguity_avg\": 3.757575757575758, \"ambiguity_cont\": 4.560267857142857}, {\"level\": \"B1\", \"ambiguity_avg\": 3.3466257668711656, \"ambiguity_cont\": 4.063025210084033}, {\"level\": \"B1\", \"ambiguity_avg\": 3.322074788902292, \"ambiguity_cont\": 3.8276972624798713}, {\"level\": \"B1\", \"ambiguity_avg\": 3.69164265129683, \"ambiguity_cont\": 4.285714285714286}, {\"level\": \"B1\", \"ambiguity_avg\": 3.2006745362563236, \"ambiguity_cont\": 3.7142857142857144}, {\"level\": \"B1\", \"ambiguity_avg\": 3.281124497991968, \"ambiguity_cont\": 3.776595744680851}, {\"level\": \"B1\", \"ambiguity_avg\": 2.9153094462540716, \"ambiguity_cont\": 3.4930232558139536}, {\"level\": \"B1\", \"ambiguity_avg\": 3.2146050670640833, \"ambiguity_cont\": 3.735812133072407}, {\"level\": \"B1\", \"ambiguity_avg\": 3.135388739946381, \"ambiguity_cont\": 3.664310954063604}, {\"level\": \"B1\", \"ambiguity_avg\": 2.992343032159265, \"ambiguity_cont\": 3.6212765957446806}, {\"level\": \"B1\", \"ambiguity_avg\": 3.46890286512928, \"ambiguity_cont\": 4.009812667261373}, {\"level\": \"B1\", \"ambiguity_avg\": 3.213507625272331, \"ambiguity_cont\": 3.814814814814815}, {\"level\": \"B1\", \"ambiguity_avg\": 3.3594351732991012, \"ambiguity_cont\": 3.986111111111111}, {\"level\": \"B1\", \"ambiguity_avg\": 3.165434906196703, \"ambiguity_cont\": 3.769109535066982}, {\"level\": \"B1\", \"ambiguity_avg\": 3.174228675136116, \"ambiguity_cont\": 3.823873121869783}, {\"level\": \"B1\", \"ambiguity_avg\": 3.0245051837888783, \"ambiguity_cont\": 3.5854922279792745}, {\"level\": \"B1\", \"ambiguity_avg\": 2.3333333333333335, \"ambiguity_cont\": 2.5428571428571427}, {\"level\": \"B1\", \"ambiguity_avg\": 3.864864864864865, \"ambiguity_cont\": 4.777777777777778}, {\"level\": \"B1\", \"ambiguity_avg\": 3.0, \"ambiguity_cont\": 3.6153846153846154}, {\"level\": \"B1\", \"ambiguity_avg\": 3.3636363636363638, \"ambiguity_cont\": 3.88}, {\"level\": \"B1\", \"ambiguity_avg\": 3.0, \"ambiguity_cont\": 3.5}, {\"level\": \"B1\", \"ambiguity_avg\": 3.8, \"ambiguity_cont\": 4.1940298507462686}, {\"level\": \"B1\", \"ambiguity_avg\": 3.3854166666666665, \"ambiguity_cont\": 3.993150684931507}, {\"level\": \"B1\", \"ambiguity_avg\": 3.169491525423729, \"ambiguity_cont\": 3.717391304347826}, {\"level\": \"B1\", \"ambiguity_avg\": 3.5789473684210527, \"ambiguity_cont\": 3.7222222222222223}, {\"level\": \"B1\", \"ambiguity_avg\": 3.5476190476190474, \"ambiguity_cont\": 4.095238095238095}, {\"level\": \"B1\", \"ambiguity_avg\": 2.735294117647059, \"ambiguity_cont\": 3.4583333333333335}, {\"level\": \"B1\", \"ambiguity_avg\": 2.4615384615384617, \"ambiguity_cont\": 2.9655172413793105}, {\"level\": \"B1\", \"ambiguity_avg\": 3.2758620689655173, \"ambiguity_cont\": 4.0}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(level_amb_df).mark_bar().encode(x = alt.X('ambiguity_cont', bin = alt.Bin(maxbins = 30)), y = 'count()').facet('level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-studio",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
