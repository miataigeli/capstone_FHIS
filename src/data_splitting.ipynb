{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c2aca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "\n",
    "# python -m spacy download es_core_news_md\n",
    "import spacy\n",
    "from spacy.lang.es.examples import sentences\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e66cd8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all texts\n",
    "def read_corpus(path=\"../corpus/\"):\n",
    "    \"\"\"\n",
    "    Given a path to a directory containing JSON files of the scraped corpus\n",
    "    documents and their metadata, load them all into a dict{list[dicts]}\n",
    "    such that:\n",
    "    {\n",
    "        \"A1\": [{\"source\": \"...\", \"content\": \"...\", ...}, {...}],\n",
    "        \"A2\": [...],\n",
    "        ...\n",
    "    }\n",
    "    path: (str) the path of the directory containing the JSON files\n",
    "    return: (dict{list[dicts]}) a dictionary of texts arranged by reading level\n",
    "    (a text is a single cohesive piece of reading material, be it a short\n",
    "    story, a poem, song lyrics, a book chapter, etc.)\n",
    "    \"\"\"\n",
    "\n",
    "    corpus = defaultdict(list)\n",
    "    for file in os.listdir(path):\n",
    "        if \"json\" in file:\n",
    "            with open(os.path.join(path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "                doc_list = json.load(f)\n",
    "                for d in doc_list:\n",
    "                    level = d[\"level\"]\n",
    "                    if level == \"A2/B1\":\n",
    "                        level = \"B1\"\n",
    "                    corpus[level].append(d)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "080acc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_corpus()\n",
    "corpus_A = corpus['A1'] + corpus['A2']\n",
    "corpus_B = corpus['B1'] + corpus['B']\n",
    "\n",
    "# shuffle\n",
    "random.shuffle(corpus_A)\n",
    "random.shuffle(corpus_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d5ab7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of A level texts: 156\n",
      "number of B level texts: 152\n",
      "eval size for A level: 16\n",
      "eval size for B level: 15\n"
     ]
    }
   ],
   "source": [
    "print(f'number of A level texts: {len(corpus_A)}')\n",
    "print(f'number of B level texts: {len(corpus_B)}')\n",
    "\n",
    "print(f'eval size for A level: {round(len(corpus_A) * 0.1)}')\n",
    "print(f'eval size for B level: {round(len(corpus_B) * 0.1)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "339b184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = corpus_A[16:] + corpus_B[15:]\n",
    "val = corpus_A[:16] + corpus_B[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f28dbea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for text in train:\n",
    "    X_train.append(text['content'])\n",
    "    y_train.append(text['level'][0])\n",
    "    \n",
    "X_val = []\n",
    "y_val = []\n",
    "for text in val:\n",
    "    X_val.append(text['content'])\n",
    "    y_val.append(text['level'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48cf1dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "with open('../data/train.json', 'w', encoding='utf-8') as fout:\n",
    "    json.dump(train, fout)\n",
    "with open('../data/val.json', 'w', encoding='utf-8') as fout:\n",
    "    json.dump(val, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10d35af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_file(split, filename, mode='X'):\n",
    "    '''\n",
    "    given a train or validation split, write to file\n",
    "    \n",
    "    split: (list) list of texts or list of labels\n",
    "    filename: (str) name for the output file\n",
    "    mode: (str) 'X' - the separator is #*20, 'y' - the separator is newline symbol \\n\n",
    "    '''\n",
    "    tofile = ''\n",
    "    if mode == 'X':\n",
    "        for text in split:\n",
    "            tofile += text + '#'*20\n",
    "        tofile = tofile[:-20]\n",
    "\n",
    "        with open(f'../data/{filename}.txt', 'w', encoding='utf-8') as fout:\n",
    "            fout.write(tofile)\n",
    "    \n",
    "    elif mode == 'y':\n",
    "        for label in split:\n",
    "            tofile += label + '\\n'\n",
    "        tofile = tofile[:-1]\n",
    "\n",
    "        with open(f'../data/{filename}.txt', 'w', encoding='utf-8') as fout:\n",
    "            fout.write(tofile)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b63c2448",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_file(X_train, 'X_train', 'X')\n",
    "to_file(y_train, 'y_train', 'y')\n",
    "to_file(X_val, 'X_val', 'X')\n",
    "to_file(y_val, 'y_val', 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa8977",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
