{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "western-there",
   "metadata": {},
   "source": [
    "## English Parser from Stanford Core-NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "attended-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.corenlp import CoreNLPServer\n",
    "import os\n",
    "import time\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "processed-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "coreNLP_dir = \"/Users/eun-youngchristinapark/Documents/stanford-corenlp-4.2.2\" # Change this to your coreNLP directory\n",
    "\n",
    "server = CoreNLPServer(\n",
    "   os.path.join(coreNLP_dir, \"stanford-corenlp-4.2.2.jar\"),\n",
    "   os.path.join(coreNLP_dir, \"stanford-corenlp-4.2.2-models.jar\")    \n",
    ")\n",
    "server.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rocky-headset",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\n",
      "  (S\n",
      "    (NP (PRP I))\n",
      "    (VP\n",
      "      (VBD put)\n",
      "      (NP (DT the) (NN book))\n",
      "      (PP (IN in) (NP (DT the) (NN box)))\n",
      "      (PP (IN on) (NP (DT the) (NN table))))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "parser = CoreNLPParser()\n",
    "parse = next(parser.raw_parse(\"I put the book in the box on the table.\"))\n",
    "print(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "official-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exempt-terrain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m  \u001b[0mCoreNLPParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'http://localhost:9000'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       ">>> parser = CoreNLPParser(url='http://localhost:9000')\n",
       "\n",
       ">>> next(\n",
       "...     parser.raw_parse('The quick brown fox jumps over the lazy dog.')\n",
       "... ).pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n",
       "                     ROOT\n",
       "                      |\n",
       "                      S\n",
       "       _______________|__________________________\n",
       "      |                         VP               |\n",
       "      |                _________|___             |\n",
       "      |               |             PP           |\n",
       "      |               |     ________|___         |\n",
       "      NP              |    |            NP       |\n",
       "  ____|__________     |    |     _______|____    |\n",
       " DT   JJ    JJ   NN  VBZ   IN   DT      JJ   NN  .\n",
       " |    |     |    |    |    |    |       |    |   |\n",
       "The quick brown fox jumps over the     lazy dog  .\n",
       "\n",
       ">>> (parse_fox, ), (parse_wolf, ) = parser.raw_parse_sents(\n",
       "...     [\n",
       "...         'The quick brown fox jumps over the lazy dog.',\n",
       "...         'The quick grey wolf jumps over the lazy fox.',\n",
       "...     ]\n",
       "... )\n",
       "\n",
       ">>> parse_fox.pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n",
       "                     ROOT\n",
       "                      |\n",
       "                      S\n",
       "       _______________|__________________________\n",
       "      |                         VP               |\n",
       "      |                _________|___             |\n",
       "      |               |             PP           |\n",
       "      |               |     ________|___         |\n",
       "      NP              |    |            NP       |\n",
       "  ____|__________     |    |     _______|____    |\n",
       " DT   JJ    JJ   NN  VBZ   IN   DT      JJ   NN  .\n",
       " |    |     |    |    |    |    |       |    |   |\n",
       "The quick brown fox jumps over the     lazy dog  .\n",
       "\n",
       ">>> parse_wolf.pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n",
       "                     ROOT\n",
       "                      |\n",
       "                      S\n",
       "       _______________|__________________________\n",
       "      |                         VP               |\n",
       "      |                _________|___             |\n",
       "      |               |             PP           |\n",
       "      |               |     ________|___         |\n",
       "      NP              |    |            NP       |\n",
       "  ____|_________      |    |     _______|____    |\n",
       " DT   JJ   JJ   NN   VBZ   IN   DT      JJ   NN  .\n",
       " |    |    |    |     |    |    |       |    |   |\n",
       "The quick grey wolf jumps over the     lazy fox  .\n",
       "\n",
       ">>> (parse_dog, ), (parse_friends, ) = parser.parse_sents(\n",
       "...     [\n",
       "...         \"I 'm a dog\".split(),\n",
       "...         \"This is my friends ' cat ( the tabby )\".split(),\n",
       "...     ]\n",
       "... )\n",
       "\n",
       ">>> parse_dog.pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n",
       "        ROOT\n",
       "         |\n",
       "         S\n",
       "  _______|____\n",
       " |            VP\n",
       " |    ________|___\n",
       " NP  |            NP\n",
       " |   |         ___|___\n",
       "PRP VBP       DT      NN\n",
       " |   |        |       |\n",
       " I   'm       a      dog\n",
       "\n",
       ">>> parse_friends.pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n",
       "     ROOT\n",
       "      |\n",
       "      S\n",
       "  ____|___________\n",
       " |                VP\n",
       " |     ___________|_____________\n",
       " |    |                         NP\n",
       " |    |                  _______|_________\n",
       " |    |                 NP               PRN\n",
       " |    |            _____|_______      ____|______________\n",
       " NP   |           NP            |    |        NP         |\n",
       " |    |     ______|_________    |    |     ___|____      |\n",
       " DT  VBZ  PRP$   NNS       POS  NN -LRB-  DT       NN  -RRB-\n",
       " |    |    |      |         |   |    |    |        |     |\n",
       "This  is   my  friends      '  cat -LRB- the     tabby -RRB-\n",
       "\n",
       ">>> parse_john, parse_mary, = parser.parse_text(\n",
       "...     'John loves Mary. Mary walks.'\n",
       "... )\n",
       "\n",
       ">>> parse_john.pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n",
       "      ROOT\n",
       "       |\n",
       "       S\n",
       "  _____|_____________\n",
       " |          VP       |\n",
       " |      ____|___     |\n",
       " NP    |        NP   |\n",
       " |     |        |    |\n",
       "NNP   VBZ      NNP   .\n",
       " |     |        |    |\n",
       "John loves     Mary  .\n",
       "\n",
       ">>> parse_mary.pretty_print()  # doctest: +NORMALIZE_WHITESPACE\n",
       "      ROOT\n",
       "       |\n",
       "       S\n",
       "  _____|____\n",
       " NP    VP   |\n",
       " |     |    |\n",
       "NNP   VBZ   .\n",
       " |     |    |\n",
       "Mary walks  .\n",
       "\n",
       "Special cases\n",
       "-------------\n",
       "\n",
       ">>> next(\n",
       "...     parser.raw_parse(\n",
       "...         'NASIRIYA, Iraqâ€”Iraqi doctors who treated former prisoner of war '\n",
       "...         'Jessica Lynch have angrily dismissed claims made in her biography '\n",
       "...         'that she was raped by her Iraqi captors.'\n",
       "...     )\n",
       "... ).height()\n",
       "20\n",
       "\n",
       ">>> next(\n",
       "...     parser.raw_parse(\n",
       "...         \"The broader Standard & Poor's 500 Index <.SPX> was 0.46 points lower, or \"\n",
       "...         '0.05 percent, at 997.02.'\n",
       "...     )\n",
       "... ).height()\n",
       "9\n",
       "\u001b[0;31mFile:\u001b[0m           ~/opt/miniconda3/lib/python3.8/site-packages/nltk/parse/corenlp.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "? CoreNLPParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-collar",
   "metadata": {},
   "source": [
    "## Spanish Dependency Parser 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "clinical-mason",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 23.4MB/s]                    \n",
      "2021-05-28 16:01:10 INFO: Downloading default packages for language: es (Spanish)...\n",
      "2021-05-28 16:01:11 INFO: File exists: /Users/eun-youngchristinapark/stanza_resources/es/default.zip.\n",
      "2021-05-28 16:01:15 INFO: Finished downloading models and saved to /Users/eun-youngchristinapark/stanza_resources.\n",
      "2021-05-28 16:01:15 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| depparse  | ancora  |\n",
      "| ner       | conll02 |\n",
      "=======================\n",
      "\n",
      "2021-05-28 16:01:15 INFO: Use device: cpu\n",
      "2021-05-28 16:01:15 INFO: Loading: tokenize\n",
      "2021-05-28 16:01:15 INFO: Loading: mwt\n",
      "2021-05-28 16:01:15 INFO: Loading: pos\n",
      "2021-05-28 16:01:15 INFO: Loading: lemma\n",
      "2021-05-28 16:01:15 INFO: Loading: depparse\n",
      "2021-05-28 16:01:15 INFO: Loading: ner\n",
      "2021-05-28 16:01:17 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('es') # download Spanish model\n",
    "nlp = stanza.Pipeline('es') # initialize Spanish neural pipeline\n",
    "doc = nlp(\"Has vendido un nuevo producto que es muy popular dentro de tu escuela.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "elementary-stuff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({\n",
      "  \"id\": 2,\n",
      "  \"text\": \"vendido\",\n",
      "  \"lemma\": \"vender\",\n",
      "  \"upos\": \"VERB\",\n",
      "  \"xpos\": \"VERB\",\n",
      "  \"feats\": \"Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part\",\n",
      "  \"head\": 0,\n",
      "  \"deprel\": \"root\",\n",
      "  \"misc\": \"start_char=4|end_char=11\"\n",
      "}, 'aux', {\n",
      "  \"id\": 1,\n",
      "  \"text\": \"Has\",\n",
      "  \"lemma\": \"haber\",\n",
      "  \"upos\": \"AUX\",\n",
      "  \"xpos\": \"AUX\",\n",
      "  \"feats\": \"Mood=Ind|Number=Sing|Person=2|Tense=Pres|VerbForm=Fin\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"aux\",\n",
      "  \"misc\": \"start_char=0|end_char=3\"\n",
      "}), ({\n",
      "  \"id\": 0,\n",
      "  \"text\": \"ROOT\"\n",
      "}, 'root', {\n",
      "  \"id\": 2,\n",
      "  \"text\": \"vendido\",\n",
      "  \"lemma\": \"vender\",\n",
      "  \"upos\": \"VERB\",\n",
      "  \"xpos\": \"VERB\",\n",
      "  \"feats\": \"Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part\",\n",
      "  \"head\": 0,\n",
      "  \"deprel\": \"root\",\n",
      "  \"misc\": \"start_char=4|end_char=11\"\n",
      "}), ({\n",
      "  \"id\": 5,\n",
      "  \"text\": \"producto\",\n",
      "  \"lemma\": \"producto\",\n",
      "  \"upos\": \"NOUN\",\n",
      "  \"xpos\": \"NOUN\",\n",
      "  \"feats\": \"Gender=Masc|Number=Sing\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"obj\",\n",
      "  \"misc\": \"start_char=21|end_char=29\"\n",
      "}, 'det', {\n",
      "  \"id\": 3,\n",
      "  \"text\": \"un\",\n",
      "  \"lemma\": \"uno\",\n",
      "  \"upos\": \"DET\",\n",
      "  \"xpos\": \"DET\",\n",
      "  \"feats\": \"Definite=Ind|Gender=Masc|Number=Sing|PronType=Art\",\n",
      "  \"head\": 5,\n",
      "  \"deprel\": \"det\",\n",
      "  \"misc\": \"start_char=12|end_char=14\"\n",
      "}), ({\n",
      "  \"id\": 5,\n",
      "  \"text\": \"producto\",\n",
      "  \"lemma\": \"producto\",\n",
      "  \"upos\": \"NOUN\",\n",
      "  \"xpos\": \"NOUN\",\n",
      "  \"feats\": \"Gender=Masc|Number=Sing\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"obj\",\n",
      "  \"misc\": \"start_char=21|end_char=29\"\n",
      "}, 'amod', {\n",
      "  \"id\": 4,\n",
      "  \"text\": \"nuevo\",\n",
      "  \"lemma\": \"nuevo\",\n",
      "  \"upos\": \"ADJ\",\n",
      "  \"xpos\": \"ADJ\",\n",
      "  \"feats\": \"Gender=Masc|Number=Sing\",\n",
      "  \"head\": 5,\n",
      "  \"deprel\": \"amod\",\n",
      "  \"misc\": \"start_char=15|end_char=20\"\n",
      "}), ({\n",
      "  \"id\": 2,\n",
      "  \"text\": \"vendido\",\n",
      "  \"lemma\": \"vender\",\n",
      "  \"upos\": \"VERB\",\n",
      "  \"xpos\": \"VERB\",\n",
      "  \"feats\": \"Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part\",\n",
      "  \"head\": 0,\n",
      "  \"deprel\": \"root\",\n",
      "  \"misc\": \"start_char=4|end_char=11\"\n",
      "}, 'obj', {\n",
      "  \"id\": 5,\n",
      "  \"text\": \"producto\",\n",
      "  \"lemma\": \"producto\",\n",
      "  \"upos\": \"NOUN\",\n",
      "  \"xpos\": \"NOUN\",\n",
      "  \"feats\": \"Gender=Masc|Number=Sing\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"obj\",\n",
      "  \"misc\": \"start_char=21|end_char=29\"\n",
      "}), ({\n",
      "  \"id\": 9,\n",
      "  \"text\": \"popular\",\n",
      "  \"lemma\": \"popular\",\n",
      "  \"upos\": \"ADJ\",\n",
      "  \"xpos\": \"ADJ\",\n",
      "  \"feats\": \"Number=Sing\",\n",
      "  \"head\": 5,\n",
      "  \"deprel\": \"acl\",\n",
      "  \"misc\": \"start_char=41|end_char=48\"\n",
      "}, 'nsubj', {\n",
      "  \"id\": 6,\n",
      "  \"text\": \"que\",\n",
      "  \"lemma\": \"que\",\n",
      "  \"upos\": \"PRON\",\n",
      "  \"xpos\": \"PRON\",\n",
      "  \"feats\": \"PronType=Int,Rel\",\n",
      "  \"head\": 9,\n",
      "  \"deprel\": \"nsubj\",\n",
      "  \"misc\": \"start_char=30|end_char=33\"\n",
      "}), ({\n",
      "  \"id\": 9,\n",
      "  \"text\": \"popular\",\n",
      "  \"lemma\": \"popular\",\n",
      "  \"upos\": \"ADJ\",\n",
      "  \"xpos\": \"ADJ\",\n",
      "  \"feats\": \"Number=Sing\",\n",
      "  \"head\": 5,\n",
      "  \"deprel\": \"acl\",\n",
      "  \"misc\": \"start_char=41|end_char=48\"\n",
      "}, 'cop', {\n",
      "  \"id\": 7,\n",
      "  \"text\": \"es\",\n",
      "  \"lemma\": \"ser\",\n",
      "  \"upos\": \"AUX\",\n",
      "  \"xpos\": \"AUX\",\n",
      "  \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "  \"head\": 9,\n",
      "  \"deprel\": \"cop\",\n",
      "  \"misc\": \"start_char=34|end_char=36\"\n",
      "}), ({\n",
      "  \"id\": 9,\n",
      "  \"text\": \"popular\",\n",
      "  \"lemma\": \"popular\",\n",
      "  \"upos\": \"ADJ\",\n",
      "  \"xpos\": \"ADJ\",\n",
      "  \"feats\": \"Number=Sing\",\n",
      "  \"head\": 5,\n",
      "  \"deprel\": \"acl\",\n",
      "  \"misc\": \"start_char=41|end_char=48\"\n",
      "}, 'advmod', {\n",
      "  \"id\": 8,\n",
      "  \"text\": \"muy\",\n",
      "  \"lemma\": \"mucho\",\n",
      "  \"upos\": \"ADV\",\n",
      "  \"xpos\": \"ADV\",\n",
      "  \"head\": 9,\n",
      "  \"deprel\": \"advmod\",\n",
      "  \"misc\": \"start_char=37|end_char=40\"\n",
      "}), ({\n",
      "  \"id\": 5,\n",
      "  \"text\": \"producto\",\n",
      "  \"lemma\": \"producto\",\n",
      "  \"upos\": \"NOUN\",\n",
      "  \"xpos\": \"NOUN\",\n",
      "  \"feats\": \"Gender=Masc|Number=Sing\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"obj\",\n",
      "  \"misc\": \"start_char=21|end_char=29\"\n",
      "}, 'acl', {\n",
      "  \"id\": 9,\n",
      "  \"text\": \"popular\",\n",
      "  \"lemma\": \"popular\",\n",
      "  \"upos\": \"ADJ\",\n",
      "  \"xpos\": \"ADJ\",\n",
      "  \"feats\": \"Number=Sing\",\n",
      "  \"head\": 5,\n",
      "  \"deprel\": \"acl\",\n",
      "  \"misc\": \"start_char=41|end_char=48\"\n",
      "}), ({\n",
      "  \"id\": 9,\n",
      "  \"text\": \"popular\",\n",
      "  \"lemma\": \"popular\",\n",
      "  \"upos\": \"ADJ\",\n",
      "  \"xpos\": \"ADJ\",\n",
      "  \"feats\": \"Number=Sing\",\n",
      "  \"head\": 5,\n",
      "  \"deprel\": \"acl\",\n",
      "  \"misc\": \"start_char=41|end_char=48\"\n",
      "}, 'advmod', {\n",
      "  \"id\": 10,\n",
      "  \"text\": \"dentro\",\n",
      "  \"lemma\": \"dentro\",\n",
      "  \"upos\": \"ADV\",\n",
      "  \"xpos\": \"ADV\",\n",
      "  \"head\": 9,\n",
      "  \"deprel\": \"advmod\",\n",
      "  \"misc\": \"start_char=49|end_char=55\"\n",
      "}), ({\n",
      "  \"id\": 13,\n",
      "  \"text\": \"escuela\",\n",
      "  \"lemma\": \"escuela\",\n",
      "  \"upos\": \"NOUN\",\n",
      "  \"xpos\": \"NOUN\",\n",
      "  \"feats\": \"Gender=Fem|Number=Sing\",\n",
      "  \"head\": 10,\n",
      "  \"deprel\": \"obl\",\n",
      "  \"misc\": \"start_char=62|end_char=69\"\n",
      "}, 'case', {\n",
      "  \"id\": 11,\n",
      "  \"text\": \"de\",\n",
      "  \"lemma\": \"de\",\n",
      "  \"upos\": \"ADP\",\n",
      "  \"xpos\": \"ADP\",\n",
      "  \"feats\": \"AdpType=Prep\",\n",
      "  \"head\": 13,\n",
      "  \"deprel\": \"case\",\n",
      "  \"misc\": \"start_char=56|end_char=58\"\n",
      "}), ({\n",
      "  \"id\": 13,\n",
      "  \"text\": \"escuela\",\n",
      "  \"lemma\": \"escuela\",\n",
      "  \"upos\": \"NOUN\",\n",
      "  \"xpos\": \"NOUN\",\n",
      "  \"feats\": \"Gender=Fem|Number=Sing\",\n",
      "  \"head\": 10,\n",
      "  \"deprel\": \"obl\",\n",
      "  \"misc\": \"start_char=62|end_char=69\"\n",
      "}, 'det', {\n",
      "  \"id\": 12,\n",
      "  \"text\": \"tu\",\n",
      "  \"lemma\": \"tu\",\n",
      "  \"upos\": \"DET\",\n",
      "  \"xpos\": \"DET\",\n",
      "  \"feats\": \"Number=Sing|Number[psor]=Sing|Person=2|Poss=Yes|PronType=Prs\",\n",
      "  \"head\": 13,\n",
      "  \"deprel\": \"det\",\n",
      "  \"misc\": \"start_char=59|end_char=61\"\n",
      "}), ({\n",
      "  \"id\": 10,\n",
      "  \"text\": \"dentro\",\n",
      "  \"lemma\": \"dentro\",\n",
      "  \"upos\": \"ADV\",\n",
      "  \"xpos\": \"ADV\",\n",
      "  \"head\": 9,\n",
      "  \"deprel\": \"advmod\",\n",
      "  \"misc\": \"start_char=49|end_char=55\"\n",
      "}, 'obl', {\n",
      "  \"id\": 13,\n",
      "  \"text\": \"escuela\",\n",
      "  \"lemma\": \"escuela\",\n",
      "  \"upos\": \"NOUN\",\n",
      "  \"xpos\": \"NOUN\",\n",
      "  \"feats\": \"Gender=Fem|Number=Sing\",\n",
      "  \"head\": 10,\n",
      "  \"deprel\": \"obl\",\n",
      "  \"misc\": \"start_char=62|end_char=69\"\n",
      "}), ({\n",
      "  \"id\": 2,\n",
      "  \"text\": \"vendido\",\n",
      "  \"lemma\": \"vender\",\n",
      "  \"upos\": \"VERB\",\n",
      "  \"xpos\": \"VERB\",\n",
      "  \"feats\": \"Gender=Masc|Number=Sing|Tense=Past|VerbForm=Part\",\n",
      "  \"head\": 0,\n",
      "  \"deprel\": \"root\",\n",
      "  \"misc\": \"start_char=4|end_char=11\"\n",
      "}, 'punct', {\n",
      "  \"id\": 14,\n",
      "  \"text\": \".\",\n",
      "  \"lemma\": \".\",\n",
      "  \"upos\": \"PUNCT\",\n",
      "  \"xpos\": \"PUNCT\",\n",
      "  \"feats\": \"PunctType=Peri\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"punct\",\n",
      "  \"misc\": \"start_char=69|end_char=70\"\n",
      "})]\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sentences:\n",
    "    #print(sentence.ents)\n",
    "    print(sentence.dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-found",
   "metadata": {},
   "source": [
    "## Spanish Dependency Parser 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "skilled-timeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-28 16:01:28 WARNING: Directory /Users/eun-youngchristinapark/Documents/stanza_corenlp already exists. Please install CoreNLP to a new directory.\n"
     ]
    }
   ],
   "source": [
    "stanza.install_corenlp(dir=\"/Users/eun-youngchristinapark/Documents/stanza_corenlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "civil-ferry",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-28 16:01:31 INFO: Downloading spanish models (version 4.1.0) into directory /Users/eun-youngchristinapark/Documents/stanza_corenlp...\n",
      "Downloading http://nlp.stanford.edu/software/stanford-corenlp-4.1.0-models-spanish.jar: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 275M/275M [00:48<00:00, 5.66MB/s] \n"
     ]
    }
   ],
   "source": [
    "stanza.download_corenlp_models(model='spanish', version='4.1.0', dir=\"/Users/eun-youngchristinapark/Documents/stanza_corenlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bridal-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanza.server import CoreNLPClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-tuesday",
   "metadata": {},
   "source": [
    "### English Parser works with Stanza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "committed-yesterday",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-28 16:02:29 INFO: Writing properties to tmp file: corenlp_server-3f2cb5d8377a4d64.props\n",
      "2021-05-28 16:02:29 INFO: Starting server with command: java -Xmx5G -cp /Users/eun-youngchristinapark/Documents/stanza_corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties corenlp_server-3f2cb5d8377a4d64.props -annotators parse -preload -outputFormat serialized\n"
     ]
    }
   ],
   "source": [
    "text = \"I am going to school.\"\n",
    "with CoreNLPClient(\n",
    "        classpath = '/Users/eun-youngchristinapark/Documents/stanza_corenlp/*',\n",
    "        annotators=['parse'],\n",
    "        timeout=30000,\n",
    "        memory='5G') as client:\n",
    "    ann = client.annotate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "pending-windows",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "child {\n",
      "  child {\n",
      "    child {\n",
      "      child {\n",
      "        value: \"I\"\n",
      "      }\n",
      "      value: \"PRP\"\n",
      "      score: -1.9594900608062744\n",
      "    }\n",
      "    value: \"NP\"\n",
      "    score: -2.8210573196411133\n",
      "  }\n",
      "  child {\n",
      "    child {\n",
      "      child {\n",
      "        value: \"am\"\n",
      "      }\n",
      "      value: \"VBP\"\n",
      "      score: -2.893768548965454\n",
      "    }\n",
      "    child {\n",
      "      child {\n",
      "        child {\n",
      "          value: \"going\"\n",
      "        }\n",
      "        value: \"VBG\"\n",
      "        score: -3.2131869792938232\n",
      "      }\n",
      "      child {\n",
      "        child {\n",
      "          child {\n",
      "            value: \"to\"\n",
      "          }\n",
      "          value: \"IN\"\n",
      "          score: -1.8374395370483398\n",
      "        }\n",
      "        child {\n",
      "          child {\n",
      "            child {\n",
      "              value: \"school\"\n",
      "            }\n",
      "            value: \"NN\"\n",
      "            score: -6.9619140625\n",
      "          }\n",
      "          value: \"NP\"\n",
      "          score: -9.055895805358887\n",
      "        }\n",
      "        value: \"PP\"\n",
      "        score: -11.415947914123535\n",
      "      }\n",
      "      value: \"VP\"\n",
      "      score: -16.762569427490234\n",
      "    }\n",
      "    value: \"VP\"\n",
      "    score: -23.903934478759766\n",
      "  }\n",
      "  child {\n",
      "    child {\n",
      "      value: \".\"\n",
      "    }\n",
      "    value: \".\"\n",
      "    score: -0.05752464756369591\n",
      "  }\n",
      "  value: \"S\"\n",
      "  score: -27.919036865234375\n",
      "}\n",
      "value: \"ROOT\"\n",
      "score: -28.090415954589844\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = ann.sentence[0]\n",
    "constituency_parse = sentence.parseTree\n",
    "print(constituency_parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-preliminary",
   "metadata": {},
   "source": [
    "### Spanish parser does not work with Stanza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "compressed-seventh",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-28 16:02:47 INFO: Using CoreNLP default properties for: spanish.  Make sure to have spanish models jar (available for download here: https://stanfordnlp.github.io/CoreNLP/) in CLASSPATH\n",
      "2021-05-28 16:02:47 INFO: Starting server with command: java -Xmx5G -cp /Users/eun-youngchristinapark/Documents/stanza_corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties spanish -annotators parse -preload -outputFormat serialized\n"
     ]
    },
    {
     "ename": "AnnotationException",
     "evalue": "edu.stanford.nlp.io.RuntimeIOException: java.io.InvalidClassException: edu.stanford.nlp.parser.shiftreduce.Weight; local class incompatible: stream classdesc serialVersionUID = 1, local class serialVersionUID = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/stanza/server/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, buf, properties, reset_default, **kwargs)\u001b[0m\n\u001b[1;32m    451\u001b[0m                               timeout=(self.timeout*2)/1000, **kwargs)\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    942\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: http://localhost:9000/?properties=%7B%27annotators%27%3A+%27parse%27%2C+%27outputFormat%27%3A+%27serialized%27%7D&resetDefault=false",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnnotationException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e8f84d05fe99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         memory='5G') as client:\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mspanish_ann\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspanish_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m#ann = client.annotate(text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/stanza/server/client.py\u001b[0m in \u001b[0;36mannotate\u001b[0;34m(self, text, annotators, output_format, properties, reset_default, **kwargs)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_default\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0mreset_default\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_default\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrequest_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"outputFormat\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"json\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/lib/python3.8/site-packages/stanza/server/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, buf, properties, reset_default, **kwargs)\u001b[0m\n\u001b[1;32m    456\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnnotationException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_default\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnnotationException\u001b[0m: edu.stanford.nlp.io.RuntimeIOException: java.io.InvalidClassException: edu.stanford.nlp.parser.shiftreduce.Weight; local class incompatible: stream classdesc serialVersionUID = 1, local class serialVersionUID = 3"
     ]
    }
   ],
   "source": [
    "spanish_text = \"Voy a la escuela.\"\n",
    "\n",
    "#CUSTOM_PROPS = {\"parse.model\": \"edu/stanford/nlp/models/srparser/spanishSR.beam.ser.gz\"}\n",
    "\n",
    "#with CoreNLPClient(properties=CUSTOM_PROPS, output_format=\"json\") as client:\n",
    "    \n",
    "with CoreNLPClient(\n",
    "        properties = 'es',\n",
    "        classpath = '/Users/eun-youngchristinapark/Documents/stanza_corenlp/*',\n",
    "        annotators=['parse'],\n",
    "        timeout=30000,\n",
    "        memory='5G') as client:\n",
    "    spanish_ann = client.annotate(spanish_text)\n",
    "    #ann = client.annotate(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-avatar",
   "metadata": {},
   "source": [
    "### Spanish Dependency Parser works with Stanza. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "white-badge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-28 16:06:17 INFO: Using CoreNLP default properties for: spanish.  Make sure to have spanish models jar (available for download here: https://stanfordnlp.github.io/CoreNLP/) in CLASSPATH\n",
      "2021-05-28 16:06:17 INFO: Starting server with command: java -Xmx5G -cp /Users/eun-youngchristinapark/Documents/stanza_corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties spanish -annotators depparse -preload -outputFormat serialized\n"
     ]
    }
   ],
   "source": [
    "spanish_text = \"MarÃ­a usa un hermoso vestido azul.\"\n",
    "\n",
    "#CUSTOM_PROPS = {\"parse.model\": \"edu/stanford/nlp/models/srparser/spanishSR.beam.ser.gz\"}\n",
    "\n",
    "#with CoreNLPClient(properties=CUSTOM_PROPS, output_format=\"json\") as client:\n",
    "    \n",
    "with CoreNLPClient(\n",
    "        properties = 'es',\n",
    "        classpath = '/Users/eun-youngchristinapark/Documents/stanza_corenlp/*',\n",
    "        annotators=['depparse'],\n",
    "        timeout=30000,\n",
    "        memory='5G') as client:\n",
    "    spanish_ann = client.annotate(spanish_text)\n",
    "    #ann = client.annotate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "solar-postcard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node {\n",
      "  sentenceIndex: 0\n",
      "  index: 1\n",
      "}\n",
      "node {\n",
      "  sentenceIndex: 0\n",
      "  index: 2\n",
      "}\n",
      "node {\n",
      "  sentenceIndex: 0\n",
      "  index: 3\n",
      "}\n",
      "node {\n",
      "  sentenceIndex: 0\n",
      "  index: 4\n",
      "}\n",
      "node {\n",
      "  sentenceIndex: 0\n",
      "  index: 5\n",
      "}\n",
      "node {\n",
      "  sentenceIndex: 0\n",
      "  index: 6\n",
      "}\n",
      "node {\n",
      "  sentenceIndex: 0\n",
      "  index: 7\n",
      "}\n",
      "edge {\n",
      "  source: 2\n",
      "  target: 1\n",
      "  dep: \"nsubj\"\n",
      "  isExtra: false\n",
      "  sourceCopy: 0\n",
      "  targetCopy: 0\n",
      "  language: Spanish\n",
      "}\n",
      "edge {\n",
      "  source: 2\n",
      "  target: 5\n",
      "  dep: \"obj\"\n",
      "  isExtra: false\n",
      "  sourceCopy: 0\n",
      "  targetCopy: 0\n",
      "  language: Spanish\n",
      "}\n",
      "edge {\n",
      "  source: 2\n",
      "  target: 7\n",
      "  dep: \"punct\"\n",
      "  isExtra: false\n",
      "  sourceCopy: 0\n",
      "  targetCopy: 0\n",
      "  language: Spanish\n",
      "}\n",
      "edge {\n",
      "  source: 5\n",
      "  target: 3\n",
      "  dep: \"det\"\n",
      "  isExtra: false\n",
      "  sourceCopy: 0\n",
      "  targetCopy: 0\n",
      "  language: Spanish\n",
      "}\n",
      "edge {\n",
      "  source: 5\n",
      "  target: 4\n",
      "  dep: \"amod\"\n",
      "  isExtra: false\n",
      "  sourceCopy: 0\n",
      "  targetCopy: 0\n",
      "  language: Spanish\n",
      "}\n",
      "edge {\n",
      "  source: 5\n",
      "  target: 6\n",
      "  dep: \"amod\"\n",
      "  isExtra: false\n",
      "  sourceCopy: 0\n",
      "  targetCopy: 0\n",
      "  language: Spanish\n",
      "}\n",
      "root: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = spanish_ann.sentence[0]\n",
    "#print(sentence)\n",
    "print(sentence.basicDependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-cycling",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
