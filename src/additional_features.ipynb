{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es-core-news-md==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-3.0.0/es_core_news_md-3.0.0-py3-none-any.whl (44.0 MB)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in c:\\users\\dasha\\appdata\\roaming\\python\\python38\\site-packages (from es-core-news-md==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (2.11.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (1.7.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (20.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (0.5.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (1.20.1+mkl)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (4.55.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (49.6.0.post20201009)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (1.26.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (3.0.4)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\dasha\\miniconda3\\lib\\site-packages (from jinja2->spacy<3.1.0,>=3.0.0->es-core-news-md==3.0.0) (1.1.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('es_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "# Use es_core_news_md pipeline for POS tagging\n",
    "!python -m spacy download es_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density of Pronouns\n",
    "\n",
    "The Density of Pronouns, as defined by the Coh-Metrix paper, is the ratio or proportion of noun phrases that are capstured by pronouns in the text compared to everything else. The paper explains that the density of pronouns is an important metric of comprehension difficulty because as the density of pronouns increases, the demands on working memory increases since the reader needs to keep the noun the pronoun refers to in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density of pronouns - the proportion of pronouns in the text\n",
    "\n",
    "# Example text\n",
    "text = \"\"\" Me llamo Darya y estoy una estudiante en la Universidad de Columbia Britanica.\n",
    "           Estudio ligüística computacional y ciencia de datos. \"\"\"\n",
    "\n",
    "# Extract POS tags\n",
    "# tokenize\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# types\n",
    "types = nltk.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.043478260869565216\n"
     ]
    }
   ],
   "source": [
    "# Pronoun proportion\n",
    "def pron_prop(text):\n",
    "    '''\n",
    "    Returns the density of pronouns of the text, which is the number of pronouns divided \n",
    "    by the number of tokens in the text. \n",
    "    --------------------------------------------\n",
    "    Argument: text (str) - a string of text\n",
    "    Returns: density of pronouns in the text \n",
    "    '''\n",
    "    doc = nlp(text)\n",
    "    total_pron = 0\n",
    "\n",
    "    for token in doc:\n",
    "        pos = token.pos_\n",
    "        if pos == 'PRON':\n",
    "            total_pron += 1\n",
    "    \n",
    "    #total_non_pron = len(tokens) - total_pron\n",
    "\n",
    "    prop_pron = total_pron/len(tokens)\n",
    "    \n",
    "    return prop_pron\n",
    "\n",
    "print(pron_prop(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentage of logical operators\n",
    "\n",
    "Logical operators, as defined by the Coh-Metrix paper, include variants of 'or', 'and', 'not' and 'if-then'. If a text has a high density of logical operators, the text is analytically dense and places a high demand on working memory, making it harder to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def prop_log_ops(text):\n",
    "    '''\n",
    "    Returns the percentage of logical operators from LOG_OPS in the text. \n",
    "    -----------------------------------------------------------\n",
    "    Argument: text (str) - a string of text\n",
    "    Returns: percentage of logical operators in the text (number of word in LOG_OPS in text / number of tokens in text)\n",
    "    '''\n",
    "\n",
    "    LOG_OPS = {'si', 'y', 'o', 'u', 'no'} # if, and, or, not\n",
    "\n",
    "    doc = nlp(text)\n",
    "    total_log_ops = 0\n",
    "\n",
    "    for token in doc:\n",
    "        if token in LOG_OPS:\n",
    "            total_log_ops += 1\n",
    "    \n",
    "    total_non_log = len(tokens) - total_log_ops\n",
    "\n",
    "    prop_logs = total_log_ops/total_non_log\n",
    "    \n",
    "    return prop_logs\n",
    "\n",
    "print(prop_log_ops(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Percentage of connectives\n",
    "\n",
    "Connectives, as defined by the Coh-Metrix paper, are words that add clarifying, temporal or causal information to text. They increase the cohesion of the text, so their number can be predictive of readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def connectives(text):\n",
    "    ''' \n",
    "    Returns the percentage of connectives from CONNECTIVES in the text. Connectives are phrases that add clarifying, temporal or causal information. \n",
    "    ----------------------------------------------------\n",
    "    Argument: text(str) - a string of text\n",
    "    Returns: number of connectives from CONNECTIVES in text\n",
    "    '''\n",
    "\n",
    "    CONNECTIVES = {'por eso', 'a pesar de', 'además', 'y', 'también', 'incluso', \n",
    "              'pero', 'aunque', 'sin embargo', 'no obstante', 'porque', 'ya que',\n",
    "              'puesto que', 'debido a que', 'a causa de que', 'como', 'así',\n",
    "              'entonces', 'por lo tanto', 'en consecuencia', 'después', 'antes',\n",
    "              'al mismo tiempo', 'finalmente', 'al principio', 'por último', \n",
    "              'dado que', 'pese a', 'es decir', 'o sea', 'y luego', 'primero',\n",
    "              'todavía', 'aún', 'cuando', 'aunque', 'por consiguiente', 'consecuentemente',\n",
    "              'por otra parte', 'es decir', 'por lo visto', 'que yo sepa', 'de todas formas',\n",
    "              'de todas maneras', 'aparte de', 'tal como', 'a vez de', 'en concreto',\n",
    "              'en pocas palabras', 'tan pronto como', 'mientras tanto', 'hasta', 'por último',\n",
    "              'pues', 'en cuanto', 'por fin', 'al mismo tiempo', 'a la misma vez', 'inmediatamente',\n",
    "              'durante', 'eventualmente', 'frecuentemente', 'al rato', 'en primer lugar', \n",
    "              'anoche', 'luego', 'nunca', 'ahora', 'muchas veces', 'al otro día', 'desde entonces',\n",
    "              'raramente', 'algunas veces', 'pronto'}\n",
    "\n",
    "    conn_count = 0\n",
    "\n",
    "    for conn in CONNECTIVES:\n",
    "        if conn in text.lower():\n",
    "            conn_count += 1\n",
    "    \n",
    "    return conn_count\n",
    "        \n",
    "print(connectives(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
