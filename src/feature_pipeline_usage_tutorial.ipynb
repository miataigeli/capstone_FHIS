{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecological-edward",
   "metadata": {},
   "source": [
    "## Tutorial for the Intended Usage of the Feature Extraction Pipeline API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-dialogue",
   "metadata": {},
   "source": [
    "##### Please ensure that you have spaCy and the \"es_core_news_md\" pipeline installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "extended-reliance",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download es_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "determined-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from utils import read_corpus\n",
    "from features import feature_pipeline\n",
    "\n",
    "# Setup pretty printer\n",
    "p = pprint.PrettyPrinter(indent=4, width=140)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-duncan",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "This notebook is intended to illustrate and explain the main workflows possible when using the feature extraction pipeline API for computing statistical/numerical features from a raw text in our collected corpus of Spanish texts. This notebook should be treated as a secondary resource for understanding the API, the primary resource being the file `features.py` containing the source code.\n",
    "\n",
    "Let's begin by broadly describing the stages of the feature extraction pipeline. To calculate any statistical feature from a raw, unprocessed text, the following steps must occur in sequence:\n",
    "1. Initialize the pipeline object\n",
    "2. Pre-process (clean up) the text to remove stray whitespaces, numerals, characters, etc.\n",
    "3. Extract fundamental attributes of the text (eg., tokens, POS tags, lemmas, etc.) using spaCy\n",
    "4. Use the extracted attributes to calculate statistical features of the text (eg., total number of tokens, type-token ratio, pronoun density, etc.)\n",
    "\n",
    "It is not possible to calculate a statistical feature without first extracting the fundamental attributes necessary for calculating that feature. In other words, we cannot reach stage 4 without completing stages 1, 2 and 3. However, the API is written with some shortcuts in place, allowing us to almost never need to explicitly call the `.preprocess()` method in our code. Depending on our usage we can even sometimes skip writing the spaCy step. It is most crucial to remember that the unprocessed text must be passed as an argument at some stage within the pipeline, but it is quite flexible as to which stage that should be.\n",
    "\n",
    "Through this tutorial we will understand these shortcuts and learn how to best apply them in workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-ownership",
   "metadata": {},
   "source": [
    "### Corpus Reading\n",
    "Let's begin by loading the corpus and seeing a broad view of what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "terminal-election",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1: 98\n",
      "A2: 71\n",
      "B: 152\n"
     ]
    }
   ],
   "source": [
    "corpus = read_corpus()\n",
    "\n",
    "for k, v in corpus.items():\n",
    "    print(f\"{k}: {len(v)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conventional-trinity",
   "metadata": {},
   "source": [
    "Let's pick a text from the corpus for the purposes of this demo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "maritime-graduate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12. LOS DÍAS DE LA SEMANA\n",
      "El año tiene cincuenta y dos semanas. Un\n",
      "mes tiene cuatro semanas y dos o tres días más.\n",
      "La semana tiene siete días. Los siete días se\n",
      "llaman: Domingo, lunes, martes, miércoles,\n",
      "jueves, viernes y sábado. El domingo es el\n",
      "primer día. Es el día de reposo. El domingo\n",
      "la gente no trabaja porque es el día de reposo.\n",
      "Los otros seis días son días de trabajo. La\n",
      "gente trabaja los otros días. Algunos discípulos\n",
      "no están satisfechos con un día de reposo.\n",
      "Ellos reposan también en la escuela. En los\n",
      "Estados Unidos los discípulos van a la escuela\n",
      "los lunes, los martes, los miércoles, los jueves\n",
      "y los viernes. En España los discípulos van a\n",
      "la escuela todos los días de trabajo; pero los\n",
      "miércoles y los sábados ellos van solamente por\n",
      "la mañana.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "unprocessed_text = corpus[\"A1\"][81][\"content\"]\n",
    "print(unprocessed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-catering",
   "metadata": {},
   "source": [
    "We can see that this piece of text is formatted somewhat \"irregularly\". It is a poem, so it has line breaks in the middle of grammatical sentences. It also has a title at the top, which is not a necessary component of the content of the text. In order to derive syntactic attributes like POS tags and dependency parses we will need to convert this text into a more standard form that can be interpreted by spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-deficit",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "Let's create a pipeline for cleaning up this text and extracting important attributes and features from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "utility-appraisal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "los días de la semana el año tiene cincuenta y dos semanas. un mes tiene cuatro semanas y dos o tres días más. la semana tiene siete días. los siete días se llaman: domingo, lunes, martes, miércoles, jueves, viernes y sábado. el domingo es el primer día. es el día de reposo. el domingo la gente no trabaja porque es el día de reposo. los otros seis días son días de trabajo. la gente trabaja los otros días. algunos discípulos no están satisfechos con un día de reposo. ellos reposan también en la escuela. en los estados unidos los discípulos van a la escuela los lunes, los martes, los miércoles, los jueves y los viernes. en españa los discípulos van a la escuela todos los días de trabajo; pero los miércoles y los sábados ellos van solamente por la mañana.\n"
     ]
    }
   ],
   "source": [
    "# This step passes the un-processed text to the pipeline and automatically cleans it up using the .preprocess() method\n",
    "pipe = feature_pipeline(unprocessed_text)\n",
    "print(pipe.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-feature",
   "metadata": {},
   "source": [
    "The text looks much more standardized now, which makes it easier for downstream functions to extract things from it in a consistent manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-waste",
   "metadata": {},
   "source": [
    "In the above cell we only called the pipeline object constructor but it automatically gave us a cleaned text. That is because the constructor already contains a call for the `.preprocess()` method if a text has been supplied to the object. This behaviour is functionally equivalent to the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prescribed-sleep",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "los días de la semana el año tiene cincuenta y dos semanas. un mes tiene cuatro semanas y dos o tres días más. la semana tiene siete días. los siete días se llaman: domingo, lunes, martes, miércoles, jueves, viernes y sábado. el domingo es el primer día. es el día de reposo. el domingo la gente no trabaja porque es el día de reposo. los otros seis días son días de trabajo. la gente trabaja los otros días. algunos discípulos no están satisfechos con un día de reposo. ellos reposan también en la escuela. en los estados unidos los discípulos van a la escuela los lunes, los martes, los miércoles, los jueves y los viernes. en españa los discípulos van a la escuela todos los días de trabajo; pero los miércoles y los sábados ellos van solamente por la mañana.\n"
     ]
    }
   ],
   "source": [
    "pipe = feature_pipeline()\n",
    "cleaned_text = pipe.preprocess(unprocessed_text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-oracle",
   "metadata": {},
   "source": [
    "\\\n",
    "As a rule of thumb, it is easiest to pass the unprocessed text to the pipeline at initialization, since the constructor has the effect of resetting all of the attributes to empty lists, giving the pipeline a blank slate for processing a new text that may come its way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-bishop",
   "metadata": {},
   "source": [
    "### Extracting Attributes of the Text using SpaCy\n",
    "Now that we have cleaned up the text into a standard form, spaCy will be able to derive some fundamental syntactic attributes from the text. Some attributes of the text that we might be interested in are the sentences, tokens and POS tags. We can try accessing them, but they won't be accessible at this stage since by default the pipeline constructor does not execute any of the spaCy methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "conditional-pointer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# The below calls to access class attributes will just return empty lists since those items have not been extracted yet\n",
    "print(pipe.sentences)\n",
    "print(pipe.tokens)\n",
    "print(pipe.pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-arrow",
   "metadata": {},
   "source": [
    "These attributes must be extracted from the text using spaCy's Spanish pipeline. It is recommended that we generate the attribute lists as and when we need them, since extracting all of the attributes for every text can be a bit slow (although the pre-processing of the text is typically the slowest step in the pipeline). We can extract some attributes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "approximate-control",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   'los días de la semana el año tiene cincuenta y dos semanas.',\n",
      "    'un mes tiene cuatro semanas y dos o tres días más.',\n",
      "    'la semana tiene siete días.',\n",
      "    'los siete días se llaman: domingo, lunes, martes, miércoles, jueves, viernes y sábado.',\n",
      "    'el domingo es el primer día.',\n",
      "    'es el día de reposo.',\n",
      "    'el domingo la gente no trabaja porque es el día de reposo.',\n",
      "    'los otros seis días son días de trabajo.',\n",
      "    'la gente trabaja los otros días.',\n",
      "    'algunos discípulos no están satisfechos con un día de reposo.',\n",
      "    'ellos reposan también en la escuela.',\n",
      "    'en los estados unidos los discípulos van a la escuela los lunes, los martes, los miércoles, los jueves y los viernes.',\n",
      "    'en españa los discípulos van a la escuela todos los días de trabajo; pero los miércoles y los sábados ellos van solamente por la '\n",
      "    'mañana.']\n",
      "\n",
      "['los', 'días', 'de', 'la', 'semana', 'el', 'año', 'tiene', 'cincuenta', 'y', 'dos', 'semanas', '.', 'un', 'mes', 'tiene', 'cuatro', 'semanas', 'y', 'dos', 'o', 'tres', 'días', 'más', '.', 'la', 'semana', 'tiene', 'siete', 'días', '.', 'los', 'siete', 'días', 'se', 'llaman', ':', 'domingo', ',', 'lunes', ',', 'martes', ',', 'miércoles', ',', 'jueves', ',', 'viernes', 'y', 'sábado', '.', 'el', 'domingo', 'es', 'el', 'primer', 'día', '.', 'es', 'el', 'día', 'de', 'reposo', '.', 'el', 'domingo', 'la', 'gente', 'no', 'trabaja', 'porque', 'es', 'el', 'día', 'de', 'reposo', '.', 'los', 'otros', 'seis', 'días', 'son', 'días', 'de', 'trabajo', '.', 'la', 'gente', 'trabaja', 'los', 'otros', 'días', '.', 'algunos', 'discípulos', 'no', 'están', 'satisfechos', 'con', 'un', 'día', 'de', 'reposo', '.', 'ellos', 'reposan', 'también', 'en', 'la', 'escuela', '.', 'en', 'los', 'estados', 'unidos', 'los', 'discípulos', 'van', 'a', 'la', 'escuela', 'los', 'lunes', ',', 'los', 'martes', ',', 'los', 'miércoles', ',', 'los', 'jueves', 'y', 'los', 'viernes', '.', 'en', 'españa', 'los', 'discípulos', 'van', 'a', 'la', 'escuela', 'todos', 'los', 'días', 'de', 'trabajo', ';', 'pero', 'los', 'miércoles', 'y', 'los', 'sábados', 'ellos', 'van', 'solamente', 'por', 'la', 'mañana', '.']\n"
     ]
    }
   ],
   "source": [
    "pipe.get_sentences()  # populates the pipe.sentences attribute\n",
    "pipe.get_tokens()  # populates the pipe.tokens attribute\n",
    "\n",
    "# Print out the attributes that were populated\n",
    "p.pprint(pipe.sentences)\n",
    "print()\n",
    "print(pipe.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-tsunami",
   "metadata": {},
   "source": [
    "\\\n",
    "Calling the methods above will populate the respective attributes with lists, but they also return the lists as outputs. We can assign the output to a variable and access it that way as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "public-biodiversity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'DET', 'NOUN', 'VERB', 'NUM', 'CCONJ', 'NUM', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'VERB', 'NUM', 'NOUN', 'CCONJ', 'NUM', 'CCONJ', 'NUM', 'NOUN', 'ADV', 'PUNCT', 'DET', 'NOUN', 'VERB', 'NUM', 'NOUN', 'PUNCT', 'DET', 'NUM', 'NOUN', 'PRON', 'AUX', 'PUNCT', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'NOUN', 'PUNCT', 'NOUN', 'CCONJ', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'AUX', 'DET', 'ADJ', 'NOUN', 'PUNCT', 'AUX', 'DET', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'DET', 'NOUN', 'ADV', 'VERB', 'SCONJ', 'AUX', 'DET', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'DET', 'DET', 'NUM', 'NOUN', 'VERB', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'VERB', 'DET', 'DET', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'ADV', 'VERB', 'ADJ', 'ADP', 'DET', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'PRON', 'AUX', 'ADV', 'ADP', 'DET', 'NOUN', 'PUNCT', 'ADP', 'DET', 'PROPN', 'PROPN', 'DET', 'NOUN', 'AUX', 'ADP', 'DET', 'NOUN', 'DET', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'CCONJ', 'DET', 'NOUN', 'PUNCT', 'ADP', 'PROPN', 'DET', 'NOUN', 'AUX', 'ADP', 'DET', 'NOUN', 'DET', 'DET', 'NOUN', 'ADP', 'NOUN', 'PUNCT', 'CCONJ', 'DET', 'NOUN', 'CCONJ', 'DET', 'NOUN', 'PRON', 'AUX', 'ADV', 'ADP', 'DET', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "tags = pipe.get_pos_tags()\n",
    "print(tags == pipe.pos_tags)  # check if they are the same\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-england",
   "metadata": {},
   "source": [
    "Putting it all together, let's create a new pipeline object and give it the text to automatically pre-process. We can then just call a `.get_*` method to access the attribute using spaCy, completely eliminating the explicit writing of the pre-processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "varying-victory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['los días', 'la semana', 'el año', 'semanas', 'un mes', 'semanas', 'días', 'la semana', 'días', 'los siete días', 'se', 'domingo', 'lunes', 'martes', 'miércoles', 'jueves', 'viernes', 'sábado', 'el domingo', 'el primer día', 'el día', 'reposo', 'el domingo', 'la gente', 'el día', 'reposo', 'los otros seis días', 'días', 'trabajo', 'la gente', 'los otros días', 'algunos discípulos', 'un día', 'reposo', 'ellos', 'la escuela', 'los estados unidos', 'unidos', 'los discípulos', 'la escuela', 'los lunes', 'los martes', 'los miércoles', 'los jueves', 'los viernes', 'españa', 'los discípulos', 'la escuela', 'los días', 'trabajo', 'los miércoles', 'los sábados', 'ellos', 'la mañana']\n"
     ]
    }
   ],
   "source": [
    "pipe = feature_pipeline(unprocessed_text)\n",
    "print(pipe.get_noun_chunks())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-devon",
   "metadata": {},
   "source": [
    "Alternatively, we could also create a blank pipeline object and pass in the text through the `.get_*` method. This is functionally the same as the above. The only difference is that the text pre-processing will occur at the spaCy stage instead of the constructor stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cheap-carbon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['los días', 'la semana', 'el año', 'semanas', 'un mes', 'semanas', 'días', 'la semana', 'días', 'los siete días', 'se', 'domingo', 'lunes', 'martes', 'miércoles', 'jueves', 'viernes', 'sábado', 'el domingo', 'el primer día', 'el día', 'reposo', 'el domingo', 'la gente', 'el día', 'reposo', 'los otros seis días', 'días', 'trabajo', 'la gente', 'los otros días', 'algunos discípulos', 'un día', 'reposo', 'ellos', 'la escuela', 'los estados unidos', 'unidos', 'los discípulos', 'la escuela', 'los lunes', 'los martes', 'los miércoles', 'los jueves', 'los viernes', 'españa', 'los discípulos', 'la escuela', 'los días', 'trabajo', 'los miércoles', 'los sábados', 'ellos', 'la mañana']\n"
     ]
    }
   ],
   "source": [
    "pipe = feature_pipeline()\n",
    "print(pipe.get_noun_chunks(unprocessed_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-tactics",
   "metadata": {},
   "source": [
    "\\\n",
    "Here is a list of all of the spaCy features supported by our pipeline so far:\n",
    "* sentences: \\\n",
    "    extraction function - `pipe.get_sentences()`, \\\n",
    "    attribute - `pipe.sentences`\n",
    "* tokens: \\\n",
    "    extraction function - `pipe.get_tokens()`, \\\n",
    "    attribute - `pipe.tokens`\n",
    "* lemmas: \\\n",
    "    extraction function - `pipe.get_lemmas()`, attribute - `pipe.lemmas`\n",
    "* POS tags: \\\n",
    "    extraction function - `pipe.get_pos_tags()`, \\\n",
    "    attribute - `pipe.pos_tags`\n",
    "* morphology tags: \\\n",
    "    extraction function - `pipe.get_morphology()`, \\\n",
    "    attribute - `pipe.morphs`\n",
    "* dependency parses: \\\n",
    "    extraction function - `pipe.get_dependency_parses()`, \\\n",
    "    attribute - `pipe.parses`\n",
    "* noun phrase chunks: \\\n",
    "    extraction function - `pipe.get_noun_chunks()`, \\\n",
    "    attribute - `pipe.noun_chunks`\n",
    "\n",
    "What if we want to extract all of the spaCy features in one go, instead of calling each of the `.get_*` methods one by one? We can do that by calling the method `.full_spacy()` which will extract all of these features, OR we could initialize the pipeline object with the flag `full_spacy=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "chicken-ranch",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "los días de la semana el año tiene cincuenta y dos semanas. un mes tiene cuatro semanas y dos o tres días más. la semana tiene siete días. los siete días se llaman: domingo, lunes, martes, miércoles, jueves, viernes y sábado. el domingo es el primer día. es el día de reposo. el domingo la gente no trabaja porque es el día de reposo. los otros seis días son días de trabajo. la gente trabaja los otros días. algunos discípulos no están satisfechos con un día de reposo. ellos reposan también en la escuela. en los estados unidos los discípulos van a la escuela los lunes, los martes, los miércoles, los jueves y los viernes. en españa los discípulos van a la escuela todos los días de trabajo; pero los miércoles y los sábados ellos van solamente por la mañana.\n",
      "\n",
      "['det', 'obl', 'case', 'det', 'nmod', 'det', 'obl', 'ROOT', 'nummod', 'cc', 'conj', 'obj', 'punct', 'det', 'nsubj', 'ROOT', 'nummod', 'obj', 'cc', 'nummod', 'cc', 'conj', 'conj', 'advmod', 'punct', 'det', 'obl', 'ROOT', 'nummod', 'obj', 'punct', 'det', 'nummod', 'nsubj', 'obj', 'ROOT', 'punct', 'obl', 'punct', 'nmod', 'punct', 'conj', 'punct', 'conj', 'punct', 'conj', 'punct', 'conj', 'cc', 'conj', 'punct', 'det', 'nsubj', 'cop', 'det', 'amod', 'ROOT', 'punct', 'cop', 'det', 'ROOT', 'case', 'nmod', 'punct', 'det', 'obl', 'det', 'nsubj', 'advmod', 'ROOT', 'mark', 'cop', 'det', 'advcl', 'case', 'nmod', 'punct', 'det', 'nmod', 'nummod', 'nsubj', 'cop', 'ROOT', 'case', 'nmod', 'punct', 'det', 'nsubj', 'ROOT', 'det', 'det', 'obj', 'punct', 'det', 'nsubj', 'advmod', 'cop', 'ROOT', 'case', 'det', 'obl', 'case', 'nmod', 'punct', 'nsubj', 'ROOT', 'advmod', 'case', 'det', 'obl', 'punct', 'case', 'det', 'obl', 'flat', 'det', 'nsubj', 'ROOT', 'case', 'det', 'obl', 'det', 'obl', 'punct', 'det', 'obl', 'punct', 'det', 'obl', 'punct', 'det', 'obl', 'cc', 'det', 'conj', 'punct', 'case', 'obl', 'det', 'nsubj', 'ROOT', 'case', 'det', 'obl', 'det', 'det', 'obl', 'case', 'nmod', 'punct', 'cc', 'det', 'obl', 'cc', 'det', 'conj', 'nsubj', 'conj', 'advmod', 'case', 'det', 'nmod', 'punct']\n"
     ]
    }
   ],
   "source": [
    "pipe = feature_pipeline(unprocessed_text, full_spacy=True)\n",
    "# Equivalent to:\n",
    "# pipe = feature_pipeline()\n",
    "# pipe.full_spacy(unprocessed_text)  # does not return any outputs, saves directly to attributes\n",
    "\n",
    "# All of the following items will automatically be extracted as part of the spaCy pipeline:\n",
    "print(pipe.text)\n",
    "print()\n",
    "print(pipe.parses)\n",
    "\n",
    "# Commented out for brevity\n",
    "# p.pprint(pipe.sentences)\n",
    "# print(pipe.tokens)\n",
    "# print(pipe.lemmas)\n",
    "# print(pipe.pos_tags)\n",
    "# print(pipe.morphs)\n",
    "# print(pipe.noun_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-bunny",
   "metadata": {},
   "source": [
    "### Numerical/Statistical Feature Extraction\n",
    "The most important aspect of the feature extraction pipeline is the ability to derive statistical/numerical features from the text given to it. For a comprehensive guide of all of the features that this pipeline is capable of computing please see the project report (TODO: LINK TO PROJECT REPORT). \\\n",
    "\\\n",
    "Using the pipeline that we created and the attributes that we extracted in the previous cell, here is how we can derive some features from a text using the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "atomic-subscriber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n",
      "0.05161290322580645\n",
      "88.93500000000002\n",
      "19.46153846153846\n"
     ]
    }
   ],
   "source": [
    "num_tokens = pipe.num_tokens()  # internally accesses pipe.tokens\n",
    "log_op_density = pipe.logical_operators()  # internally accesses pipe.tokens\n",
    "fh_score, syls_per_sent = pipe.fernandez_huerta_score()  # internally accesses pipe.tokens and pipe.sentences\n",
    "\n",
    "print(num_tokens)\n",
    "print(log_op_density)\n",
    "print(fh_score)\n",
    "print(syls_per_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-minimum",
   "metadata": {},
   "source": [
    "It is important to note that any of the statistical feature functions can be called directly without needing to run any of the spaCy extractors first. As long as a feature pipeline object has been created, calling any of the feature functions will automatically extract the spaCy features necessary for computing the desired statistical feature. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "little-paragraph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n",
      "0.05161290322580645\n",
      "88.93500000000002\n",
      "19.46153846153846\n"
     ]
    }
   ],
   "source": [
    "# Explicitly specifying full_spacy=False for demonstration purposes (default behaviour)\n",
    "pipe = feature_pipeline(unprocessed_text, full_spacy=False)\n",
    "# The above step simply cleans up the text. No spaCy features are extracted\n",
    "\n",
    "# All of these methods try to access their required spaCy attributes, and if\n",
    "# they find that the attribute does not yet exist the necessary method will\n",
    "# be called interally to generate those attributes.\n",
    "\n",
    "num_tokens = pipe.num_tokens()  # internally accesses pipe.tokens\n",
    "log_op_density = pipe.logical_operators()  # internally accesses pipe.tokens\n",
    "fh_score, syls_per_sent = pipe.fernandez_huerta_score()  # internally accesses pipe.tokens and pipe.sentences\n",
    "\n",
    "print(num_tokens)\n",
    "print(log_op_density)\n",
    "print(fh_score)\n",
    "print(syls_per_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-metadata",
   "metadata": {},
   "source": [
    "Alternatively, please note that the order of where the text is provided to the pipeline may also be switched. That is, the unprocessed text does not have to be provided to the pipeline at the initialization stage; it can be provided directly to the feature function as well. The text will automatically be cleaned up and the necessary attributes will be extracted using spaCy, following which the statistic will be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "encouraging-boston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "los días de la semana el año tiene cincuenta y dos semanas. un mes tiene cuatro semanas y dos o tres días más. la semana tiene siete días. los siete días se llaman: domingo, lunes, martes, miércoles, jueves, viernes y sábado. el domingo es el primer día. es el día de reposo. el domingo la gente no trabaja porque es el día de reposo. los otros seis días son días de trabajo. la gente trabaja los otros días. algunos discípulos no están satisfechos con un día de reposo. ellos reposan también en la escuela. en los estados unidos los discípulos van a la escuela los lunes, los martes, los miércoles, los jueves y los viernes. en españa los discípulos van a la escuela todos los días de trabajo; pero los miércoles y los sábados ellos van solamente por la mañana.\n",
      "\n",
      "['los', 'días', 'de', 'la', 'semana', 'el', 'año', 'tiene', 'cincuenta', 'y', 'dos', 'semanas', '.', 'un', 'mes', 'tiene', 'cuatro', 'semanas', 'y', 'dos', 'o', 'tres', 'días', 'más', '.', 'la', 'semana', 'tiene', 'siete', 'días', '.', 'los', 'siete', 'días', 'se', 'llaman', ':', 'domingo', ',', 'lunes', ',', 'martes', ',', 'miércoles', ',', 'jueves', ',', 'viernes', 'y', 'sábado', '.', 'el', 'domingo', 'es', 'el', 'primer', 'día', '.', 'es', 'el', 'día', 'de', 'reposo', '.', 'el', 'domingo', 'la', 'gente', 'no', 'trabaja', 'porque', 'es', 'el', 'día', 'de', 'reposo', '.', 'los', 'otros', 'seis', 'días', 'son', 'días', 'de', 'trabajo', '.', 'la', 'gente', 'trabaja', 'los', 'otros', 'días', '.', 'algunos', 'discípulos', 'no', 'están', 'satisfechos', 'con', 'un', 'día', 'de', 'reposo', '.', 'ellos', 'reposan', 'también', 'en', 'la', 'escuela', '.', 'en', 'los', 'estados', 'unidos', 'los', 'discípulos', 'van', 'a', 'la', 'escuela', 'los', 'lunes', ',', 'los', 'martes', ',', 'los', 'miércoles', ',', 'los', 'jueves', 'y', 'los', 'viernes', '.', 'en', 'españa', 'los', 'discípulos', 'van', 'a', 'la', 'escuela', 'todos', 'los', 'días', 'de', 'trabajo', ';', 'pero', 'los', 'miércoles', 'y', 'los', 'sábados', 'ellos', 'van', 'solamente', 'por', 'la', 'mañana', '.']\n",
      "\n",
      "163\n",
      "0.05161290322580645\n",
      "88.93500000000002\n",
      "19.46153846153846\n"
     ]
    }
   ],
   "source": [
    "pipe = feature_pipeline()\n",
    "\n",
    "# Saves the cleaned text to pipe.text and the extracted list of tokens to pipe.tokens\n",
    "num_tokens = pipe.num_tokens(text=unprocessed_text)\n",
    "print(pipe.text)\n",
    "print()\n",
    "print(pipe.tokens)\n",
    "print()\n",
    "\n",
    "# No need to pass any arguments, it internally accesses pipe.tokens\n",
    "log_op_density = pipe.logical_operators()\n",
    "\n",
    "# No need to pass any arguments, it internally accesses pipe.tokens\n",
    "# and extracts pipe.sentences from the cleaned up pipe.text\n",
    "fh_score, syls_per_sent = pipe.fernandez_huerta_score()\n",
    "\n",
    "print(num_tokens)\n",
    "print(log_op_density)\n",
    "print(fh_score)\n",
    "print(syls_per_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-equity",
   "metadata": {},
   "source": [
    "Finally, let's extract all of the available statistical features in one go. Accomplishing this is as simple as creating a pipeline object and calling the `.feature_extractor()` method. We explicitly only write two lines, and the unprocessed text can be supplied to the pipeline at either line, but under the hood all 4 stages of processing the text will take place. \\\n",
    "(If the object is initialized with a text, `.feature_extractor()` does not require any arguments. Otherwise, if the object is initialized *without* a text, `.feature_extractor()` must be given a text in order to perform pre-processing and spaCy attribute extraction.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "silver-suspect",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-08 12:52:16 INFO: Using CoreNLP default properties for: spanish.  Make sure to have spanish models jar (available for download here: https://stanfordnlp.github.io/CoreNLP/) in CLASSPATH\n",
      "2021-06-08 12:52:21 INFO: Starting server with command: java -Xmx5G -cp C:/Users/rsss9/stanza_corenlp/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 30000 -threads 5 -maxCharLength 100000 -quiet False -serverProperties spanish -annotators depparse -preload -outputFormat serialized\n"
     ]
    }
   ],
   "source": [
    "pipe = feature_pipeline(\n",
    "    # These arguments are necessary for extracting the entire set of features\n",
    "    dep_parse_flag=True,\n",
    "    dep_parse_classpath=\"C:/Users/rsss9/stanza_corenlp/*\",\n",
    "    result_root=\"../wordnet_spa\",\n",
    ")\n",
    "# It is necessary to start the CoreNLP client before extraction\n",
    "pipe.corenlp_client.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "patent-garbage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'ADJ': 0.012269938650306749,\n",
      "    'ADP': 0.07975460122699386,\n",
      "    'ADV': 0.03067484662576687,\n",
      "    'AUX': 0.049079754601226995,\n",
      "    'CCONJ': 0.04294478527607362,\n",
      "    'CONJ': 0.0,\n",
      "    'CONTENT': 0.5571428571428572,\n",
      "    'DET': 0.2147239263803681,\n",
      "    'EOL': 0.0,\n",
      "    'FUNCTION': 0.44285714285714284,\n",
      "    'Fut': 0.0,\n",
      "    'INTJ': 0.0,\n",
      "    'Imp': 0.0,\n",
      "    'NOUN': 0.294478527607362,\n",
      "    'NUM': 0.049079754601226995,\n",
      "    'PART': 0.0,\n",
      "    'PRON': 0.018404907975460124,\n",
      "    'PROPN': 0.018404907975460124,\n",
      "    'PUNCT': 0.1411042944785276,\n",
      "    'Past': 0.0,\n",
      "    'Pres': 1.0,\n",
      "    'SCONJ': 0.006134969325153374,\n",
      "    'SPACE': 0.0,\n",
      "    'SYM': 0.0,\n",
      "    'VERB': 0.04294478527607362,\n",
      "    'X': 0.0,\n",
      "    'avg_ambiguation_all_words': 2.337078651685393,\n",
      "    'avg_ambiguation_content_words': 2.757575757575758,\n",
      "    'avg_degree_of_abstraction': 7.303819444444444,\n",
      "    'avg_parse_tree_depth': 2.923076923076923,\n",
      "    'avg_rank_of_lemmas_in_freq_list': 657.8036809815951,\n",
      "    'avg_sent_length': 12.538461538461538,\n",
      "    'fernandez_huerta_score': 88.93500000000002,\n",
      "    'logical_operator_density': 0.05161290322580645,\n",
      "    'min_degree_of_abstraction': 4.0,\n",
      "    'noun_phrase_density': 0.7037037037037037,\n",
      "    'num_connectives': 4,\n",
      "    'pronoun_density': 0.01875,\n",
      "    'proportion_of_A_level_tokens': 0.75,\n",
      "    'proportion_of_A_level_types': 0.7307692307692307,\n",
      "    'syllables_per_sentence': 19.46153846153846,\n",
      "    'total_tokens': 163,\n",
      "    'total_tokens_w/o_stopwords': 48,\n",
      "    'type_token_ratio': 0.3987730061349693}\n"
     ]
    }
   ],
   "source": [
    "p.pprint(pipe.feature_extractor(unprocessed_text))\n",
    "\n",
    "# ALTERNATIVELY:\n",
    "# pipe = feature_pipeline(unprocessed_text)\n",
    "# pipe.feature_extractor()\n",
    "\n",
    "# It is necessary to stop the CoreNLP client after extraction\n",
    "pipe.corenlp_client.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
